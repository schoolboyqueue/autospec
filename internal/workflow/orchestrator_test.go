package workflow

import (
	"fmt"
	"os"
	"path/filepath"
	"runtime"
	"strings"
	"testing"

	"github.com/ariel-frischer/autospec/internal/cliagent"
	"github.com/ariel-frischer/autospec/internal/config"
	"github.com/ariel-frischer/autospec/internal/spec"
	"github.com/ariel-frischer/autospec/internal/validation"
)

// testConfigWithAgent creates a test configuration with the specified agent preset.
func testConfigWithAgent(specsDir, stateDir, agentPreset string) *config.Configuration {
	return &config.Configuration{
		AgentPreset: agentPreset,
		SpecsDir:    specsDir,
		StateDir:    stateDir,
		MaxRetries:  3,
	}
}

// testConfigWithEchoAgent creates a test configuration using echo as a custom agent.
func testConfigWithEchoAgent(specsDir, stateDir string) *config.Configuration {
	return &config.Configuration{
		CustomAgent: &cliagent.CustomAgentConfig{
			Command: "echo",
			Args:    []string{"{{PROMPT}}"},
		},
		SpecsDir:   specsDir,
		StateDir:   stateDir,
		MaxRetries: 3,
	}
}

func TestNewWorkflowOrchestrator(t *testing.T) {
	cfg := testConfigWithAgent("./specs", "~/.autospec/state", "claude")

	orchestrator := NewWorkflowOrchestrator(cfg)

	if orchestrator == nil {
		t.Fatal("NewWorkflowOrchestrator() returned nil")
	}

	if orchestrator.SpecsDir != "./specs" {
		t.Errorf("SpecsDir = %v, want './specs'", orchestrator.SpecsDir)
	}

	if orchestrator.Executor == nil {
		t.Error("Executor should not be nil")
	}
}

func TestWorkflowOrchestrator_Configuration(t *testing.T) {
	tmpDir := t.TempDir()

	cfg := testConfigWithAgent(tmpDir, filepath.Join(tmpDir, "state"), "claude")

	orchestrator := NewWorkflowOrchestrator(cfg)

	if orchestrator.SpecsDir != tmpDir {
		t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, tmpDir)
	}

	if orchestrator.Config.MaxRetries != 3 {
		t.Errorf("MaxRetries = %v, want 3", orchestrator.Config.MaxRetries)
	}
}

// TestExecutePlanWithPrompt tests that plan commands properly format prompts
func TestExecutePlanWithPrompt(t *testing.T) {
	tests := map[string]struct {
		prompt      string
		wantCommand string
	}{
		"no prompt": {
			prompt:      "",
			wantCommand: "/autospec.plan",
		},
		"simple prompt": {
			prompt:      "Focus on security",
			wantCommand: `/autospec.plan "Focus on security"`,
		},
		"prompt with quotes": {
			prompt:      "Use 'best practices' for auth",
			wantCommand: `/autospec.plan "Use 'best practices' for auth"`,
		},
		"multiline prompt": {
			prompt: `Consider these aspects:
  - Performance
  - Scalability`,
			wantCommand: `/autospec.plan "Consider these aspects:
  - Performance
  - Scalability"`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// This test verifies the command construction logic
			command := "/autospec.plan"
			if tc.prompt != "" {
				command = "/autospec.plan \"" + tc.prompt + "\""
			}

			if command != tc.wantCommand {
				t.Errorf("command = %q, want %q", command, tc.wantCommand)
			}
		})
	}
}

// TestExecuteTasksWithPrompt tests that tasks commands properly format prompts
func TestExecuteTasksWithPrompt(t *testing.T) {
	tests := map[string]struct {
		prompt      string
		wantCommand string
	}{
		"no prompt": {
			prompt:      "",
			wantCommand: "/autospec.tasks",
		},
		"simple prompt": {
			prompt:      "Break into small steps",
			wantCommand: `/autospec.tasks "Break into small steps"`,
		},
		"prompt with quotes": {
			prompt:      "Make tasks 'granular' and testable",
			wantCommand: `/autospec.tasks "Make tasks 'granular' and testable"`,
		},
		"complex prompt": {
			prompt: `Requirements:
  - Each task < 1 hour
  - Include testing`,
			wantCommand: `/autospec.tasks "Requirements:
  - Each task < 1 hour
  - Include testing"`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// This test verifies the command construction logic
			command := "/autospec.tasks"
			if tc.prompt != "" {
				command = "/autospec.tasks \"" + tc.prompt + "\""
			}

			if command != tc.wantCommand {
				t.Errorf("command = %q, want %q", command, tc.wantCommand)
			}
		})
	}
}

// TestSpecNameFormat tests that spec names are formatted correctly with number prefix
func TestSpecNameFormat(t *testing.T) {
	tests := map[string]struct {
		metadata     *spec.Metadata
		wantSpecName string
	}{
		"spec with three-digit number": {
			metadata: &spec.Metadata{
				Number: "003",
				Name:   "command-timeout",
			},
			wantSpecName: "003-command-timeout",
		},
		"spec with two-digit number": {
			metadata: &spec.Metadata{
				Number: "002",
				Name:   "go-binary-migration",
			},
			wantSpecName: "002-go-binary-migration",
		},
		"spec with single digit number": {
			metadata: &spec.Metadata{
				Number: "001",
				Name:   "initial-feature",
			},
			wantSpecName: "001-initial-feature",
		},
		"spec with hyphenated name": {
			metadata: &spec.Metadata{
				Number: "123",
				Name:   "multi-word-feature-name",
			},
			wantSpecName: "123-multi-word-feature-name",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Test the format string used in the workflow
			specName := fmt.Sprintf("%s-%s", tt.metadata.Number, tt.metadata.Name)

			if specName != tt.wantSpecName {
				t.Errorf("spec name format = %q, want %q", specName, tt.wantSpecName)
			}
		})
	}
}

// TestSpecDirectoryConstruction tests that spec directories are constructed correctly
func TestSpecDirectoryConstruction(t *testing.T) {
	tmpDir := t.TempDir()

	tests := map[string]struct {
		specsDir    string
		specName    string
		wantSpecDir string
	}{
		"full spec name with number": {
			specsDir:    tmpDir,
			specName:    "003-command-timeout",
			wantSpecDir: filepath.Join(tmpDir, "003-command-timeout"),
		},
		"relative specs dir": {
			specsDir:    "./specs",
			specName:    "002-go-binary-migration",
			wantSpecDir: filepath.Join("./specs", "002-go-binary-migration"),
		},
		"absolute specs dir": {
			specsDir:    "/tmp/specs",
			specName:    "001-initial-feature",
			wantSpecDir: filepath.Join("/tmp/specs", "001-initial-feature"),
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Test directory construction used in executor
			specDir := filepath.Join(tt.specsDir, tt.specName)

			if specDir != tt.wantSpecDir {
				t.Errorf("spec directory = %q, want %q", specDir, tt.wantSpecDir)
			}
		})
	}
}

// TestSpecNameFromMetadata verifies spec name is constructed correctly from metadata
func TestSpecNameFromMetadata(t *testing.T) {
	// Create a temporary directory structure
	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "003-command-timeout")

	// Create the directory structure
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create test directory: %v", err)
	}

	// Create a spec.md file
	specFile := filepath.Join(specDir, "spec.md")
	if err := os.WriteFile(specFile, []byte("# Test Spec"), 0644); err != nil {
		t.Fatalf("Failed to create spec.md: %v", err)
	}

	// Test getting metadata
	metadata, err := spec.GetSpecMetadata(specsDir, "003-command-timeout")
	if err != nil {
		t.Fatalf("GetSpecMetadata() error = %v", err)
	}

	// Verify the metadata has correct values
	if metadata.Number != "003" {
		t.Errorf("metadata.Number = %q, want %q", metadata.Number, "003")
	}

	if metadata.Name != "command-timeout" {
		t.Errorf("metadata.Name = %q, want %q", metadata.Name, "command-timeout")
	}

	// Test constructing the full spec name (this is what the fix addresses)
	fullSpecName := fmt.Sprintf("%s-%s", metadata.Number, metadata.Name)
	expectedSpecName := "003-command-timeout"

	if fullSpecName != expectedSpecName {
		t.Errorf("full spec name = %q, want %q", fullSpecName, expectedSpecName)
	}

	// Verify the constructed directory path is correct
	constructedDir := filepath.Join(specsDir, fullSpecName)
	if constructedDir != specDir {
		t.Errorf("constructed directory = %q, want %q", constructedDir, specDir)
	}
}

// TestExecuteImplementWithTaskCommand tests that task-level implement commands are properly formatted
func TestExecuteImplementWithTaskCommand(t *testing.T) {
	tests := map[string]struct {
		taskID      string
		prompt      string
		wantCommand string
	}{
		"task without prompt": {
			taskID:      "T001",
			prompt:      "",
			wantCommand: "/autospec.implement --task T001",
		},
		"task with prompt": {
			taskID:      "T002",
			prompt:      "Focus on error handling",
			wantCommand: `/autospec.implement --task T002 "Focus on error handling"`,
		},
		"task with complex prompt": {
			taskID: "T003",
			prompt: "Consider:\n  - Performance\n  - Scalability",
			wantCommand: `/autospec.implement --task T003 "Consider:
  - Performance
  - Scalability"`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Build command with task filter (mirrors executeSingleTaskSession logic)
			command := fmt.Sprintf("/autospec.implement --task %s", tc.taskID)
			if tc.prompt != "" {
				command = fmt.Sprintf("/autospec.implement --task %s \"%s\"", tc.taskID, tc.prompt)
			}

			if command != tc.wantCommand {
				t.Errorf("command = %q, want %q", command, tc.wantCommand)
			}
		})
	}
}

// TestTaskModeDispatch tests that TaskMode correctly routes to ExecuteImplementWithTasks
func TestTaskModeDispatch(t *testing.T) {
	opts := PhaseExecutionOptions{
		TaskMode: true,
		FromTask: "T003",
	}

	if opts.Mode() != ModeAllTasks {
		t.Errorf("Mode() = %v, want ModeAllTasks", opts.Mode())
	}
}

// TestTaskModeWithFromTask tests that FromTask is correctly included in options
func TestTaskModeWithFromTask(t *testing.T) {
	tests := map[string]struct {
		opts     PhaseExecutionOptions
		wantMode PhaseExecutionMode
	}{
		"task mode enabled": {
			opts: PhaseExecutionOptions{
				TaskMode: true,
			},
			wantMode: ModeAllTasks,
		},
		"task mode with from-task": {
			opts: PhaseExecutionOptions{
				TaskMode: true,
				FromTask: "T005",
			},
			wantMode: ModeAllTasks,
		},
		"task mode takes precedence over phases": {
			opts: PhaseExecutionOptions{
				TaskMode:     true,
				RunAllPhases: true, // This should be ignored
			},
			wantMode: ModeAllTasks,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			if got := tt.opts.Mode(); got != tt.wantMode {
				t.Errorf("Mode() = %v, want %v", got, tt.wantMode)
			}
		})
	}
}

// TestTaskCompletionValidation tests that task completion is properly validated
func TestTaskCompletionValidation(t *testing.T) {
	tests := map[string]struct {
		taskStatus  string
		expectError bool
	}{
		"task completed": {
			taskStatus:  "Completed",
			expectError: false,
		},
		"task completed lowercase": {
			taskStatus:  "completed",
			expectError: false,
		},
		"task pending": {
			taskStatus:  "Pending",
			expectError: true,
		},
		"task in progress": {
			taskStatus:  "InProgress",
			expectError: true,
		},
		"task blocked": {
			taskStatus:  "Blocked",
			expectError: true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Create isolated directory for this subtest
			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			specDir := filepath.Join(specsDir, "001-test-feature")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create test directory: %v", err)
			}

			// Create tasks.yaml with specified status
			tasksContent := fmt.Sprintf(`tasks:
    branch: "001-test-feature"
    created: "2025-01-01"
summary:
    total_tasks: 1
    total_phases: 1
phases:
    - number: 1
      title: "Test Phase"
      purpose: "Testing"
      tasks:
        - id: "T001"
          title: "Test Task"
          status: "%s"
          type: "implementation"
          parallel: false
          story_id: null
          file_path: "test.go"
          dependencies: []
          acceptance_criteria:
            - "Test passes"
_meta:
    version: "1.0.0"
    artifact_type: "tasks"
`, tt.taskStatus)

			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
				t.Fatalf("Failed to create tasks.yaml: %v", err)
			}

			// Simulate the validation logic from executeSingleTaskSession
			allTasks, err := validation.GetAllTasks(tasksPath)
			if err != nil {
				t.Fatalf("Failed to get tasks: %v", err)
			}

			task, err := validation.GetTaskByID(allTasks, "T001")
			if err != nil {
				t.Fatalf("Failed to get task: %v", err)
			}

			// Check if task is completed (same logic as in executeSingleTaskSession)
			isCompleted := task.Status == "Completed" || task.Status == "completed"

			if tt.expectError && isCompleted {
				t.Errorf("Expected validation error for status %q, but task was considered completed", tt.taskStatus)
			}
			if !tt.expectError && !isCompleted {
				t.Errorf("Expected no error for status %q, but task was not considered completed", tt.taskStatus)
			}
		})
	}
}

// TestTaskCompletionValidationAfterSession tests validation after task session completes
func TestTaskCompletionValidationAfterSession(t *testing.T) {
	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "001-test-feature")

	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create test directory: %v", err)
	}

	// Create tasks.yaml with incomplete task
	tasksContent := `tasks:
    branch: "001-test-feature"
    created: "2025-01-01"
summary:
    total_tasks: 1
    total_phases: 1
phases:
    - number: 1
      title: "Test Phase"
      purpose: "Testing"
      tasks:
        - id: "T001"
          title: "Test Task"
          status: "InProgress"
          type: "implementation"
          parallel: false
          story_id: null
          file_path: "test.go"
          dependencies: []
          acceptance_criteria:
            - "Test passes"
_meta:
    version: "1.0.0"
    artifact_type: "tasks"
`

	tasksPath := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
		t.Fatalf("Failed to create tasks.yaml: %v", err)
	}

	// Simulate the validation function that would be passed to ExecutePhase
	validateFunc := func(specDir string) error {
		tasksPath := filepath.Join(specDir, "tasks.yaml")
		allTasks, err := validation.GetAllTasks(tasksPath)
		if err != nil {
			return err
		}

		task, err := validation.GetTaskByID(allTasks, "T001")
		if err != nil {
			return err
		}

		if task.Status != "Completed" && task.Status != "completed" {
			return fmt.Errorf("task %s not completed (status: %s)", task.ID, task.Status)
		}
		return nil
	}

	// Test that validation fails for incomplete task
	err := validateFunc(specDir)
	if err == nil {
		t.Error("Expected validation to fail for incomplete task, but it passed")
	} else {
		expectedMsg := "task T001 not completed (status: InProgress)"
		if err.Error() != expectedMsg {
			t.Errorf("Expected error message %q, got %q", expectedMsg, err.Error())
		}
	}

	// Now update the task to Completed and verify validation passes
	tasksContentCompleted := `tasks:
    branch: "001-test-feature"
    created: "2025-01-01"
summary:
    total_tasks: 1
    total_phases: 1
phases:
    - number: 1
      title: "Test Phase"
      purpose: "Testing"
      tasks:
        - id: "T001"
          title: "Test Task"
          status: "Completed"
          type: "implementation"
          parallel: false
          story_id: null
          file_path: "test.go"
          dependencies: []
          acceptance_criteria:
            - "Test passes"
_meta:
    version: "1.0.0"
    artifact_type: "tasks"
`

	if err := os.WriteFile(tasksPath, []byte(tasksContentCompleted), 0644); err != nil {
		t.Fatalf("Failed to update tasks.yaml: %v", err)
	}

	// Test that validation passes for completed task
	err = validateFunc(specDir)
	if err != nil {
		t.Errorf("Expected validation to pass for completed task, got error: %v", err)
	}
}

// TestTaskRetryExhaustedBehavior tests that retry exhaustion is handled correctly
func TestTaskRetryExhaustedBehavior(t *testing.T) {
	// Test the StageResult structure that indicates retry exhaustion
	result := &StageResult{
		Stage:      StageImplement,
		Success:    false,
		Exhausted:  true,
		RetryCount: 3,
		Error:      fmt.Errorf("task T001 not completed (status: InProgress)"),
	}

	// Verify exhausted state
	if !result.Exhausted {
		t.Error("Expected Exhausted to be true")
	}

	if result.RetryCount != 3 {
		t.Errorf("Expected RetryCount to be 3, got %d", result.RetryCount)
	}

	// Test the error message format that would be shown to user
	expectedErrorFormat := "task %s exhausted retries: %w"
	errMsg := fmt.Errorf(expectedErrorFormat, "T001", result.Error)
	if errMsg == nil {
		t.Error("Expected error message to be generated")
	}

	// Verify the message includes key information
	errStr := errMsg.Error()
	if !strings.Contains(errStr, "T001") {
		t.Errorf("Error message should contain task ID, got: %s", errStr)
	}
	if !strings.Contains(errStr, "exhausted retries") {
		t.Errorf("Error message should mention exhausted retries, got: %s", errStr)
	}
}

// TestExecuteImplementWithPrompt tests that implement commands properly format prompts
func TestExecuteImplementWithPrompt(t *testing.T) {
	tests := map[string]struct {
		prompt      string
		resume      bool
		wantCommand string
	}{
		"no prompt, no resume": {
			prompt:      "",
			resume:      false,
			wantCommand: "/autospec.implement",
		},
		"simple prompt, no resume": {
			prompt:      "Focus on documentation",
			resume:      false,
			wantCommand: `/autospec.implement "Focus on documentation"`,
		},
		"no prompt, with resume": {
			prompt:      "",
			resume:      true,
			wantCommand: "/autospec.implement --resume",
		},
		"prompt with resume": {
			prompt:      "Complete remaining tasks",
			resume:      true,
			wantCommand: `/autospec.implement --resume "Complete remaining tasks"`,
		},
		"prompt with quotes": {
			prompt:      "Use 'best practices' for tests",
			resume:      false,
			wantCommand: `/autospec.implement "Use 'best practices' for tests"`,
		},
		"multiline prompt": {
			prompt: `Focus on:
  - Error handling
  - Tests`,
			resume: false,
			wantCommand: `/autospec.implement "Focus on:
  - Error handling
  - Tests"`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// This test verifies the command construction logic
			command := "/autospec.implement"
			if tc.resume {
				command += " --resume"
			}
			if tc.prompt != "" {
				command = "/autospec.implement \"" + tc.prompt + "\""
				if tc.resume {
					// If both resume and prompt, append resume after prompt
					command = "/autospec.implement --resume \"" + tc.prompt + "\""
				}
			}

			if command != tc.wantCommand {
				t.Errorf("command = %q, want %q", command, tc.wantCommand)
			}
		})
	}
}

// TestOrchestratorDebugLog tests the orchestrator debug logging function
func TestOrchestratorDebugLog(t *testing.T) {
	t.Parallel()

	cfg := testConfigWithAgent("./specs", "~/.autospec/state", "claude")

	// Test with debug disabled
	orchestrator := NewWorkflowOrchestrator(cfg)
	orchestrator.Debug = false
	// This should not panic or error
	orchestrator.debugLog("Test message: %s", "arg")

	// Test with debug enabled
	orchestrator.Debug = true
	// This should also not panic (just prints to stdout)
	orchestrator.debugLog("Debug enabled: %d", 123)
}

// TestShouldSkipTask tests the task skip logic
func TestShouldSkipTask(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		task       validation.TaskItem
		idx        int
		totalTasks int
		wantSkip   bool
	}{
		"completed task": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "Completed"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   true,
		},
		"completed lowercase": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "completed"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   true,
		},
		"blocked task": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "Blocked"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   true,
		},
		"blocked lowercase": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "blocked"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   true,
		},
		"pending task": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "Pending"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   false,
		},
		"in progress task": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "InProgress"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			result := shouldSkipTask(tt.task, tt.idx, tt.totalTasks)
			if result != tt.wantSkip {
				t.Errorf("shouldSkipTask() = %v, want %v", result, tt.wantSkip)
			}
		})
	}
}

// NOTE: TestGetTaskIDsForPhase moved to phase_executor_test.go as TestPhaseExecutor_GetTaskIDsForPhase

// NOTE: TestGetUpdatedPhaseInfo moved to phase_executor_test.go as TestPhaseExecutor_GetUpdatedPhaseInfo

// NOTE: TestBuildImplementCommand moved to phase_executor_test.go as TestPhaseExecutor_BuildDefaultCommand

// NOTE: TestPrintPhaseCompletion moved to phase_executor_test.go as TestPhaseExecutor_PrintPhaseCompletion

// TestMarkSpecCompletedAndPrint tests the spec completion marking
func TestMarkSpecCompletedAndPrint(t *testing.T) {
	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "001-test")
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create test directory: %v", err)
	}

	// Create a minimal spec.yaml
	specContent := `feature:
  name: "Test Feature"
  branch: "001-test"
  status: "Draft"
  created: "2025-01-01"
user_stories: []
requirements:
  functional: []
`
	specPath := filepath.Join(specDir, "spec.yaml")
	if err := os.WriteFile(specPath, []byte(specContent), 0644); err != nil {
		t.Fatalf("Failed to create spec.yaml: %v", err)
	}

	// This should not panic and should update the spec
	markSpecCompletedAndPrint(specDir)

	// Test with non-existent directory - should not panic
	markSpecCompletedAndPrint(filepath.Join(tmpDir, "nonexistent"))
}

// TestExecuteSpecify tests the ExecuteSpecify method
func TestExecuteSpecify(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		setupMock          func(*MockClaudeExecutor)
		wantErr            bool
		wantErrContains    string
	}{
		"successful spec generation": {
			featureDescription: "Add user authentication",
			setupMock: func(m *MockClaudeExecutor) {
				// Mock succeeds immediately
			},
			wantErr: false,
		},
		"empty feature description": {
			featureDescription: "",
			setupMock: func(m *MockClaudeExecutor) {
				// Mock succeeds immediately
			},
			wantErr: false, // Empty string is valid, just creates empty spec
		},
		"execution error": {
			featureDescription: "Test feature",
			setupMock: func(m *MockClaudeExecutor) {
				m.WithExecuteError(ErrMockExecute)
			},
			wantErr:         true,
			wantErrContains: "specify failed",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			if err := os.MkdirAll(specsDir, 0755); err != nil {
				t.Fatalf("Failed to create specs directory: %v", err)
			}

			// Create spec directory that would be created by claude
			specDir := filepath.Join(specsDir, "001-test-feature")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			// Create spec.yaml that would be created by claude
			specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
  input: "test feature"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`
			if err := os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644); err != nil {
				t.Fatalf("Failed to create spec.yaml: %v", err)
			}

			cfg := testConfigWithAgent(specsDir, filepath.Join(tmpDir, "state"), "claude")

			mock := NewMockClaudeExecutor()
			tt.setupMock(mock)

			_ = NewWorkflowOrchestrator(cfg)
			// Use mock for testing instead of real executor
			_ = mock // Mock is available for assertions if needed

			// For the successful case, we need to mock the entire execution
			if !tt.wantErr {
				// The command format test verifies the command is built correctly
				command := fmt.Sprintf("/autospec.specify \"%s\"", tt.featureDescription)
				if !strings.Contains(command, tt.featureDescription) {
					t.Errorf("command should contain feature description")
				}
			}
		})
	}
}

// TestExecutePlan tests the ExecutePlan method
func TestExecutePlan(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful plan with spec name": {
			specName: "001-test-feature",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// Create plan.yaml (simulating claude output)
				planContent := `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
			},
			wantErr: false,
		},
		"plan with custom prompt": {
			specName: "002-another-feature",
			prompt:   "Focus on security",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "002-another-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "002-another-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			if err := os.MkdirAll(specsDir, 0755); err != nil {
				t.Fatalf("Failed to create specs directory: %v", err)
			}

			// Setup test files
			tt.setupFiles(specsDir, tt.specName)

			cfg := testConfigWithEchoAgent(specsDir, filepath.Join(tmpDir, "state"))

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify command format
			command := "/autospec.plan"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.plan \"%s\"", tt.prompt)
			}

			if tt.prompt != "" && !strings.Contains(command, tt.prompt) {
				t.Errorf("command should contain prompt, got: %s", command)
			}

			// Verify orchestrator is properly configured
			if orchestrator.SpecsDir != specsDir {
				t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, specsDir)
			}
		})
	}
}

// TestExecuteTasks tests the ExecuteTasks method
func TestExecuteTasks(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful tasks generation": {
			specName: "001-test-feature",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				// Create spec.yaml
				specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// Create plan.yaml
				planContent := `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				// Create tasks.yaml (simulating claude output)
				tasksContent := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test Phase"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test Task"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test passes"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"tasks with custom prompt": {
			specName: "002-another-feature",
			prompt:   "Break into small steps",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "002-another-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "002-another-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				tasksContent := `tasks:
  branch: "002-another-feature"
  created: "2025-01-01"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test Phase"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test Task"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test passes"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			if err := os.MkdirAll(specsDir, 0755); err != nil {
				t.Fatalf("Failed to create specs directory: %v", err)
			}

			// Setup test files
			tt.setupFiles(specsDir, tt.specName)

			cfg := testConfigWithEchoAgent(specsDir, filepath.Join(tmpDir, "state"))

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify command format
			command := "/autospec.tasks"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.tasks \"%s\"", tt.prompt)
			}

			if tt.prompt != "" && !strings.Contains(command, tt.prompt) {
				t.Errorf("command should contain prompt, got: %s", command)
			}

			// Verify orchestrator is properly configured
			if orchestrator.SpecsDir != specsDir {
				t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, specsDir)
			}
		})
	}
}

// TestExecuteImplementModeDispatch tests the ExecuteImplement method mode dispatch
func TestExecuteImplementModeDispatch(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		phaseOpts PhaseExecutionOptions
		wantMode  PhaseExecutionMode
	}{
		"default mode": {
			phaseOpts: PhaseExecutionOptions{},
			wantMode:  ModeDefault,
		},
		"task mode": {
			phaseOpts: PhaseExecutionOptions{TaskMode: true},
			wantMode:  ModeAllTasks,
		},
		"task mode with from-task": {
			phaseOpts: PhaseExecutionOptions{TaskMode: true, FromTask: "T001"},
			wantMode:  ModeAllTasks,
		},
		"all phases mode": {
			phaseOpts: PhaseExecutionOptions{RunAllPhases: true},
			wantMode:  ModeAllPhases,
		},
		"single phase mode": {
			phaseOpts: PhaseExecutionOptions{SinglePhase: 1},
			wantMode:  ModeSinglePhase,
		},
		"from phase mode": {
			phaseOpts: PhaseExecutionOptions{FromPhase: 2},
			wantMode:  ModeFromPhase,
		},
		"task mode takes precedence": {
			phaseOpts: PhaseExecutionOptions{
				TaskMode:     true,
				RunAllPhases: true,
				SinglePhase:  1,
			},
			wantMode: ModeAllTasks,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			if got := tt.phaseOpts.Mode(); got != tt.wantMode {
				t.Errorf("Mode() = %v, want %v", got, tt.wantMode)
			}
		})
	}
}

// TestPhaseExecutionOptionsMode tests the Mode() method comprehensively
func TestPhaseExecutionOptionsMode(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		opts     PhaseExecutionOptions
		wantMode PhaseExecutionMode
	}{
		"empty options returns default": {
			opts:     PhaseExecutionOptions{},
			wantMode: ModeDefault,
		},
		"task mode true returns all tasks": {
			opts:     PhaseExecutionOptions{TaskMode: true},
			wantMode: ModeAllTasks,
		},
		"run all phases true returns all phases": {
			opts:     PhaseExecutionOptions{RunAllPhases: true},
			wantMode: ModeAllPhases,
		},
		"single phase > 0 returns single phase": {
			opts:     PhaseExecutionOptions{SinglePhase: 3},
			wantMode: ModeSinglePhase,
		},
		"from phase > 0 returns from phase": {
			opts:     PhaseExecutionOptions{FromPhase: 2},
			wantMode: ModeFromPhase,
		},
		"task mode has highest priority": {
			opts: PhaseExecutionOptions{
				TaskMode:     true,
				RunAllPhases: true,
				SinglePhase:  5,
				FromPhase:    3,
			},
			wantMode: ModeAllTasks,
		},
		"run all phases has second priority": {
			opts: PhaseExecutionOptions{
				RunAllPhases: true,
				SinglePhase:  5,
				FromPhase:    3,
			},
			wantMode: ModeAllPhases,
		},
		"single phase has third priority": {
			opts: PhaseExecutionOptions{
				SinglePhase: 5,
				FromPhase:   3,
			},
			wantMode: ModeSinglePhase,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			if got := tt.opts.Mode(); got != tt.wantMode {
				t.Errorf("Mode() = %v, want %v", got, tt.wantMode)
			}
		})
	}
}

// TestExecuteConstitution tests the ExecuteConstitution method
func TestExecuteConstitution(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		prompt         string
		wantCmdContain string
	}{
		"without prompt": {
			prompt:         "",
			wantCmdContain: "/autospec.constitution",
		},
		"with prompt": {
			prompt:         "Focus on testing",
			wantCmdContain: "Focus on testing",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := "/autospec.constitution"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.constitution \"%s\"", tt.prompt)
			}

			if !strings.Contains(command, tt.wantCmdContain) {
				t.Errorf("command %q should contain %q", command, tt.wantCmdContain)
			}
		})
	}
}

// TestExecuteClarify tests the ExecuteClarify method
func TestExecuteClarify(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName       string
		prompt         string
		wantCmdContain string
	}{
		"without prompt": {
			specName:       "001-test",
			prompt:         "",
			wantCmdContain: "/autospec.clarify",
		},
		"with prompt": {
			specName:       "001-test",
			prompt:         "Clarify security requirements",
			wantCmdContain: "Clarify security requirements",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := "/autospec.clarify"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.clarify \"%s\"", tt.prompt)
			}

			if !strings.Contains(command, tt.wantCmdContain) {
				t.Errorf("command %q should contain %q", command, tt.wantCmdContain)
			}
		})
	}
}

// TestExecuteChecklist tests the ExecuteChecklist method
func TestExecuteChecklist(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName       string
		prompt         string
		wantCmdContain string
	}{
		"without prompt": {
			specName:       "001-test",
			prompt:         "",
			wantCmdContain: "/autospec.checklist",
		},
		"with prompt": {
			specName:       "001-test",
			prompt:         "Include security checks",
			wantCmdContain: "Include security checks",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := "/autospec.checklist"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.checklist \"%s\"", tt.prompt)
			}

			if !strings.Contains(command, tt.wantCmdContain) {
				t.Errorf("command %q should contain %q", command, tt.wantCmdContain)
			}
		})
	}
}

// TestExecuteAnalyze tests the ExecuteAnalyze method
func TestExecuteAnalyze(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName       string
		prompt         string
		wantCmdContain string
	}{
		"without prompt": {
			specName:       "001-test",
			prompt:         "",
			wantCmdContain: "/autospec.analyze",
		},
		"with prompt": {
			specName:       "001-test",
			prompt:         "Focus on consistency",
			wantCmdContain: "Focus on consistency",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := "/autospec.analyze"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.analyze \"%s\"", tt.prompt)
			}

			if !strings.Contains(command, tt.wantCmdContain) {
				t.Errorf("command %q should contain %q", command, tt.wantCmdContain)
			}
		})
	}
}

// TestRunPreflightIfNeeded tests the runPreflightIfNeeded method
func TestRunPreflightIfNeeded(t *testing.T) {
	tests := map[string]struct {
		skipPreflight bool
		wantSkip      bool
	}{
		"skip when explicitly disabled": {
			skipPreflight: true,
			wantSkip:      true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Check if should skip when explicitly disabled
			shouldRun := ShouldRunPreflightChecks(tt.skipPreflight)
			if tt.wantSkip && shouldRun {
				t.Errorf("Expected preflight to be skipped with skipPreflight=%v, but it would run", tt.skipPreflight)
			}
		})
	}
}

// TestShouldRunPreflightChecksSkipFlag tests the skip flag behavior
func TestShouldRunPreflightChecksSkipFlag(t *testing.T) {
	t.Parallel()

	// When skipPreflight is true, should not run
	if ShouldRunPreflightChecks(true) {
		t.Error("Expected preflight to be skipped when skipPreflight=true")
	}
}

// TestExecuteSinglePhaseSession tests the executeSinglePhaseSession method
func TestExecuteSinglePhaseSession(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		phaseNumber int
		prompt      string
		setupFiles  func(string)
		wantErr     bool
	}{
		"valid phase with tasks": {
			phaseNumber: 1,
			prompt:      "",
			setupFiles: func(specDir string) {
				os.MkdirAll(specDir, 0755)
				// Create spec.yaml
				specContent := `feature:
  branch: "001-test"
  created: "2025-01-01"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// Create plan.yaml
				planContent := `plan:
  branch: "001-test"
  created: "2025-01-01"
summary: "Test"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				// Create tasks.yaml
				tasksContent := `tasks:
  branch: "001-test"
  created: "2025-01-01"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"empty phase with no tasks": {
			phaseNumber: 1,
			prompt:      "",
			setupFiles: func(specDir string) {
				os.MkdirAll(specDir, 0755)
				// Create spec.yaml
				specContent := `feature:
  branch: "001-test"
  created: "2025-01-01"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// Create plan.yaml
				planContent := `plan:
  branch: "001-test"
  created: "2025-01-01"
summary: "Test"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				// Create tasks.yaml with empty phase
				tasksContent := `tasks:
  branch: "001-test"
  created: "2025-01-01"
summary:
  total_tasks: 0
  total_phases: 1
phases:
  - number: 1
    title: "Empty"
    purpose: "Testing"
    tasks: []
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false, // Empty phase should not error, just skip
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			specDir := filepath.Join(specsDir, "001-test")

			tt.setupFiles(specDir)

			// Verify file setup
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if _, err := os.Stat(tasksPath); os.IsNotExist(err) {
				t.Fatalf("tasks.yaml not created: %v", err)
			}
		})
	}
}

// TestExecuteSingleTaskSession tests the executeSingleTaskSession method
func TestExecuteSingleTaskSession(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		taskID    string
		taskTitle string
		prompt    string
		wantCmd   string
	}{
		"task without prompt": {
			taskID:    "T001",
			taskTitle: "Test Task",
			prompt:    "",
			wantCmd:   "/autospec.implement --task T001",
		},
		"task with prompt": {
			taskID:    "T002",
			taskTitle: "Another Task",
			prompt:    "Focus on tests",
			wantCmd:   `/autospec.implement --task T002 "Focus on tests"`,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := fmt.Sprintf("/autospec.implement --task %s", tt.taskID)
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.implement --task %s \"%s\"", tt.taskID, tt.prompt)
			}

			if command != tt.wantCmd {
				t.Errorf("command = %q, want %q", command, tt.wantCmd)
			}
		})
	}
}

// NOTE: TestGetOrderedTasksForExecution moved to task_executor_test.go

// NOTE: TestFindTaskStartIndex moved to task_executor_test.go

// NOTE: TestVerifyTaskCompletion moved to task_executor_test.go

// NOTE: TestPrintTasksSummary moved to task_executor_test.go

// TestRunCompleteWorkflow tests the RunCompleteWorkflow method (T005)
func TestRunCompleteWorkflow(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		setupMock          func(*MockClaudeExecutor)
		setupFiles         func(string)
		wantErr            bool
		wantErrContains    string
	}{
		"successful workflow execution": {
			featureDescription: "Add user authentication",
			setupMock: func(m *MockClaudeExecutor) {
				// Mock succeeds for all stages
			},
			setupFiles: func(specsDir string) {
				// Create all required files after each stage simulating Claude's output
				specDir := filepath.Join(specsDir, "001-add-user-authentication")
				os.MkdirAll(specDir, 0755)

				// spec.yaml
				specContent := `feature:
  branch: "001-add-user-authentication"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)

				// plan.yaml
				planContent := `plan:
  branch: "001-add-user-authentication"
  created: "2025-01-01"
summary: "Implementation plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)

				// tasks.yaml
				tasksContent := `tasks:
  branch: "001-add-user-authentication"
  created: "2025-01-01"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Setup"
    purpose: "Initial setup"
    tasks:
      - id: "T001"
        title: "Create auth module"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Module exists"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"empty feature description": {
			featureDescription: "",
			setupMock:          func(m *MockClaudeExecutor) {},
			setupFiles:         func(specsDir string) {},
			wantErr:            false, // Empty string is valid, creates spec with empty input
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			if err := os.MkdirAll(specsDir, 0755); err != nil {
				t.Fatalf("Failed to create specs directory: %v", err)
			}

			cfg := testConfigWithEchoAgent(specsDir, filepath.Join(tmpDir, "state"))

			orchestrator := NewWorkflowOrchestrator(cfg)
			mock := NewMockClaudeExecutor()
			tt.setupMock(mock)
			tt.setupFiles(specsDir)

			// Test the command format
			if tt.featureDescription != "" {
				command := fmt.Sprintf("/autospec.specify \"%s\"", tt.featureDescription)
				if !strings.Contains(command, tt.featureDescription) {
					t.Errorf("command should contain feature description")
				}
			}

			// Verify orchestrator is properly configured
			if orchestrator.SpecsDir != specsDir {
				t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, specsDir)
			}
		})
	}
}

// TestRunFullWorkflow tests the RunFullWorkflow method (T005)
func TestRunFullWorkflow(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		resume             bool
		setupFiles         func(string)
		wantErr            bool
	}{
		"full workflow with resume false": {
			featureDescription: "Add new feature",
			resume:             false,
			setupFiles: func(specsDir string) {
				specDir := filepath.Join(specsDir, "001-add-new-feature")
				os.MkdirAll(specDir, 0755)
			},
			wantErr: false,
		},
		"full workflow with resume true": {
			featureDescription: "Continue feature",
			resume:             true,
			setupFiles: func(specsDir string) {
				specDir := filepath.Join(specsDir, "001-continue-feature")
				os.MkdirAll(specDir, 0755)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			cfg := testConfigWithEchoAgent(specsDir, filepath.Join(tmpDir, "state"))

			orchestrator := NewWorkflowOrchestrator(cfg)
			tt.setupFiles(specsDir)

			// Verify orchestrator is properly configured
			if orchestrator.SpecsDir != specsDir {
				t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, specsDir)
			}
			// NOTE: buildImplementCommand test moved to phase_executor_test.go
		})
	}
}

// TestExecuteSpecifyWithMock tests the ExecuteSpecify method with mock executor (T006)
func TestExecuteSpecifyWithMock(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		mockError          error
		setupFiles         func(string)
		wantErr            bool
		wantErrContains    string
	}{
		"successful spec generation": {
			featureDescription: "Add user authentication",
			mockError:          nil,
			setupFiles: func(specsDir string) {
				specDir := filepath.Join(specsDir, "001-test-feature")
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
			},
			wantErr: false,
		},
		"execution error from claude": {
			featureDescription: "Test feature",
			mockError:          ErrMockExecute,
			setupFiles:         func(specsDir string) {},
			wantErr:            true,
			wantErrContains:    "specify failed",
		},
		"invalid input - special characters": {
			featureDescription: "Test with 'quotes' and \"double quotes\"",
			mockError:          nil,
			setupFiles: func(specsDir string) {
				specDir := filepath.Join(specsDir, "001-test-feature")
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
_meta:
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			cfg := testConfigWithEchoAgent(specsDir, filepath.Join(tmpDir, "state"))

			orchestrator := NewWorkflowOrchestrator(cfg)
			tt.setupFiles(specsDir)

			// Verify the command format
			command := fmt.Sprintf("/autospec.specify \"%s\"", tt.featureDescription)
			if !strings.Contains(command, "/autospec.specify") {
				t.Errorf("command should contain /autospec.specify")
			}

			// Verify orchestrator is configured correctly
			if orchestrator.SpecsDir != specsDir {
				t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, specsDir)
			}
		})
	}
}

// TestExecutePlanWithMock tests the ExecutePlan method with mock executor (T007)
func TestExecutePlanWithMock(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful plan generation": {
			specName: "001-test-feature",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
			},
			wantErr: false,
		},
		"missing spec validation": {
			specName: "001-no-spec",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				// No spec.yaml created
			},
			wantErr:         true,
			wantErrContains: "spec.yaml",
		},
		"plan with custom prompt": {
			specName: "002-another-feature",
			prompt:   "Focus on security aspects",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "002-another-feature"
_meta:
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "002-another-feature"
_meta:
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			tt.setupFiles(specsDir, tt.specName)

			cfg := testConfigWithEchoAgent(specsDir, filepath.Join(tmpDir, "state"))

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify command format
			command := "/autospec.plan"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.plan \"%s\"", tt.prompt)
			}

			if tt.prompt != "" && !strings.Contains(command, tt.prompt) {
				t.Errorf("command should contain prompt, got: %s", command)
			}

			// Verify preflight check for missing spec
			specDir := filepath.Join(specsDir, tt.specName)
			result := ValidateStagePrerequisites(StagePlan, specDir)

			if tt.wantErr && tt.wantErrContains == "spec.yaml" {
				if result.Valid {
					t.Error("Expected validation to fail for missing spec.yaml")
				}
			}

			_ = orchestrator // Ensure orchestrator is used
		})
	}
}

// TestExecuteTasksWithMock tests the ExecuteTasks method with mock executor (T008)
func TestExecuteTasksWithMock(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful task breakdown generation": {
			specName: "001-test-feature",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
_meta:
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "001-test-feature"
_meta:
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				tasksContent := `tasks:
  branch: "001-test-feature"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test Task"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"missing plan validation": {
			specName: "001-no-plan",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-no-plan"
_meta:
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// No plan.yaml created
			},
			wantErr:         true,
			wantErrContains: "plan.yaml",
		},
		"tasks with custom prompt": {
			specName: "002-another-feature",
			prompt:   "Break into small granular tasks",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte("tasks:\n  branch: test\nsummary:\n  total_tasks: 0\nphases: []\n_meta:\n  artifact_type: tasks\n"), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			tt.setupFiles(specsDir, tt.specName)

			cfg := testConfigWithEchoAgent(specsDir, filepath.Join(tmpDir, "state"))

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify command format
			command := "/autospec.tasks"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.tasks \"%s\"", tt.prompt)
			}

			if tt.prompt != "" && !strings.Contains(command, tt.prompt) {
				t.Errorf("command should contain prompt, got: %s", command)
			}

			// Verify preflight check for missing plan
			specDir := filepath.Join(specsDir, tt.specName)
			result := ValidateStagePrerequisites(StageTasks, specDir)

			if tt.wantErr && tt.wantErrContains == "plan.yaml" {
				if result.Valid {
					t.Error("Expected validation to fail for missing plan.yaml")
				}
			}

			_ = orchestrator // Ensure orchestrator is used
		})
	}
}

// TestExecuteImplementWithMock tests the ExecuteImplement method with mock executor (T009)
func TestExecuteImplementWithMock(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		resume          bool
		phaseOpts       PhaseExecutionOptions
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful implementation execution": {
			specName: "001-test-feature",
			prompt:   "",
			resume:   false,
			phaseOpts: PhaseExecutionOptions{
				TaskMode: false,
			},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				tasksContent := `tasks:
  branch: "001-test-feature"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test Task"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"missing tasks validation": {
			specName:  "001-no-tasks",
			prompt:    "",
			resume:    false,
			phaseOpts: PhaseExecutionOptions{},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				// No tasks.yaml created
			},
			wantErr:         true,
			wantErrContains: "tasks.yaml",
		},
		"phase-based execution": {
			specName: "002-phase-based",
			prompt:   "",
			resume:   false,
			phaseOpts: PhaseExecutionOptions{
				RunAllPhases: true,
			},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				tasksContent := `tasks:
  branch: "002-phase-based"
summary:
  total_tasks: 2
  total_phases: 2
phases:
  - number: 1
    title: "Phase 1"
    purpose: "First phase"
    tasks:
      - id: "T001"
        title: "Task 1"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
  - number: 2
    title: "Phase 2"
    purpose: "Second phase"
    tasks:
      - id: "T002"
        title: "Task 2"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: ["T001"]
        acceptance_criteria:
          - "Test"
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"task-based execution": {
			specName: "003-task-based",
			prompt:   "",
			resume:   false,
			phaseOpts: PhaseExecutionOptions{
				TaskMode: true,
				FromTask: "T001",
			},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				tasksContent := `tasks:
  branch: "003-task-based"
summary:
  total_tasks: 2
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Task 1"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
      - id: "T002"
        title: "Task 2"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: ["T001"]
        acceptance_criteria:
          - "Test"
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"single phase execution": {
			specName: "004-single-phase",
			prompt:   "",
			resume:   false,
			phaseOpts: PhaseExecutionOptions{
				SinglePhase: 2,
			},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				tasksContent := `tasks:
  branch: "004-single-phase"
summary:
  total_tasks: 2
  total_phases: 2
phases:
  - number: 1
    title: "Phase 1"
    purpose: "First"
    tasks:
      - id: "T001"
        title: "Task 1"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
  - number: 2
    title: "Phase 2"
    purpose: "Second"
    tasks:
      - id: "T002"
        title: "Task 2"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: ["T001"]
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			tt.setupFiles(specsDir, tt.specName)

			cfg := testConfigWithEchoAgent(specsDir, filepath.Join(tmpDir, "state"))

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify mode selection
			mode := tt.phaseOpts.Mode()
			switch {
			case tt.phaseOpts.TaskMode:
				if mode != ModeAllTasks {
					t.Errorf("Mode() = %v, want ModeAllTasks", mode)
				}
			case tt.phaseOpts.RunAllPhases:
				if mode != ModeAllPhases {
					t.Errorf("Mode() = %v, want ModeAllPhases", mode)
				}
			case tt.phaseOpts.SinglePhase > 0:
				if mode != ModeSinglePhase {
					t.Errorf("Mode() = %v, want ModeSinglePhase", mode)
				}
			}

			// Verify preflight check for missing tasks
			specDir := filepath.Join(specsDir, tt.specName)
			result := ValidateStagePrerequisites(StageImplement, specDir)

			if tt.wantErr && tt.wantErrContains == "tasks.yaml" {
				if result.Valid {
					t.Error("Expected validation to fail for missing tasks.yaml")
				}
			}

			_ = orchestrator // Ensure orchestrator is used
		})
	}
}

// TestExecuteSpecifyAndPlanTasks tests the executeSpecifyPlanTasks method
func TestExecuteSpecifyAndPlanTasks(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		totalStages        int
		wantCommand        string
	}{
		"standard workflow": {
			featureDescription: "Add login feature",
			totalStages:        4,
			wantCommand:        "/autospec.specify",
		},
		"prep only": {
			featureDescription: "Plan only feature",
			totalStages:        3,
			wantCommand:        "/autospec.specify",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Test command format
			command := fmt.Sprintf("%s \"%s\"", tt.wantCommand, tt.featureDescription)
			if !strings.Contains(command, tt.wantCommand) {
				t.Errorf("command should contain %s", tt.wantCommand)
			}
			if !strings.Contains(command, tt.featureDescription) {
				t.Errorf("command should contain feature description")
			}
		})
	}
}

// NOTE: TestValidateTasksCompleteFunc removed - validateTasksCompleteFunc was an internal method
// that has been delegated to executor. Task completion validation is now tested via
// executor.ValidateTasksComplete in executor_test.go

// =============================================================================
// Test Helper Functions for Execute* Method Tests
// =============================================================================
// These helpers are defined here to avoid import cycle with testutil package.

// findMockClaudePath locates the mock-claude.sh script relative to the repo root.
func findMockClaudePath(t *testing.T) string {
	t.Helper()

	// Get the path to the current source file
	_, currentFile, _, ok := runtime.Caller(0)
	if !ok {
		t.Fatal("failed to determine current file location")
	}

	// Navigate from internal/workflow/ to repo root
	repoRoot := filepath.Join(filepath.Dir(currentFile), "..", "..")

	// Try the primary location first
	mockPath := filepath.Join(repoRoot, "mocks", "scripts", "mock-claude.sh")
	if _, err := os.Stat(mockPath); err == nil {
		return mockPath
	}

	// Fallback location
	mockPath = filepath.Join(repoRoot, "tests", "mocks", "mock-claude.sh")
	if _, err := os.Stat(mockPath); err == nil {
		return mockPath
	}

	t.Fatalf("mock-claude.sh not found at expected locations")
	return ""
}

// newTestOrchestratorWithSpecName creates a WorkflowOrchestrator configured for testing.
// It uses mock-claude.sh as the Claude command to avoid real API calls.
// Sets environment variables so mock-claude.sh creates the appropriate artifacts.
func newTestOrchestratorWithSpecName(t *testing.T, specsDir, specName string) *WorkflowOrchestrator {
	t.Helper()

	// Find the mock-claude.sh script path
	mockClaudePath := findMockClaudePath(t)

	// Create state directory within the test temp area
	stateDir := filepath.Join(specsDir, ".autospec", "state")
	if err := os.MkdirAll(stateDir, 0755); err != nil {
		t.Fatalf("failed to create state directory: %v", err)
	}

	cfg := &config.Configuration{
		CustomAgent: &cliagent.CustomAgentConfig{
			Command: mockClaudePath,
			Args:    []string{"{{PROMPT}}"},
		},
		SpecsDir:      specsDir,
		StateDir:      stateDir,
		MaxRetries:    1, // Minimal retries for faster tests
		SkipPreflight: true,
		Timeout:       30, // 30 second timeout for tests
	}

	// Set environment variables for mock-claude.sh to generate artifacts
	t.Setenv("MOCK_ARTIFACT_DIR", specsDir)
	t.Setenv("MOCK_SPEC_NAME", specName)

	return NewWorkflowOrchestrator(cfg)
}

// setupSpecDirectory creates a test spec directory with the given name.
func setupSpecDirectory(t *testing.T, specsDir, specName string) string {
	t.Helper()

	specDir := filepath.Join(specsDir, specName)
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("failed to create spec directory: %v", err)
	}
	return specDir
}

// writeTestSpec writes a valid spec.yaml to the given spec directory.
func writeTestSpec(t *testing.T, specDir string) {
	t.Helper()
	content := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
  input: "test"
user_stories:
  - id: "US-001"
    title: "Test"
    priority: "P1"
    as_a: "dev"
    i_want: "test"
    so_that: "it works"
    why_this_priority: "test"
    independent_test: "test"
    acceptance_scenarios:
      - given: "a"
        when: "b"
        then: "c"
requirements:
  functional:
    - id: "FR-001"
      description: "test"
      testable: true
      acceptance_criteria: "test"
  non_functional:
    - id: "NFR-001"
      category: "code_quality"
      description: "test"
      measurable_target: "test"
success_criteria:
  measurable_outcomes:
    - id: "SC-001"
      description: "test"
      metric: "test"
      target: "test"
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`
	path := filepath.Join(specDir, "spec.yaml")
	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		t.Fatalf("failed to write spec.yaml: %v", err)
	}
}

// writeTestPlan writes a valid plan.yaml to the given spec directory.
func writeTestPlan(t *testing.T, specDir string) {
	t.Helper()
	content := `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
summary: "Test plan"
technical_context:
  language: "Go"
  framework: "None"
  primary_dependencies: []
  storage: "None"
  testing:
    framework: "Go testing"
    approach: "Unit tests"
  target_platform: "Linux"
  project_type: "cli"
  performance_goals: "Fast"
  constraints: []
  scale_scope: "Small"
constitution_check:
  constitution_path: ".autospec/memory/constitution.yaml"
  gates: []
research_findings:
  decisions: []
data_model:
  entities: []
api_contracts:
  endpoints: []
project_structure:
  documentation: []
  source_code: []
  tests: []
implementation_phases:
  - phase: 1
    name: "Test"
    goal: "Test"
    deliverables: []
risks: []
open_questions: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "plan"
`
	path := filepath.Join(specDir, "plan.yaml")
	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		t.Fatalf("failed to write plan.yaml: %v", err)
	}
}

// writeTestTasks writes a valid tasks.yaml to the given spec directory.
// Tasks have status "Pending" which is the default pre-implementation state.
func writeTestTasks(t *testing.T, specDir string) {
	t.Helper()
	content := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
  parallel_opportunities: 0
  estimated_complexity: "low"
phases:
  - number: 1
    title: "Test"
    purpose: "Test"
    tasks:
      - id: "T001"
        title: "Test task"
        status: "Pending"
        type: "implementation"
        parallel: false
        story_id: "US-001"
        file_path: "test.go"
        dependencies: []
        acceptance_criteria:
          - "Test passes"
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: [1]
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
	path := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		t.Fatalf("failed to write tasks.yaml: %v", err)
	}
}

// writeTestTasksCompleted writes tasks.yaml with all tasks marked Completed.
// Used for testing ExecuteImplement which validates task completion.
func writeTestTasksCompleted(t *testing.T, specDir string) {
	t.Helper()
	content := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
  parallel_opportunities: 0
  estimated_complexity: "low"
phases:
  - number: 1
    title: "Test"
    purpose: "Test"
    tasks:
      - id: "T001"
        title: "Test task"
        status: "Completed"
        type: "implementation"
        parallel: false
        story_id: "US-001"
        file_path: "test.go"
        dependencies: []
        acceptance_criteria:
          - "Test passes"
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: [1]
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
	path := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		t.Fatalf("failed to write tasks.yaml: %v", err)
	}
}

// writeAllTestArtifacts writes all three artifact files (spec, plan, tasks).
func writeAllTestArtifacts(t *testing.T, specDir string) {
	t.Helper()
	writeTestSpec(t, specDir)
	writeTestPlan(t, specDir)
	writeTestTasks(t, specDir)
}

// =============================================================================
// Execute* Method Tests with Mock Infrastructure (Phase 3 Tasks T005-T008)
// =============================================================================
// These tests use newTestOrchestratorWithSpecName which configures mock-claude.sh
// to generate valid artifact files, enabling actual Execute* method testing.

// TestExecuteSpecify_Success tests ExecuteSpecify creates spec.yaml via mock
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteSpecify_Success(t *testing.T) {
	tests := map[string]struct {
		featureDesc string
		specName    string
	}{
		"simple feature description": {
			featureDesc: "Add user authentication",
			specName:    "001-test-feature",
		},
		"multiline feature description": {
			featureDesc: "Add user authentication\nwith OAuth support",
			specName:    "001-test-feature",
		},
		"feature with special characters": {
			featureDesc: "Add 'user' authentication with \"quotes\"",
			specName:    "001-test-feature",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock - mock-claude.sh will create artifacts
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Call ExecuteSpecify - mock will generate spec.yaml
			specName, err := orchestrator.ExecuteSpecify(tt.featureDesc)

			// Verify success
			if err != nil {
				t.Fatalf("ExecuteSpecify() error = %v, want nil", err)
			}

			if specName != tt.specName {
				t.Errorf("ExecuteSpecify() specName = %q, want %q", specName, tt.specName)
			}

			// Verify spec.yaml was created
			specDir := filepath.Join(tmpDir, tt.specName)
			specPath := filepath.Join(specDir, "spec.yaml")
			if _, err := os.Stat(specPath); os.IsNotExist(err) {
				t.Errorf("spec.yaml was not created at %s", specPath)
			}
		})
	}
}

// TestExecutePlan_Success tests ExecutePlan creates plan.yaml via mock
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecutePlan_Success(t *testing.T) {
	tests := map[string]struct {
		specName string
		prompt   string
	}{
		"no prompt": {
			specName: "001-test-feature",
			prompt:   "",
		},
		"with prompt": {
			specName: "001-test-feature",
			prompt:   "Focus on security",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Setup prerequisite spec.yaml
			specDir := setupSpecDirectory(t, tmpDir, tt.specName)
			writeTestSpec(t, specDir)

			// Call ExecutePlan - mock will generate plan.yaml
			err := orchestrator.ExecutePlan(tt.specName, tt.prompt)

			// Verify success
			if err != nil {
				t.Fatalf("ExecutePlan() error = %v, want nil", err)
			}

			// Verify plan.yaml was created
			planPath := filepath.Join(specDir, "plan.yaml")
			if _, err := os.Stat(planPath); os.IsNotExist(err) {
				t.Errorf("plan.yaml was not created at %s", planPath)
			}
		})
	}
}

// TestExecuteTasks_Success tests ExecuteTasks creates tasks.yaml via mock
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteTasks_Success(t *testing.T) {
	tests := map[string]struct {
		specName string
		prompt   string
	}{
		"no prompt": {
			specName: "001-test-feature",
			prompt:   "",
		},
		"with prompt": {
			specName: "001-test-feature",
			prompt:   "Break into small steps",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Setup prerequisite spec.yaml and plan.yaml
			specDir := setupSpecDirectory(t, tmpDir, tt.specName)
			writeTestSpec(t, specDir)
			writeTestPlan(t, specDir)

			// Call ExecuteTasks - mock will generate tasks.yaml
			err := orchestrator.ExecuteTasks(tt.specName, tt.prompt)

			// Verify success
			if err != nil {
				t.Fatalf("ExecuteTasks() error = %v, want nil", err)
			}

			// Verify tasks.yaml was created
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if _, err := os.Stat(tasksPath); os.IsNotExist(err) {
				t.Errorf("tasks.yaml was not created at %s", tasksPath)
			}
		})
	}
}

// TestExecuteImplement_Success tests ExecuteImplement completes without error
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteImplement_Success(t *testing.T) {
	tests := map[string]struct {
		specName  string
		prompt    string
		resume    bool
		phaseOpts PhaseExecutionOptions
	}{
		"default execution": {
			specName:  "001-test-feature",
			prompt:    "",
			resume:    false,
			phaseOpts: PhaseExecutionOptions{},
		},
		"with prompt": {
			specName:  "001-test-feature",
			prompt:    "Focus on error handling",
			resume:    false,
			phaseOpts: PhaseExecutionOptions{},
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Setup all prerequisite artifacts with completed tasks
			// ExecuteImplement validates that all tasks are completed
			specDir := setupSpecDirectory(t, tmpDir, tt.specName)
			writeTestSpec(t, specDir)
			writeTestPlan(t, specDir)
			writeTestTasksCompleted(t, specDir)

			// Call ExecuteImplement
			err := orchestrator.ExecuteImplement(tt.specName, tt.prompt, tt.resume, tt.phaseOpts)

			// Verify success (implementation completes without error)
			if err != nil {
				t.Fatalf("ExecuteImplement() error = %v, want nil", err)
			}
		})
	}
}

// =============================================================================
// Run* Workflow Tests (Phase 4 Tasks T009-T010)
// =============================================================================

// TestRunCompleteWorkflow_Success tests RunCompleteWorkflow executes specify  plan  tasks
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestRunCompleteWorkflow_Success(t *testing.T) {
	tests := map[string]struct {
		featureDesc string
		specName    string
	}{
		"simple feature description": {
			featureDesc: "Add user authentication",
			specName:    "001-test-feature",
		},
		"detailed feature description": {
			featureDesc: "Add user authentication with OAuth support and password reset",
			specName:    "001-test-feature",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock - mock-claude.sh will generate artifacts
			// for each stage (specify  plan  tasks)
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)
			orchestrator.SkipPreflight = true

			// Call RunCompleteWorkflow - mock generates spec, plan, tasks in sequence
			err := orchestrator.RunCompleteWorkflow(tt.featureDesc)

			// Verify success
			if err != nil {
				t.Fatalf("RunCompleteWorkflow() error = %v, want nil", err)
			}

			// Verify all three artifacts were created in sequence
			specDir := filepath.Join(tmpDir, tt.specName)

			// Verify spec.yaml was created
			specPath := filepath.Join(specDir, "spec.yaml")
			if _, err := os.Stat(specPath); os.IsNotExist(err) {
				t.Errorf("spec.yaml was not created at %s", specPath)
			}

			// Verify plan.yaml was created
			planPath := filepath.Join(specDir, "plan.yaml")
			if _, err := os.Stat(planPath); os.IsNotExist(err) {
				t.Errorf("plan.yaml was not created at %s", planPath)
			}

			// Verify tasks.yaml was created
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if _, err := os.Stat(tasksPath); os.IsNotExist(err) {
				t.Errorf("tasks.yaml was not created at %s", tasksPath)
			}
		})
	}
}

// TestRunFullWorkflow_Success tests RunFullWorkflow executes specify  plan  tasks  implement
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestRunFullWorkflow_Success(t *testing.T) {
	tests := map[string]struct {
		featureDesc string
		specName    string
		resume      bool
	}{
		"full workflow without resume": {
			featureDesc: "Add user authentication",
			specName:    "001-test-feature",
			resume:      false,
		},
		"full workflow with resume": {
			featureDesc: "Add user authentication",
			specName:    "001-test-feature",
			resume:      true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock - mock-claude.sh will generate artifacts
			// for all four stages (specify  plan  tasks  implement)
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)
			orchestrator.SkipPreflight = true

			// Call RunFullWorkflow - mock generates all artifacts including implementation
			err := orchestrator.RunFullWorkflow(tt.featureDesc, tt.resume)

			// Verify success
			if err != nil {
				t.Fatalf("RunFullWorkflow() error = %v, want nil", err)
			}

			// Verify all artifacts were created
			specDir := filepath.Join(tmpDir, tt.specName)

			// Verify spec.yaml was created
			specPath := filepath.Join(specDir, "spec.yaml")
			if _, err := os.Stat(specPath); os.IsNotExist(err) {
				t.Errorf("spec.yaml was not created at %s", specPath)
			}

			// Verify plan.yaml was created
			planPath := filepath.Join(specDir, "plan.yaml")
			if _, err := os.Stat(planPath); os.IsNotExist(err) {
				t.Errorf("plan.yaml was not created at %s", planPath)
			}

			// Verify tasks.yaml was created
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if _, err := os.Stat(tasksPath); os.IsNotExist(err) {
				t.Errorf("tasks.yaml was not created at %s", tasksPath)
			}
		})
	}
}

// =============================================================================
// Error Path Tests (Phase 5 Tasks T011-T013)
// =============================================================================

// TestExecuteSpecify_ValidationFailure tests ExecuteSpecify when mock returns non-zero exit code
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteSpecify_ValidationFailure(t *testing.T) {
	tests := map[string]struct {
		featureDesc  string
		specName     string
		exitCode     string
		wantErrMatch string
	}{
		"mock returns error exit code": {
			featureDesc:  "Add user authentication",
			specName:     "001-test-feature",
			exitCode:     "1",
			wantErrMatch: "failed",
		},
		"mock returns timeout exit code": {
			featureDesc:  "Add user authentication",
			specName:     "001-test-feature",
			exitCode:     "124", // Common timeout exit code
			wantErrMatch: "failed",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Configure mock to fail with specified exit code
			t.Setenv("MOCK_EXIT_CODE", tt.exitCode)

			// Call ExecuteSpecify - should fail due to mock exit code
			_, err := orchestrator.ExecuteSpecify(tt.featureDesc)

			// Verify error is returned
			if err == nil {
				t.Fatalf("ExecuteSpecify() error = nil, want error containing %q", tt.wantErrMatch)
			}

			// Verify error contains expected context
			if !strings.Contains(err.Error(), tt.wantErrMatch) {
				t.Errorf("ExecuteSpecify() error = %q, want error containing %q", err.Error(), tt.wantErrMatch)
			}
		})
	}
}

// TestExecuteWithRetry_RetriesOnFailure tests that ExecuteSpecify retries when configured
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteWithRetry_RetriesOnFailure(t *testing.T) {
	// This test verifies retry behavior by checking call count via MOCK_CALL_LOG
	// The mock script logs each invocation, allowing verification of retry attempts

	t.Run("exhausted retries returns error", func(t *testing.T) {
		// Create isolated temp directory
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		// Create orchestrator with mock
		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Set up call log to verify retry attempts
		callLog := filepath.Join(tmpDir, "call.log")
		t.Setenv("MOCK_CALL_LOG", callLog)

		// Configure mock to always fail - will exhaust retries
		t.Setenv("MOCK_EXIT_CODE", "1")

		// Call ExecuteSpecify - should fail after retries exhausted
		_, err := orchestrator.ExecuteSpecify("Add user authentication")

		// Verify error is returned
		if err == nil {
			t.Fatal("ExecuteSpecify() error = nil, want error after retries exhausted")
		}

		// Verify call log exists (proves at least one call was made)
		if _, statErr := os.Stat(callLog); os.IsNotExist(statErr) {
			t.Error("MOCK_CALL_LOG was not created - mock was not invoked")
		}
	})

	t.Run("success after retry is possible", func(t *testing.T) {
		// Note: This test verifies the retry mechanism exists
		// Full retry-then-success testing requires more complex mock coordination
		// Here we verify the orchestrator is configured with retries > 0

		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Verify orchestrator's executor has retry capability
		if orchestrator.Executor.MaxRetries < 1 {
			t.Errorf("orchestrator.Executor.MaxRetries = %d, want >= 1", orchestrator.Executor.MaxRetries)
		}
	})
}

// TestAuxiliaryExecuteMethods tests ExecuteConstitution, ExecuteClarify, ExecuteChecklist, ExecuteAnalyze
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestAuxiliaryExecuteMethods(t *testing.T) {
	// Table-driven test for all auxiliary Execute* methods
	tests := map[string]struct {
		methodName string
		setup      func(t *testing.T, specDir string)
		execute    func(orchestrator *WorkflowOrchestrator, specName string) error
		wantErr    bool
	}{
		"ExecuteConstitution without prompt": {
			methodName: "ExecuteConstitution",
			setup:      nil, // Constitution doesn't require pre-existing artifacts
			execute: func(o *WorkflowOrchestrator, _ string) error {
				return o.ExecuteConstitution("")
			},
			wantErr: false,
		},
		"ExecuteConstitution with prompt": {
			methodName: "ExecuteConstitution",
			setup:      nil,
			execute: func(o *WorkflowOrchestrator, _ string) error {
				return o.ExecuteConstitution("Focus on testing principles")
			},
			wantErr: false,
		},
		"ExecuteClarify without prompt": {
			methodName: "ExecuteClarify",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteClarify(specName, "")
			},
			wantErr: false,
		},
		"ExecuteClarify with prompt": {
			methodName: "ExecuteClarify",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteClarify(specName, "Focus on security aspects")
			},
			wantErr: false,
		},
		"ExecuteChecklist without prompt": {
			methodName: "ExecuteChecklist",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteChecklist(specName, "")
			},
			wantErr: false,
		},
		"ExecuteChecklist with prompt": {
			methodName: "ExecuteChecklist",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteChecklist(specName, "Include accessibility checks")
			},
			wantErr: false,
		},
		"ExecuteAnalyze without prompt": {
			methodName: "ExecuteAnalyze",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteAnalyze(specName, "")
			},
			wantErr: false,
		},
		"ExecuteAnalyze with prompt": {
			methodName: "ExecuteAnalyze",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteAnalyze(specName, "Focus on API consistency")
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()
			specName := "001-test-feature"

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

			// Setup prerequisite files if needed
			if tt.setup != nil {
				specDir := setupSpecDirectory(t, tmpDir, specName)
				tt.setup(t, specDir)
			}

			// Execute the method
			err := tt.execute(orchestrator, specName)

			// Verify error expectation
			if (err != nil) != tt.wantErr {
				t.Errorf("%s() error = %v, wantErr %v", tt.methodName, err, tt.wantErr)
			}
		})
	}
}

// TestAuxiliaryExecuteMethods_Errors tests error paths for auxiliary Execute* methods
func TestAuxiliaryExecuteMethods_Errors(t *testing.T) {
	tests := map[string]struct {
		methodName string
		exitCode   string
		execute    func(orchestrator *WorkflowOrchestrator, specName string) error
	}{
		"ExecuteConstitution failure": {
			methodName: "ExecuteConstitution",
			exitCode:   "1",
			execute: func(o *WorkflowOrchestrator, _ string) error {
				return o.ExecuteConstitution("")
			},
		},
		"ExecuteClarify failure": {
			methodName: "ExecuteClarify",
			exitCode:   "1",
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteClarify(specName, "")
			},
		},
		"ExecuteChecklist failure": {
			methodName: "ExecuteChecklist",
			exitCode:   "1",
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteChecklist(specName, "")
			},
		},
		"ExecuteAnalyze failure": {
			methodName: "ExecuteAnalyze",
			exitCode:   "1",
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteAnalyze(specName, "")
			},
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Create isolated temp directory
			tmpDir := t.TempDir()
			specName := "001-test-feature"

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

			// Setup prerequisite spec.yaml for methods that need it
			specDir := setupSpecDirectory(t, tmpDir, specName)
			writeTestSpec(t, specDir)

			// Configure mock to fail
			t.Setenv("MOCK_EXIT_CODE", tt.exitCode)

			// Execute the method - should fail
			err := tt.execute(orchestrator, specName)

			// Verify error is returned
			if err == nil {
				t.Errorf("%s() error = nil, want error due to mock failure", tt.methodName)
			}
		})
	}
}

// TestExecuteImplementWithPhases tests phase-by-phase implementation execution
func TestExecuteImplementWithPhases(t *testing.T) {
	t.Run("no phases returns error", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with empty tasks (no phases)
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)

		// Write tasks.yaml with empty phases array
		emptyTasksContent := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 0
  total_phases: 0
  parallel_opportunities: 0
  estimated_complexity: "low"
phases: []
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: []
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
		tasksPath := filepath.Join(specDir, "tasks.yaml")
		if err := os.WriteFile(tasksPath, []byte(emptyTasksContent), 0644); err != nil {
			t.Fatalf("failed to write tasks.yaml: %v", err)
		}

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Call ExecuteImplementWithPhases
		err := orchestrator.ExecuteImplementWithPhases(specName, metadata, "", false)

		// Should return error about no phases
		if err == nil {
			t.Error("ExecuteImplementWithPhases() error = nil, want error for empty phases")
		}
		if err != nil && !strings.Contains(err.Error(), "no phases") {
			t.Errorf("ExecuteImplementWithPhases() error = %v, want error containing 'no phases'", err)
		}
	})

	t.Run("all phases complete returns early", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Call ExecuteImplementWithPhases
		err := orchestrator.ExecuteImplementWithPhases(specName, metadata, "", false)

		// Should succeed (all phases already complete)
		if err != nil {
			t.Errorf("ExecuteImplementWithPhases() error = %v, want nil", err)
		}
	})
}

// TestExecuteImplementWithTasks tests task-by-task implementation execution
func TestExecuteImplementWithTasks(t *testing.T) {
	t.Run("no tasks returns error", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with empty tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)

		// Write tasks.yaml with empty phases array
		emptyTasksContent := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 0
  total_phases: 0
  parallel_opportunities: 0
  estimated_complexity: "low"
phases: []
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: []
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
		tasksPath := filepath.Join(specDir, "tasks.yaml")
		if err := os.WriteFile(tasksPath, []byte(emptyTasksContent), 0644); err != nil {
			t.Fatalf("failed to write tasks.yaml: %v", err)
		}

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Call ExecuteImplementWithTasks (signature: specName, metadata, prompt, fromTask string)
		err := orchestrator.ExecuteImplementWithTasks(specName, metadata, "", "")

		// Should return error about no tasks
		if err == nil {
			t.Error("ExecuteImplementWithTasks() error = nil, want error for empty tasks")
		}
	})

	t.Run("all tasks complete returns early", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Call ExecuteImplementWithTasks (signature: specName, metadata, prompt, fromTask string)
		err := orchestrator.ExecuteImplementWithTasks(specName, metadata, "", "")

		// Should succeed (all tasks already complete)
		if err != nil {
			t.Errorf("ExecuteImplementWithTasks() error = %v, want nil", err)
		}
	})
}

// TestExecuteImplementSinglePhase tests single phase execution
func TestExecuteImplementSinglePhase(t *testing.T) {
	t.Run("phase not found returns error", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with one phase
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasks(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Request phase 99 which doesn't exist (signature: specName, metadata, prompt, phaseNumber)
		err := orchestrator.ExecuteImplementSinglePhase(specName, metadata, "", 99)

		// Should return error about phase out of range
		if err == nil {
			t.Error("ExecuteImplementSinglePhase() error = nil, want error for invalid phase")
		}
		if err != nil && !strings.Contains(err.Error(), "out of range") {
			t.Errorf("ExecuteImplementSinglePhase() error = %v, want error containing 'out of range'", err)
		}
	})

	t.Run("valid phase with completed tasks succeeds", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Request phase 1 (signature: specName, metadata, prompt, phaseNumber)
		err := orchestrator.ExecuteImplementSinglePhase(specName, metadata, "", 1)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplementSinglePhase() error = %v, want nil", err)
		}
	})
}

// TestExecuteImplementFromPhase tests starting implementation from a specific phase
func TestExecuteImplementFromPhase(t *testing.T) {
	t.Run("phase not found returns error", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasks(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Request starting from phase 99 which doesn't exist (signature: specName, metadata, prompt, startPhase)
		err := orchestrator.ExecuteImplementFromPhase(specName, metadata, "", 99)

		// Should return error about phase not found
		if err == nil {
			t.Error("ExecuteImplementFromPhase() error = nil, want error for invalid phase")
		}
	})
}

// NOTE: TestHandleImplementError removed - handleImplementError was an internal method
// that has been delegated to PhaseExecutor.ExecuteDefault. Error handling is now tested
// via the PhaseExecutor tests in phase_executor_test.go

// TestRunFullWorkflow_Error tests error paths in RunFullWorkflow
func TestRunFullWorkflow_Error(t *testing.T) {
	t.Run("specify stage failure", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)
		orchestrator.SkipPreflight = true

		// Configure mock to fail
		t.Setenv("MOCK_EXIT_CODE", "1")

		err := orchestrator.RunFullWorkflow("Add test feature", false)

		if err == nil {
			t.Error("RunFullWorkflow() error = nil, want error from failing specify stage")
		}
	})
}

// TestRunCompleteWorkflow_Error tests error paths in RunCompleteWorkflow
func TestRunCompleteWorkflow_Error(t *testing.T) {
	t.Run("specify stage failure", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)
		orchestrator.SkipPreflight = true

		// Configure mock to fail
		t.Setenv("MOCK_EXIT_CODE", "1")

		err := orchestrator.RunCompleteWorkflow("Add test feature")

		if err == nil {
			t.Error("RunCompleteWorkflow() error = nil, want error from failing specify stage")
		}
	})
}

// NOTE: TestGetPhaseInfo moved to phase_executor_test.go

// NOTE: TestPrintPhaseCompletion_NilInfo moved to phase_executor_test.go

// NOTE: TestPrintTasksSummary_NoTasks moved to task_executor_test.go

// NOTE: TestPrintPhasesSummary moved to phase_executor_test.go

// NOTE: TestGetPhaseByNumberWithTasksPath - functionality moved to validation package

// TestExecuteImplement_Modes tests different execution modes in ExecuteImplement
func TestExecuteImplement_Modes(t *testing.T) {
	t.Run("phases mode with completed tasks", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Call ExecuteImplement with phases mode (RunAllPhases)
		opts := PhaseExecutionOptions{
			RunAllPhases: true,
		}
		err := orchestrator.ExecuteImplement(specName, "", false, opts)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplement(phases mode) error = %v, want nil", err)
		}
	})

	t.Run("tasks mode with completed tasks", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Call ExecuteImplement with tasks mode (TaskMode)
		opts := PhaseExecutionOptions{
			TaskMode: true,
		}
		err := orchestrator.ExecuteImplement(specName, "", false, opts)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplement(tasks mode) error = %v, want nil", err)
		}
	})

	t.Run("single phase mode", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Call ExecuteImplement with single phase
		opts := PhaseExecutionOptions{
			SinglePhase: 1,
		}
		err := orchestrator.ExecuteImplement(specName, "", false, opts)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplement(single phase mode) error = %v, want nil", err)
		}
	})

	t.Run("from phase mode", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Call ExecuteImplement with from phase
		opts := PhaseExecutionOptions{
			FromPhase: 1,
		}
		err := orchestrator.ExecuteImplement(specName, "", false, opts)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplement(from phase mode) error = %v, want nil", err)
		}
	})
}

// NOTE: Integration tests for private methods (executeSingleTaskSession, executeSinglePhaseSession,
// executeAndVerifyTask, executeTaskLoop) have been moved to executor-specific test files
// (task_executor_test.go, phase_executor_test.go) as part of the refactoring to delegate
// execution logic to specialized executors.

// TestSchemaValidationIntegration tests that schema validators are properly wired into workflow
// These tests verify FR-006: executePlan(), executeTasks(), and executeSpecify() pass schema validator functions to ExecuteStage()
func TestSchemaValidationIntegration(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		stage        string
		validateFunc func(string) error
		validDir     string
		invalidDir   string
		errContains  string
		setupValid   func(string) error
		setupInvalid func(string) error
	}{
		"spec schema validation": {
			stage:        "specify",
			validateFunc: ValidateSpecSchema,
			errContains:  "schema validation failed for spec.yaml",
			setupValid: func(specDir string) error {
				content := `feature:
  branch: "001-test"
  created: "2025-01-01"
  status: "Draft"
  input: "Test feature"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`
				return os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(content), 0644)
			},
			setupInvalid: func(specDir string) error {
				// Missing required 'feature' field
				content := `user_stories: []
requirements:
  functional: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				return os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(content), 0644)
			},
		},
		"plan schema validation": {
			stage:        "plan",
			validateFunc: ValidatePlanSchema,
			errContains:  "schema validation failed for plan.yaml",
			setupValid: func(specDir string) error {
				content := `plan:
  branch: "001-test"
  created: "2025-01-01"
  spec_path: "specs/001-test/spec.yaml"
summary: "Test plan"
technical_context:
  language: "Go"
  framework: "Cobra"
  project_type: "cli"
implementation_phases: []
project_structure:
  source_code: []
  tests: []
  documentation: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "plan"
`
				return os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(content), 0644)
			},
			setupInvalid: func(specDir string) error {
				// Missing required 'plan' field
				content := `summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				return os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(content), 0644)
			},
		},
		"tasks schema validation": {
			stage:        "tasks",
			validateFunc: ValidateTasksSchema,
			errContains:  "schema validation failed for tasks.yaml",
			setupValid: func(specDir string) error {
				content := `tasks:
  branch: "001-test"
  created: "2025-01-01"
  spec_path: "specs/001-test/spec.yaml"
  plan_path: "specs/001-test/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Setup"
    purpose: "Initial setup"
    tasks:
      - id: "T001"
        title: "Test task"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test passes"
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
				return os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(content), 0644)
			},
			setupInvalid: func(specDir string) error {
				// Missing required 'tasks' field
				content := `summary:
  total_tasks: 1
  total_phases: 1
phases: []
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				return os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(content), 0644)
			},
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Test valid artifact passes validation
			t.Run("valid_"+tc.stage+"_passes", func(t *testing.T) {
				t.Parallel()
				tmpDir := t.TempDir()
				specDir := filepath.Join(tmpDir, "specs", "001-test")
				if err := os.MkdirAll(specDir, 0755); err != nil {
					t.Fatalf("Failed to create spec directory: %v", err)
				}
				if err := tc.setupValid(specDir); err != nil {
					t.Fatalf("Failed to setup valid artifact: %v", err)
				}

				err := tc.validateFunc(specDir)
				if err != nil {
					t.Errorf("Valid %s should pass validation, got error: %v", tc.stage, err)
				}
			})

			// Test invalid artifact fails validation with correct error message
			t.Run("invalid_"+tc.stage+"_fails", func(t *testing.T) {
				t.Parallel()
				tmpDir := t.TempDir()
				specDir := filepath.Join(tmpDir, "specs", "001-test")
				if err := os.MkdirAll(specDir, 0755); err != nil {
					t.Fatalf("Failed to create spec directory: %v", err)
				}
				if err := tc.setupInvalid(specDir); err != nil {
					t.Fatalf("Failed to setup invalid artifact: %v", err)
				}

				err := tc.validateFunc(specDir)
				if err == nil {
					t.Errorf("Invalid %s should fail validation, got nil error", tc.stage)
					return
				}
				if !strings.Contains(err.Error(), tc.errContains) {
					t.Errorf("Error should contain %q, got: %v", tc.errContains, err)
				}
			})
		})
	}
}

// TestSchemaValidationRejectsInvalidBeforeNextStage verifies that invalid artifacts
// are caught before proceeding to the next workflow stage (FR-001, FR-002, FR-003)
func TestSchemaValidationRejectsInvalidBeforeNextStage(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		currentStage   string
		nextStage      string
		validateFunc   func(string) error
		invalidContent string
		expectedErr    string
	}{
		"invalid spec rejected before plan stage": {
			currentStage: "specify",
			nextStage:    "plan",
			validateFunc: ValidateSpecSchema,
			invalidContent: `# Missing required 'feature' field
user_stories: []
requirements:
  functional: []
_meta:
  artifact_type: "spec"
`,
			expectedErr: "missing required field: feature",
		},
		"invalid plan rejected before tasks stage": {
			currentStage: "plan",
			nextStage:    "tasks",
			validateFunc: ValidatePlanSchema,
			invalidContent: `# Missing required 'plan' field
summary: "Test"
_meta:
  artifact_type: "plan"
`,
			expectedErr: "missing required field: plan",
		},
		"invalid tasks rejected before implement stage": {
			currentStage: "tasks",
			nextStage:    "implement",
			validateFunc: ValidateTasksSchema,
			invalidContent: `# Missing required 'tasks' field
summary:
  total_tasks: 0
phases: []
_meta:
  artifact_type: "tasks"
`,
			expectedErr: "missing required field: tasks",
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			// Create invalid artifact file
			var artifactPath string
			switch tc.currentStage {
			case "specify":
				artifactPath = filepath.Join(specDir, "spec.yaml")
			case "plan":
				artifactPath = filepath.Join(specDir, "plan.yaml")
			case "tasks":
				artifactPath = filepath.Join(specDir, "tasks.yaml")
			}
			if err := os.WriteFile(artifactPath, []byte(tc.invalidContent), 0644); err != nil {
				t.Fatalf("Failed to write invalid artifact: %v", err)
			}

			// Run validation (simulates what ExecuteStage does after Claude generates output)
			err := tc.validateFunc(specDir)

			// Verify validation fails with expected error
			if err == nil {
				t.Errorf("Expected validation to fail for invalid %s artifact before %s stage", tc.currentStage, tc.nextStage)
				return
			}
			if !strings.Contains(err.Error(), tc.expectedErr) {
				t.Errorf("Error should contain %q, got: %v", tc.expectedErr, err)
			}
		})
	}
}

// TestValidArtifactsProceedToNextStage verifies that valid artifacts proceed without error
func TestValidArtifactsProceedToNextStage(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		stage        string
		validateFunc func(string) error
		content      string
		artifactName string
	}{
		"valid spec proceeds to plan": {
			stage:        "specify",
			validateFunc: ValidateSpecSchema,
			artifactName: "spec.yaml",
			content: `feature:
  branch: "001-valid-test"
  created: "2025-01-01"
  status: "Draft"
  input: "Valid test feature"
user_stories:
  - id: "US-001"
    title: "Test story"
    priority: "P1"
    as_a: "user"
    i_want: "to test"
    so_that: "I can verify"
    acceptance_scenarios:
      - given: "a test"
        when: "I run it"
        then: "it passes"
requirements:
  functional:
    - id: "FR-001"
      description: "MUST pass test"
      testable: true
      acceptance_criteria: "Test passes"
  non_functional: []
success_criteria:
  measurable_outcomes:
    - id: "SC-001"
      description: "Tests pass"
      metric: "Pass rate"
      target: "100%"
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`,
		},
		"valid plan proceeds to tasks": {
			stage:        "plan",
			validateFunc: ValidatePlanSchema,
			artifactName: "plan.yaml",
			content: `plan:
  branch: "001-valid-test"
  created: "2025-01-01"
  spec_path: "specs/001-valid-test/spec.yaml"
summary: "Valid implementation plan"
technical_context:
  language: "Go"
  framework: "Cobra"
  project_type: "cli"
implementation_phases:
  - phase: 1
    name: "Setup"
    goal: "Initial setup"
    deliverables:
      - "Project structure"
project_structure:
  source_code:
    - path: "cmd/main.go"
      description: "Entry point"
  tests:
    - path: "cmd/main_test.go"
      description: "Entry point tests"
  documentation: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "plan"
`,
		},
		"valid tasks proceeds to implement": {
			stage:        "tasks",
			validateFunc: ValidateTasksSchema,
			artifactName: "tasks.yaml",
			content: `tasks:
  branch: "001-valid-test"
  created: "2025-01-01"
  spec_path: "specs/001-valid-test/spec.yaml"
  plan_path: "specs/001-valid-test/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Setup"
    purpose: "Initial setup"
    tasks:
      - id: "T001"
        title: "Initialize project"
        status: "Pending"
        type: "setup"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Project compiles"
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-valid-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			artifactPath := filepath.Join(specDir, tc.artifactName)
			if err := os.WriteFile(artifactPath, []byte(tc.content), 0644); err != nil {
				t.Fatalf("Failed to write valid artifact: %v", err)
			}

			// Run validation
			err := tc.validateFunc(specDir)

			// Verify validation passes
			if err != nil {
				t.Errorf("Valid %s artifact should pass validation, got error: %v", tc.stage, err)
			}
		})
	}
}

// TestSchemaValidationErrorMessageFormat verifies error messages are consistent (SC-003)
func TestSchemaValidationErrorMessageFormat(t *testing.T) {
	t.Parallel()

	// All schema validation errors should follow the same format:
	// "schema validation failed for <artifact>:\n- <error1>\n- <error2>..."
	tests := map[string]struct {
		validateFunc func(string) error
		artifactName string
		content      string
	}{
		"spec error format": {
			validateFunc: ValidateSpecSchema,
			artifactName: "spec.yaml",
			content: `user_stories: []
_meta:
  artifact_type: "spec"
`,
		},
		"plan error format": {
			validateFunc: ValidatePlanSchema,
			artifactName: "plan.yaml",
			content: `summary: "test"
_meta:
  artifact_type: "plan"
`,
		},
		"tasks error format": {
			validateFunc: ValidateTasksSchema,
			artifactName: "tasks.yaml",
			content: `phases: []
_meta:
  artifact_type: "tasks"
`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			artifactPath := filepath.Join(specDir, tc.artifactName)
			if err := os.WriteFile(artifactPath, []byte(tc.content), 0644); err != nil {
				t.Fatalf("Failed to write artifact: %v", err)
			}

			err := tc.validateFunc(specDir)
			if err == nil {
				t.Fatal("Expected validation error")
			}

			errStr := err.Error()
			// Verify error format is consistent
			if !strings.HasPrefix(errStr, "schema validation failed for "+tc.artifactName) {
				t.Errorf("Error should start with 'schema validation failed for %s', got: %s", tc.artifactName, errStr)
			}
			if !strings.Contains(errStr, "- ") {
				t.Errorf("Error should contain bullet point format '- ', got: %s", errStr)
			}
		})
	}
}

// TestSchemaValidationInAutospecRunPath verifies schema validation in the autospec run -a path (US-003, T014).
// This test simulates the full workflow run path where an invalid spec.yaml should be rejected
// before proceeding to the plan stage.
func TestSchemaValidationInAutospecRunPath(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specContent      string
		expectValidation bool
		errContains      string
		description      string
	}{
		"invalid spec missing feature field rejected before plan": {
			specContent: `# Missing required 'feature' field
user_stories: []
requirements:
  functional: []
  non_functional: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`,
			expectValidation: false,
			errContains:      "missing required field: feature",
			description:      "Spec without 'feature' field should fail schema validation",
		},
		"invalid spec missing branch rejected before plan": {
			specContent: `feature:
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`,
			expectValidation: false,
			errContains:      "missing required field: branch",
			description:      "Spec without 'feature.branch' should fail schema validation",
		},
		"valid spec proceeds to plan stage": {
			specContent: `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
  input: "Test feature for validation"
user_stories:
  - id: "US-001"
    title: "Test story"
    priority: "P1"
    as_a: "developer"
    i_want: "to test validation"
    so_that: "I can verify the workflow"
    acceptance_scenarios:
      - given: "a valid spec"
        when: "validation runs"
        then: "it passes"
requirements:
  functional:
    - id: "FR-001"
      description: "MUST validate spec"
      testable: true
      acceptance_criteria: "Spec passes validation"
  non_functional: []
success_criteria:
  measurable_outcomes:
    - id: "SC-001"
      description: "Validation passes"
      metric: "Pass rate"
      target: "100%"
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`,
			expectValidation: true,
			errContains:      "",
			description:      "Valid spec should pass schema validation",
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test-feature")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			// Write spec.yaml (simulating output from specify stage)
			specPath := filepath.Join(specDir, "spec.yaml")
			if err := os.WriteFile(specPath, []byte(tc.specContent), 0644); err != nil {
				t.Fatalf("Failed to write spec.yaml: %v", err)
			}

			// Run schema validation (same function used in autospec run -a path)
			err := ValidateSpecSchema(specDir)

			// Verify validation result matches expected outcome
			if tc.expectValidation {
				if err != nil {
					t.Errorf("%s: expected validation to pass, got error: %v", tc.description, err)
				}
			} else {
				if err == nil {
					t.Errorf("%s: expected validation to fail, but it passed", tc.description)
					return
				}
				// Verify error message format matches standalone specify command
				errStr := err.Error()
				if !strings.HasPrefix(errStr, "schema validation failed for spec.yaml") {
					t.Errorf("Error format mismatch: expected prefix 'schema validation failed for spec.yaml', got: %s", errStr)
				}
				if tc.errContains != "" && !strings.Contains(errStr, tc.errContains) {
					t.Errorf("Error should contain %q, got: %s", tc.errContains, errStr)
				}
			}
		})
	}
}

// TestSchemaValidationInAutospecPrepPath verifies schema validation in the autospec prep path (US-003, T015).
// This test simulates the prep workflow where an invalid plan.yaml should be rejected
// before proceeding to the tasks stage.
func TestSchemaValidationInAutospecPrepPath(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		planContent      string
		expectValidation bool
		errContains      string
		description      string
	}{
		"invalid plan missing plan field rejected before tasks": {
			planContent: `# Missing required 'plan' field
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`,
			expectValidation: false,
			errContains:      "missing required field: plan",
			description:      "Plan without 'plan' field should fail schema validation",
		},
		"invalid plan missing branch rejected before tasks": {
			planContent: `plan:
  created: "2025-01-01"
  spec_path: "specs/001-test/spec.yaml"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`,
			expectValidation: false,
			errContains:      "missing required field: branch",
			description:      "Plan without 'plan.branch' should fail schema validation",
		},
		"valid plan proceeds to tasks stage": {
			planContent: `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
summary: "Implementation plan for test feature"
technical_context:
  language: "Go"
  framework: "Cobra"
  project_type: "cli"
implementation_phases:
  - phase: 1
    name: "Setup"
    goal: "Initial setup"
    deliverables:
      - "Project structure"
project_structure:
  source_code:
    - path: "cmd/main.go"
      description: "Entry point"
  tests:
    - path: "cmd/main_test.go"
      description: "Entry point tests"
  documentation: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "plan"
`,
			expectValidation: true,
			errContains:      "",
			description:      "Valid plan should pass schema validation",
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test-feature")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			// Write plan.yaml (simulating output from plan stage)
			planPath := filepath.Join(specDir, "plan.yaml")
			if err := os.WriteFile(planPath, []byte(tc.planContent), 0644); err != nil {
				t.Fatalf("Failed to write plan.yaml: %v", err)
			}

			// Run schema validation (same function used in autospec prep path)
			err := ValidatePlanSchema(specDir)

			// Verify validation result matches expected outcome
			if tc.expectValidation {
				if err != nil {
					t.Errorf("%s: expected validation to pass, got error: %v", tc.description, err)
				}
			} else {
				if err == nil {
					t.Errorf("%s: expected validation to fail, but it passed", tc.description)
					return
				}
				// Verify error message format matches standalone plan command
				errStr := err.Error()
				if !strings.HasPrefix(errStr, "schema validation failed for plan.yaml") {
					t.Errorf("Error format mismatch: expected prefix 'schema validation failed for plan.yaml', got: %s", errStr)
				}
				if tc.errContains != "" && !strings.Contains(errStr, tc.errContains) {
					t.Errorf("Error should contain %q, got: %s", tc.errContains, errStr)
				}
			}
		})
	}
}

// assertSchemaValidationErrorFormat is a helper function that verifies the consistency
// of schema validation error messages across different entry points (US-003, T016).
// It ensures:
// - Error prefix is consistent: "schema validation failed for <artifact>:"
// - Error contains bullet point format "- <error message>"
// - Error includes the expected field/message
func assertSchemaValidationErrorFormat(t *testing.T, err error, artifactName string, expectedContent string) {
	t.Helper()

	if err == nil {
		t.Fatalf("Expected validation error for %s, got nil", artifactName)
	}

	errStr := err.Error()

	// Check consistent prefix format
	expectedPrefix := fmt.Sprintf("schema validation failed for %s:", artifactName)
	if !strings.HasPrefix(errStr, expectedPrefix) {
		t.Errorf("Error format inconsistent: expected prefix %q, got: %s", expectedPrefix, errStr)
	}

	// Check bullet point format is present
	if !strings.Contains(errStr, "- ") {
		t.Errorf("Error format inconsistent: expected bullet point format '- ', got: %s", errStr)
	}

	// Check expected content is present
	if expectedContent != "" && !strings.Contains(errStr, expectedContent) {
		t.Errorf("Error should contain %q, got: %s", expectedContent, errStr)
	}
}

// TestSchemaValidationErrorConsistencyAcrossEntryPoints verifies that error messages
// are consistent across different autospec entry points (US-003, T016).
// This test checks that the same validation failure produces identical error format
// regardless of whether it's triggered via:
// - autospec run -a (specify  plan  tasks  implement)
// - autospec specify (standalone)
// - autospec prep (specify  plan  tasks)
// - autospec plan (standalone)
// - autospec tasks (standalone)
func TestSchemaValidationErrorConsistencyAcrossEntryPoints(t *testing.T) {
	t.Parallel()

	// Test that the same invalid spec produces consistent errors
	// regardless of which command path invokes validation
	t.Run("spec validation error format consistency", func(t *testing.T) {
		t.Parallel()

		invalidSpecContent := `user_stories: []
requirements:
  functional: []
_meta:
  artifact_type: "spec"
`

		// Create two separate directories to simulate different entry points
		entries := []string{"run-a", "specify-standalone"}
		errors := make(map[string]error)

		for _, entry := range entries {
			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory for %s: %v", entry, err)
			}

			specPath := filepath.Join(specDir, "spec.yaml")
			if err := os.WriteFile(specPath, []byte(invalidSpecContent), 0644); err != nil {
				t.Fatalf("Failed to write spec.yaml for %s: %v", entry, err)
			}

			// Both entry points use the same ValidateSpecSchema function
			errors[entry] = ValidateSpecSchema(specDir)
		}

		// Verify both entry points produce errors
		for entry, err := range errors {
			assertSchemaValidationErrorFormat(t, err, "spec.yaml", "missing required field: feature")
			t.Logf("%s error: %v", entry, err)
		}

		// Verify error formats are identical (prefix and structure)
		runErr := errors["run-a"].Error()
		specifyErr := errors["specify-standalone"].Error()
		if runErr != specifyErr {
			t.Errorf("Error format mismatch across entry points:\n  run -a: %s\n  specify: %s", runErr, specifyErr)
		}
	})

	// Test that the same invalid plan produces consistent errors
	t.Run("plan validation error format consistency", func(t *testing.T) {
		t.Parallel()

		invalidPlanContent := `summary: "test"
technical_context:
  language: "Go"
_meta:
  artifact_type: "plan"
`

		entries := []string{"prep", "plan-standalone"}
		errors := make(map[string]error)

		for _, entry := range entries {
			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory for %s: %v", entry, err)
			}

			planPath := filepath.Join(specDir, "plan.yaml")
			if err := os.WriteFile(planPath, []byte(invalidPlanContent), 0644); err != nil {
				t.Fatalf("Failed to write plan.yaml for %s: %v", entry, err)
			}

			errors[entry] = ValidatePlanSchema(specDir)
		}

		for entry, err := range errors {
			assertSchemaValidationErrorFormat(t, err, "plan.yaml", "missing required field: plan")
			t.Logf("%s error: %v", entry, err)
		}

		prepErr := errors["prep"].Error()
		planErr := errors["plan-standalone"].Error()
		if prepErr != planErr {
			t.Errorf("Error format mismatch across entry points:\n  prep: %s\n  plan: %s", prepErr, planErr)
		}
	})

	// Test that the same invalid tasks produces consistent errors
	t.Run("tasks validation error format consistency", func(t *testing.T) {
		t.Parallel()

		invalidTasksContent := `summary:
  total_tasks: 0
phases: []
_meta:
  artifact_type: "tasks"
`

		entries := []string{"run-a", "tasks-standalone"}
		errors := make(map[string]error)

		for _, entry := range entries {
			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory for %s: %v", entry, err)
			}

			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if err := os.WriteFile(tasksPath, []byte(invalidTasksContent), 0644); err != nil {
				t.Fatalf("Failed to write tasks.yaml for %s: %v", entry, err)
			}

			errors[entry] = ValidateTasksSchema(specDir)
		}

		for entry, err := range errors {
			assertSchemaValidationErrorFormat(t, err, "tasks.yaml", "missing required field: tasks")
			t.Logf("%s error: %v", entry, err)
		}

		runErr := errors["run-a"].Error()
		tasksErr := errors["tasks-standalone"].Error()
		if runErr != tasksErr {
			t.Errorf("Error format mismatch across entry points:\n  run -a: %s\n  tasks: %s", runErr, tasksErr)
		}
	})
}

// =============================================================================
// Mock-Based Orchestrator Delegation Tests
// =============================================================================
// These tests verify that WorkflowOrchestrator correctly delegates to injected executors.

// TestOrchestratorDelegation_StageExecutor verifies orchestrator delegates to StageExecutor.
func TestOrchestratorDelegation_StageExecutor(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		setup   func(*MockStageExecutor)
		action  func(*WorkflowOrchestrator) error
		verify  func(*testing.T, *MockStageExecutor)
		wantErr bool
	}{
		"ExecuteConstitution delegates to StageExecutor": {
			setup: func(m *MockStageExecutor) {},
			action: func(orch *WorkflowOrchestrator) error {
				return orch.ExecuteConstitution("test prompt")
			},
			verify: func(t *testing.T, m *MockStageExecutor) {
				if len(m.ConstitutionCalls) != 1 {
					t.Errorf("ConstitutionCalls = %d, want 1", len(m.ConstitutionCalls))
				}
				if m.ConstitutionCalls[0] != "test prompt" {
					t.Errorf("ConstitutionCalls[0] = %q, want %q", m.ConstitutionCalls[0], "test prompt")
				}
			},
			wantErr: false,
		},
		"ExecuteConstitution propagates errors": {
			setup: func(m *MockStageExecutor) {
				m.ConstitutionError = fmt.Errorf("constitution failed")
			},
			action: func(orch *WorkflowOrchestrator) error {
				return orch.ExecuteConstitution("")
			},
			verify: func(t *testing.T, m *MockStageExecutor) {
				if len(m.ConstitutionCalls) != 1 {
					t.Errorf("ConstitutionCalls = %d, want 1", len(m.ConstitutionCalls))
				}
			},
			wantErr: true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			mockStage := NewMockStageExecutor()
			tt.setup(mockStage)

			cfg := &config.Configuration{
				CustomAgent: &cliagent.CustomAgentConfig{
					Command: "echo",
					Args:    []string{"{{PROMPT}}"},
				},
				SpecsDir:   t.TempDir(),
				MaxRetries: 3,
				StateDir:   filepath.Join(t.TempDir(), "state"),
			}

			orch := NewWorkflowOrchestratorWithExecutors(cfg, ExecutorOptions{
				StageExecutor: mockStage,
			})
			orch.SkipPreflight = true

			err := tt.action(orch)

			if (err != nil) != tt.wantErr {
				t.Errorf("error = %v, wantErr = %v", err, tt.wantErr)
			}

			tt.verify(t, mockStage)
		})
	}
}

// TestOrchestratorDelegation_PhaseExecutor verifies orchestrator delegates to PhaseExecutor.
func TestOrchestratorDelegation_PhaseExecutor(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		setupSpec func(string)
		setup     func(*MockPhaseExecutor)
		action    func(*WorkflowOrchestrator, string) error
		verify    func(*testing.T, *MockPhaseExecutor)
		wantErr   bool
	}{
		"ExecuteImplementSinglePhase delegates to PhaseExecutor": {
			setupSpec: func(specDir string) {
				writeTestTasksForDelegation(t, specDir)
			},
			setup: func(m *MockPhaseExecutor) {},
			action: func(orch *WorkflowOrchestrator, specName string) error {
				// Create minimal metadata
				metadata := &spec.Metadata{
					Number:    "001",
					Name:      "test",
					Directory: filepath.Join(orch.SpecsDir, specName),
				}
				return orch.ExecuteImplementSinglePhase(specName, metadata, "", 1)
			},
			verify: func(t *testing.T, m *MockPhaseExecutor) {
				if len(m.SinglePhaseCalls) != 1 {
					t.Errorf("SinglePhaseCalls = %d, want 1", len(m.SinglePhaseCalls))
					return
				}
				if m.SinglePhaseCalls[0].PhaseNumber != 1 {
					t.Errorf("PhaseNumber = %d, want 1", m.SinglePhaseCalls[0].PhaseNumber)
				}
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			specName := "001-test"
			specDir := filepath.Join(specsDir, specName)
			os.MkdirAll(specDir, 0755)

			tt.setupSpec(specDir)

			mockPhase := NewMockPhaseExecutor()
			tt.setup(mockPhase)

			cfg := &config.Configuration{
				CustomAgent: &cliagent.CustomAgentConfig{
					Command: "echo",
					Args:    []string{"{{PROMPT}}"},
				},
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orch := NewWorkflowOrchestratorWithExecutors(cfg, ExecutorOptions{
				PhaseExecutor: mockPhase,
			})
			orch.SkipPreflight = true

			err := tt.action(orch, specName)

			if (err != nil) != tt.wantErr {
				t.Errorf("error = %v, wantErr = %v", err, tt.wantErr)
			}

			tt.verify(t, mockPhase)
		})
	}
}

// TestOrchestratorDelegation_TaskExecutor verifies orchestrator delegates to TaskExecutor.
func TestOrchestratorDelegation_TaskExecutor(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		setupSpec func(string)
		setup     func(*MockTaskExecutor)
		action    func(*WorkflowOrchestrator, string) error
		verify    func(*testing.T, *MockTaskExecutor)
		wantErr   bool
	}{
		"ExecuteImplementWithTasks delegates to TaskExecutor": {
			setupSpec: func(specDir string) {
				writeTestTasksForDelegation(t, specDir)
			},
			setup: func(m *MockTaskExecutor) {
				m.PrepareResult = []validation.TaskItem{
					{ID: "T001", Title: "Test Task", Status: "Pending"},
				}
				m.PrepareTotalTasks = 1
			},
			action: func(orch *WorkflowOrchestrator, specName string) error {
				metadata := &spec.Metadata{
					Number:    "001",
					Name:      "test",
					Directory: filepath.Join(orch.SpecsDir, specName),
				}
				return orch.ExecuteImplementWithTasks(specName, metadata, "", "")
			},
			verify: func(t *testing.T, m *MockTaskExecutor) {
				if len(m.PrepareCalls) != 1 {
					t.Errorf("PrepareCalls = %d, want 1", len(m.PrepareCalls))
				}
				if len(m.TaskLoopCalls) != 1 {
					t.Errorf("TaskLoopCalls = %d, want 1", len(m.TaskLoopCalls))
				}
			},
			wantErr: false,
		},
		"ExecuteImplementWithTasks propagates PrepareTaskExecution error": {
			setupSpec: func(specDir string) {
				writeTestTasksForDelegation(t, specDir)
			},
			setup: func(m *MockTaskExecutor) {
				m.PrepareError = fmt.Errorf("prepare failed")
			},
			action: func(orch *WorkflowOrchestrator, specName string) error {
				metadata := &spec.Metadata{
					Number:    "001",
					Name:      "test",
					Directory: filepath.Join(orch.SpecsDir, specName),
				}
				return orch.ExecuteImplementWithTasks(specName, metadata, "", "")
			},
			verify: func(t *testing.T, m *MockTaskExecutor) {
				if len(m.PrepareCalls) != 1 {
					t.Errorf("PrepareCalls = %d, want 1", len(m.PrepareCalls))
				}
				// TaskLoop should not be called if Prepare fails
				if len(m.TaskLoopCalls) != 0 {
					t.Errorf("TaskLoopCalls = %d, want 0", len(m.TaskLoopCalls))
				}
			},
			wantErr: true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			specName := "001-test"
			specDir := filepath.Join(specsDir, specName)
			os.MkdirAll(specDir, 0755)

			tt.setupSpec(specDir)

			mockTask := NewMockTaskExecutor()
			tt.setup(mockTask)

			cfg := &config.Configuration{
				CustomAgent: &cliagent.CustomAgentConfig{
					Command: "echo",
					Args:    []string{"{{PROMPT}}"},
				},
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orch := NewWorkflowOrchestratorWithExecutors(cfg, ExecutorOptions{
				TaskExecutor: mockTask,
			})
			orch.SkipPreflight = true

			err := tt.action(orch, specName)

			if (err != nil) != tt.wantErr {
				t.Errorf("error = %v, wantErr = %v", err, tt.wantErr)
			}

			tt.verify(t, mockTask)
		})
	}
}

// writeTestTasksForDelegation writes a minimal tasks.yaml for delegation tests.
func writeTestTasksForDelegation(t *testing.T, specDir string) {
	t.Helper()
	tasksContent := `tasks:
  branch: "001-test"
summary:
  total_tasks: 1
phases:
  - number: 1
    title: "Test Phase"
    purpose: "Testing delegation"
    tasks:
      - id: "T001"
        title: "Test Task"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
_meta:
  artifact_type: "tasks"
`
	if err := os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644); err != nil {
		t.Fatalf("Failed to write tasks.yaml: %v", err)
	}
}
