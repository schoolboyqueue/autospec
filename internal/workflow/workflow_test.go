package workflow

import (
	"fmt"
	"os"
	"path/filepath"
	"runtime"
	"strings"
	"testing"

	"github.com/ariel-frischer/autospec/internal/config"
	"github.com/ariel-frischer/autospec/internal/spec"
	"github.com/ariel-frischer/autospec/internal/validation"
)

func TestNewWorkflowOrchestrator(t *testing.T) {
	cfg := &config.Configuration{
		ClaudeCmd:  "claude",
		ClaudeArgs: []string{"-p"},
		SpecsDir:   "./specs",
		MaxRetries: 3,
		StateDir:   "~/.autospec/state",
	}

	orchestrator := NewWorkflowOrchestrator(cfg)

	if orchestrator == nil {
		t.Fatal("NewWorkflowOrchestrator() returned nil")
	}

	if orchestrator.SpecsDir != "./specs" {
		t.Errorf("SpecsDir = %v, want './specs'", orchestrator.SpecsDir)
	}

	if orchestrator.Executor == nil {
		t.Error("Executor should not be nil")
	}
}

func TestWorkflowOrchestrator_Configuration(t *testing.T) {
	tmpDir := t.TempDir()

	cfg := &config.Configuration{
		ClaudeCmd:  "claude",
		ClaudeArgs: []string{"-p"},
		SpecsDir:   tmpDir,
		MaxRetries: 3,
		StateDir:   filepath.Join(tmpDir, "state"),
	}

	orchestrator := NewWorkflowOrchestrator(cfg)

	if orchestrator.SpecsDir != tmpDir {
		t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, tmpDir)
	}

	if orchestrator.Config.MaxRetries != 3 {
		t.Errorf("MaxRetries = %v, want 3", orchestrator.Config.MaxRetries)
	}
}

// TestExecutePlanWithPrompt tests that plan commands properly format prompts
func TestExecutePlanWithPrompt(t *testing.T) {
	tests := map[string]struct {
		prompt      string
		wantCommand string
	}{
		"no prompt": {
			prompt:      "",
			wantCommand: "/autospec.plan",
		},
		"simple prompt": {
			prompt:      "Focus on security",
			wantCommand: `/autospec.plan "Focus on security"`,
		},
		"prompt with quotes": {
			prompt:      "Use 'best practices' for auth",
			wantCommand: `/autospec.plan "Use 'best practices' for auth"`,
		},
		"multiline prompt": {
			prompt: `Consider these aspects:
  - Performance
  - Scalability`,
			wantCommand: `/autospec.plan "Consider these aspects:
  - Performance
  - Scalability"`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// This test verifies the command construction logic
			command := "/autospec.plan"
			if tc.prompt != "" {
				command = "/autospec.plan \"" + tc.prompt + "\""
			}

			if command != tc.wantCommand {
				t.Errorf("command = %q, want %q", command, tc.wantCommand)
			}
		})
	}
}

// TestExecuteTasksWithPrompt tests that tasks commands properly format prompts
func TestExecuteTasksWithPrompt(t *testing.T) {
	tests := map[string]struct {
		prompt      string
		wantCommand string
	}{
		"no prompt": {
			prompt:      "",
			wantCommand: "/autospec.tasks",
		},
		"simple prompt": {
			prompt:      "Break into small steps",
			wantCommand: `/autospec.tasks "Break into small steps"`,
		},
		"prompt with quotes": {
			prompt:      "Make tasks 'granular' and testable",
			wantCommand: `/autospec.tasks "Make tasks 'granular' and testable"`,
		},
		"complex prompt": {
			prompt: `Requirements:
  - Each task < 1 hour
  - Include testing`,
			wantCommand: `/autospec.tasks "Requirements:
  - Each task < 1 hour
  - Include testing"`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// This test verifies the command construction logic
			command := "/autospec.tasks"
			if tc.prompt != "" {
				command = "/autospec.tasks \"" + tc.prompt + "\""
			}

			if command != tc.wantCommand {
				t.Errorf("command = %q, want %q", command, tc.wantCommand)
			}
		})
	}
}

// TestSpecNameFormat tests that spec names are formatted correctly with number prefix
func TestSpecNameFormat(t *testing.T) {
	tests := map[string]struct {
		metadata     *spec.Metadata
		wantSpecName string
	}{
		"spec with three-digit number": {
			metadata: &spec.Metadata{
				Number: "003",
				Name:   "command-timeout",
			},
			wantSpecName: "003-command-timeout",
		},
		"spec with two-digit number": {
			metadata: &spec.Metadata{
				Number: "002",
				Name:   "go-binary-migration",
			},
			wantSpecName: "002-go-binary-migration",
		},
		"spec with single digit number": {
			metadata: &spec.Metadata{
				Number: "001",
				Name:   "initial-feature",
			},
			wantSpecName: "001-initial-feature",
		},
		"spec with hyphenated name": {
			metadata: &spec.Metadata{
				Number: "123",
				Name:   "multi-word-feature-name",
			},
			wantSpecName: "123-multi-word-feature-name",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Test the format string used in the workflow
			specName := fmt.Sprintf("%s-%s", tt.metadata.Number, tt.metadata.Name)

			if specName != tt.wantSpecName {
				t.Errorf("spec name format = %q, want %q", specName, tt.wantSpecName)
			}
		})
	}
}

// TestSpecDirectoryConstruction tests that spec directories are constructed correctly
func TestSpecDirectoryConstruction(t *testing.T) {
	tmpDir := t.TempDir()

	tests := map[string]struct {
		specsDir    string
		specName    string
		wantSpecDir string
	}{
		"full spec name with number": {
			specsDir:    tmpDir,
			specName:    "003-command-timeout",
			wantSpecDir: filepath.Join(tmpDir, "003-command-timeout"),
		},
		"relative specs dir": {
			specsDir:    "./specs",
			specName:    "002-go-binary-migration",
			wantSpecDir: filepath.Join("./specs", "002-go-binary-migration"),
		},
		"absolute specs dir": {
			specsDir:    "/tmp/specs",
			specName:    "001-initial-feature",
			wantSpecDir: filepath.Join("/tmp/specs", "001-initial-feature"),
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Test directory construction used in executor
			specDir := filepath.Join(tt.specsDir, tt.specName)

			if specDir != tt.wantSpecDir {
				t.Errorf("spec directory = %q, want %q", specDir, tt.wantSpecDir)
			}
		})
	}
}

// TestSpecNameFromMetadata verifies spec name is constructed correctly from metadata
func TestSpecNameFromMetadata(t *testing.T) {
	// Create a temporary directory structure
	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "003-command-timeout")

	// Create the directory structure
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create test directory: %v", err)
	}

	// Create a spec.md file
	specFile := filepath.Join(specDir, "spec.md")
	if err := os.WriteFile(specFile, []byte("# Test Spec"), 0644); err != nil {
		t.Fatalf("Failed to create spec.md: %v", err)
	}

	// Test getting metadata
	metadata, err := spec.GetSpecMetadata(specsDir, "003-command-timeout")
	if err != nil {
		t.Fatalf("GetSpecMetadata() error = %v", err)
	}

	// Verify the metadata has correct values
	if metadata.Number != "003" {
		t.Errorf("metadata.Number = %q, want %q", metadata.Number, "003")
	}

	if metadata.Name != "command-timeout" {
		t.Errorf("metadata.Name = %q, want %q", metadata.Name, "command-timeout")
	}

	// Test constructing the full spec name (this is what the fix addresses)
	fullSpecName := fmt.Sprintf("%s-%s", metadata.Number, metadata.Name)
	expectedSpecName := "003-command-timeout"

	if fullSpecName != expectedSpecName {
		t.Errorf("full spec name = %q, want %q", fullSpecName, expectedSpecName)
	}

	// Verify the constructed directory path is correct
	constructedDir := filepath.Join(specsDir, fullSpecName)
	if constructedDir != specDir {
		t.Errorf("constructed directory = %q, want %q", constructedDir, specDir)
	}
}

// TestExecuteImplementWithTaskCommand tests that task-level implement commands are properly formatted
func TestExecuteImplementWithTaskCommand(t *testing.T) {
	tests := map[string]struct {
		taskID      string
		prompt      string
		wantCommand string
	}{
		"task without prompt": {
			taskID:      "T001",
			prompt:      "",
			wantCommand: "/autospec.implement --task T001",
		},
		"task with prompt": {
			taskID:      "T002",
			prompt:      "Focus on error handling",
			wantCommand: `/autospec.implement --task T002 "Focus on error handling"`,
		},
		"task with complex prompt": {
			taskID: "T003",
			prompt: "Consider:\n  - Performance\n  - Scalability",
			wantCommand: `/autospec.implement --task T003 "Consider:
  - Performance
  - Scalability"`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Build command with task filter (mirrors executeSingleTaskSession logic)
			command := fmt.Sprintf("/autospec.implement --task %s", tc.taskID)
			if tc.prompt != "" {
				command = fmt.Sprintf("/autospec.implement --task %s \"%s\"", tc.taskID, tc.prompt)
			}

			if command != tc.wantCommand {
				t.Errorf("command = %q, want %q", command, tc.wantCommand)
			}
		})
	}
}

// TestTaskModeDispatch tests that TaskMode correctly routes to ExecuteImplementWithTasks
func TestTaskModeDispatch(t *testing.T) {
	opts := PhaseExecutionOptions{
		TaskMode: true,
		FromTask: "T003",
	}

	if opts.Mode() != ModeAllTasks {
		t.Errorf("Mode() = %v, want ModeAllTasks", opts.Mode())
	}
}

// TestTaskModeWithFromTask tests that FromTask is correctly included in options
func TestTaskModeWithFromTask(t *testing.T) {
	tests := map[string]struct {
		opts     PhaseExecutionOptions
		wantMode PhaseExecutionMode
	}{
		"task mode enabled": {
			opts: PhaseExecutionOptions{
				TaskMode: true,
			},
			wantMode: ModeAllTasks,
		},
		"task mode with from-task": {
			opts: PhaseExecutionOptions{
				TaskMode: true,
				FromTask: "T005",
			},
			wantMode: ModeAllTasks,
		},
		"task mode takes precedence over phases": {
			opts: PhaseExecutionOptions{
				TaskMode:     true,
				RunAllPhases: true, // This should be ignored
			},
			wantMode: ModeAllTasks,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			if got := tt.opts.Mode(); got != tt.wantMode {
				t.Errorf("Mode() = %v, want %v", got, tt.wantMode)
			}
		})
	}
}

// TestTaskCompletionValidation tests that task completion is properly validated
func TestTaskCompletionValidation(t *testing.T) {
	tests := map[string]struct {
		taskStatus  string
		expectError bool
	}{
		"task completed": {
			taskStatus:  "Completed",
			expectError: false,
		},
		"task completed lowercase": {
			taskStatus:  "completed",
			expectError: false,
		},
		"task pending": {
			taskStatus:  "Pending",
			expectError: true,
		},
		"task in progress": {
			taskStatus:  "InProgress",
			expectError: true,
		},
		"task blocked": {
			taskStatus:  "Blocked",
			expectError: true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Create isolated directory for this subtest
			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			specDir := filepath.Join(specsDir, "001-test-feature")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create test directory: %v", err)
			}

			// Create tasks.yaml with specified status
			tasksContent := fmt.Sprintf(`tasks:
    branch: "001-test-feature"
    created: "2025-01-01"
summary:
    total_tasks: 1
    total_phases: 1
phases:
    - number: 1
      title: "Test Phase"
      purpose: "Testing"
      tasks:
        - id: "T001"
          title: "Test Task"
          status: "%s"
          type: "implementation"
          parallel: false
          story_id: null
          file_path: "test.go"
          dependencies: []
          acceptance_criteria:
            - "Test passes"
_meta:
    version: "1.0.0"
    artifact_type: "tasks"
`, tt.taskStatus)

			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
				t.Fatalf("Failed to create tasks.yaml: %v", err)
			}

			// Simulate the validation logic from executeSingleTaskSession
			allTasks, err := validation.GetAllTasks(tasksPath)
			if err != nil {
				t.Fatalf("Failed to get tasks: %v", err)
			}

			task, err := validation.GetTaskByID(allTasks, "T001")
			if err != nil {
				t.Fatalf("Failed to get task: %v", err)
			}

			// Check if task is completed (same logic as in executeSingleTaskSession)
			isCompleted := task.Status == "Completed" || task.Status == "completed"

			if tt.expectError && isCompleted {
				t.Errorf("Expected validation error for status %q, but task was considered completed", tt.taskStatus)
			}
			if !tt.expectError && !isCompleted {
				t.Errorf("Expected no error for status %q, but task was not considered completed", tt.taskStatus)
			}
		})
	}
}

// TestTaskCompletionValidationAfterSession tests validation after task session completes
func TestTaskCompletionValidationAfterSession(t *testing.T) {
	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "001-test-feature")

	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create test directory: %v", err)
	}

	// Create tasks.yaml with incomplete task
	tasksContent := `tasks:
    branch: "001-test-feature"
    created: "2025-01-01"
summary:
    total_tasks: 1
    total_phases: 1
phases:
    - number: 1
      title: "Test Phase"
      purpose: "Testing"
      tasks:
        - id: "T001"
          title: "Test Task"
          status: "InProgress"
          type: "implementation"
          parallel: false
          story_id: null
          file_path: "test.go"
          dependencies: []
          acceptance_criteria:
            - "Test passes"
_meta:
    version: "1.0.0"
    artifact_type: "tasks"
`

	tasksPath := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
		t.Fatalf("Failed to create tasks.yaml: %v", err)
	}

	// Simulate the validation function that would be passed to ExecutePhase
	validateFunc := func(specDir string) error {
		tasksPath := filepath.Join(specDir, "tasks.yaml")
		allTasks, err := validation.GetAllTasks(tasksPath)
		if err != nil {
			return err
		}

		task, err := validation.GetTaskByID(allTasks, "T001")
		if err != nil {
			return err
		}

		if task.Status != "Completed" && task.Status != "completed" {
			return fmt.Errorf("task %s not completed (status: %s)", task.ID, task.Status)
		}
		return nil
	}

	// Test that validation fails for incomplete task
	err := validateFunc(specDir)
	if err == nil {
		t.Error("Expected validation to fail for incomplete task, but it passed")
	} else {
		expectedMsg := "task T001 not completed (status: InProgress)"
		if err.Error() != expectedMsg {
			t.Errorf("Expected error message %q, got %q", expectedMsg, err.Error())
		}
	}

	// Now update the task to Completed and verify validation passes
	tasksContentCompleted := `tasks:
    branch: "001-test-feature"
    created: "2025-01-01"
summary:
    total_tasks: 1
    total_phases: 1
phases:
    - number: 1
      title: "Test Phase"
      purpose: "Testing"
      tasks:
        - id: "T001"
          title: "Test Task"
          status: "Completed"
          type: "implementation"
          parallel: false
          story_id: null
          file_path: "test.go"
          dependencies: []
          acceptance_criteria:
            - "Test passes"
_meta:
    version: "1.0.0"
    artifact_type: "tasks"
`

	if err := os.WriteFile(tasksPath, []byte(tasksContentCompleted), 0644); err != nil {
		t.Fatalf("Failed to update tasks.yaml: %v", err)
	}

	// Test that validation passes for completed task
	err = validateFunc(specDir)
	if err != nil {
		t.Errorf("Expected validation to pass for completed task, got error: %v", err)
	}
}

// TestTaskRetryExhaustedBehavior tests that retry exhaustion is handled correctly
func TestTaskRetryExhaustedBehavior(t *testing.T) {
	// Test the StageResult structure that indicates retry exhaustion
	result := &StageResult{
		Stage:      StageImplement,
		Success:    false,
		Exhausted:  true,
		RetryCount: 3,
		Error:      fmt.Errorf("task T001 not completed (status: InProgress)"),
	}

	// Verify exhausted state
	if !result.Exhausted {
		t.Error("Expected Exhausted to be true")
	}

	if result.RetryCount != 3 {
		t.Errorf("Expected RetryCount to be 3, got %d", result.RetryCount)
	}

	// Test the error message format that would be shown to user
	expectedErrorFormat := "task %s exhausted retries: %w"
	errMsg := fmt.Errorf(expectedErrorFormat, "T001", result.Error)
	if errMsg == nil {
		t.Error("Expected error message to be generated")
	}

	// Verify the message includes key information
	errStr := errMsg.Error()
	if !strings.Contains(errStr, "T001") {
		t.Errorf("Error message should contain task ID, got: %s", errStr)
	}
	if !strings.Contains(errStr, "exhausted retries") {
		t.Errorf("Error message should mention exhausted retries, got: %s", errStr)
	}
}

// TestExecuteImplementWithPrompt tests that implement commands properly format prompts
func TestExecuteImplementWithPrompt(t *testing.T) {
	tests := map[string]struct {
		prompt      string
		resume      bool
		wantCommand string
	}{
		"no prompt, no resume": {
			prompt:      "",
			resume:      false,
			wantCommand: "/autospec.implement",
		},
		"simple prompt, no resume": {
			prompt:      "Focus on documentation",
			resume:      false,
			wantCommand: `/autospec.implement "Focus on documentation"`,
		},
		"no prompt, with resume": {
			prompt:      "",
			resume:      true,
			wantCommand: "/autospec.implement --resume",
		},
		"prompt with resume": {
			prompt:      "Complete remaining tasks",
			resume:      true,
			wantCommand: `/autospec.implement --resume "Complete remaining tasks"`,
		},
		"prompt with quotes": {
			prompt:      "Use 'best practices' for tests",
			resume:      false,
			wantCommand: `/autospec.implement "Use 'best practices' for tests"`,
		},
		"multiline prompt": {
			prompt: `Focus on:
  - Error handling
  - Tests`,
			resume: false,
			wantCommand: `/autospec.implement "Focus on:
  - Error handling
  - Tests"`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// This test verifies the command construction logic
			command := "/autospec.implement"
			if tc.resume {
				command += " --resume"
			}
			if tc.prompt != "" {
				command = "/autospec.implement \"" + tc.prompt + "\""
				if tc.resume {
					// If both resume and prompt, append resume after prompt
					command = "/autospec.implement --resume \"" + tc.prompt + "\""
				}
			}

			if command != tc.wantCommand {
				t.Errorf("command = %q, want %q", command, tc.wantCommand)
			}
		})
	}
}

// TestOrchestratorDebugLog tests the orchestrator debug logging function
func TestOrchestratorDebugLog(t *testing.T) {
	t.Parallel()

	cfg := &config.Configuration{
		ClaudeCmd:  "claude",
		SpecsDir:   "./specs",
		MaxRetries: 3,
		StateDir:   "~/.autospec/state",
	}

	// Test with debug disabled
	orchestrator := NewWorkflowOrchestrator(cfg)
	orchestrator.Debug = false
	// This should not panic or error
	orchestrator.debugLog("Test message: %s", "arg")

	// Test with debug enabled
	orchestrator.Debug = true
	// This should also not panic (just prints to stdout)
	orchestrator.debugLog("Debug enabled: %d", 123)
}

// TestShouldSkipTask tests the task skip logic
func TestShouldSkipTask(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		task       validation.TaskItem
		idx        int
		totalTasks int
		wantSkip   bool
	}{
		"completed task": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "Completed"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   true,
		},
		"completed lowercase": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "completed"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   true,
		},
		"blocked task": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "Blocked"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   true,
		},
		"blocked lowercase": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "blocked"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   true,
		},
		"pending task": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "Pending"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   false,
		},
		"in progress task": {
			task:       validation.TaskItem{ID: "T001", Title: "Test", Status: "InProgress"},
			idx:        0,
			totalTasks: 3,
			wantSkip:   false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			result := shouldSkipTask(tt.task, tt.idx, tt.totalTasks)
			if result != tt.wantSkip {
				t.Errorf("shouldSkipTask() = %v, want %v", result, tt.wantSkip)
			}
		})
	}
}

// TestGetTaskIDsForPhase tests task ID extraction for a phase
func TestGetTaskIDsForPhase(t *testing.T) {
	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "001-test")
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create test directory: %v", err)
	}

	tasksContent := `tasks:
    branch: "001-test"
    created: "2025-01-01"
summary:
    total_tasks: 3
    total_phases: 2
phases:
    - number: 1
      title: "Phase 1"
      purpose: "First phase"
      tasks:
        - id: "T001"
          title: "Task 1"
          status: "Pending"
          type: "implementation"
          parallel: false
          dependencies: []
        - id: "T002"
          title: "Task 2"
          status: "Pending"
          type: "test"
          parallel: false
          dependencies: []
    - number: 2
      title: "Phase 2"
      purpose: "Second phase"
      tasks:
        - id: "T003"
          title: "Task 3"
          status: "Pending"
          type: "implementation"
          parallel: false
          dependencies: ["T001"]
_meta:
    version: "1.0.0"
    artifact_type: "tasks"
`

	tasksPath := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
		t.Fatalf("Failed to create tasks.yaml: %v", err)
	}

	tests := map[string]struct {
		phaseNumber int
		wantIDs     []string
	}{
		"phase 1 tasks": {
			phaseNumber: 1,
			wantIDs:     []string{"T001", "T002"},
		},
		"phase 2 tasks": {
			phaseNumber: 2,
			wantIDs:     []string{"T003"},
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			taskIDs := getTaskIDsForPhase(tasksPath, tt.phaseNumber)

			if len(taskIDs) != len(tt.wantIDs) {
				t.Errorf("getTaskIDsForPhase() returned %d IDs, want %d", len(taskIDs), len(tt.wantIDs))
				return
			}

			for i, id := range taskIDs {
				if id != tt.wantIDs[i] {
					t.Errorf("taskIDs[%d] = %q, want %q", i, id, tt.wantIDs[i])
				}
			}
		})
	}
}

// TestGetUpdatedPhaseInfo tests phase info retrieval
func TestGetUpdatedPhaseInfo(t *testing.T) {
	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "001-test")
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create test directory: %v", err)
	}

	tasksContent := `tasks:
    branch: "001-test"
    created: "2025-01-01"
summary:
    total_tasks: 2
    total_phases: 2
phases:
    - number: 1
      title: "Phase 1"
      purpose: "First phase"
      tasks:
        - id: "T001"
          title: "Task 1"
          status: "Completed"
          type: "implementation"
          parallel: false
          dependencies: []
    - number: 2
      title: "Phase 2"
      purpose: "Second phase"
      tasks:
        - id: "T002"
          title: "Task 2"
          status: "Pending"
          type: "implementation"
          parallel: false
          dependencies: []
_meta:
    version: "1.0.0"
    artifact_type: "tasks"
`

	tasksPath := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
		t.Fatalf("Failed to create tasks.yaml: %v", err)
	}

	tests := map[string]struct {
		phaseNumber int
		wantNil     bool
	}{
		"existing phase 1": {
			phaseNumber: 1,
			wantNil:     false,
		},
		"existing phase 2": {
			phaseNumber: 2,
			wantNil:     false,
		},
		"non-existent phase 99": {
			phaseNumber: 99,
			wantNil:     true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			result := getUpdatedPhaseInfo(tasksPath, tt.phaseNumber)

			if tt.wantNil && result != nil {
				t.Errorf("getUpdatedPhaseInfo() = %v, want nil", result)
			}
			if !tt.wantNil && result == nil {
				t.Errorf("getUpdatedPhaseInfo() = nil, want non-nil")
			}
			if result != nil && result.Number != tt.phaseNumber {
				t.Errorf("getUpdatedPhaseInfo().Number = %d, want %d", result.Number, tt.phaseNumber)
			}
		})
	}
}

// TestBuildImplementCommand tests the implement command builder
func TestBuildImplementCommand(t *testing.T) {
	t.Parallel()

	cfg := &config.Configuration{
		ClaudeCmd:  "claude",
		SpecsDir:   "./specs",
		MaxRetries: 3,
		StateDir:   "~/.autospec/state",
	}

	orchestrator := NewWorkflowOrchestrator(cfg)

	tests := map[string]struct {
		resume      bool
		wantCommand string
	}{
		"without resume": {
			resume:      false,
			wantCommand: "/autospec.implement",
		},
		"with resume": {
			resume:      true,
			wantCommand: "/autospec.implement --resume",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			command := orchestrator.buildImplementCommand(tt.resume)
			if command != tt.wantCommand {
				t.Errorf("buildImplementCommand() = %q, want %q", command, tt.wantCommand)
			}
		})
	}
}

// TestPrintPhaseCompletion tests the phase completion message printing
func TestPrintPhaseCompletion(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		phaseNumber  int
		updatedPhase *validation.PhaseInfo
	}{
		"with updated phase info": {
			phaseNumber: 1,
			updatedPhase: &validation.PhaseInfo{
				Number:         1,
				Title:          "Setup",
				TotalTasks:     3,
				CompletedTasks: 3,
				BlockedTasks:   0,
			},
		},
		"without updated phase info": {
			phaseNumber:  2,
			updatedPhase: nil,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Just verify it doesn't panic
			printPhaseCompletion(tt.phaseNumber, tt.updatedPhase)
		})
	}
}

// TestMarkSpecCompletedAndPrint tests the spec completion marking
func TestMarkSpecCompletedAndPrint(t *testing.T) {
	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "001-test")
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create test directory: %v", err)
	}

	// Create a minimal spec.yaml
	specContent := `feature:
  name: "Test Feature"
  branch: "001-test"
  status: "Draft"
  created: "2025-01-01"
user_stories: []
requirements:
  functional: []
`
	specPath := filepath.Join(specDir, "spec.yaml")
	if err := os.WriteFile(specPath, []byte(specContent), 0644); err != nil {
		t.Fatalf("Failed to create spec.yaml: %v", err)
	}

	// This should not panic and should update the spec
	markSpecCompletedAndPrint(specDir)

	// Test with non-existent directory - should not panic
	markSpecCompletedAndPrint(filepath.Join(tmpDir, "nonexistent"))
}

// TestExecuteSpecify tests the ExecuteSpecify method
func TestExecuteSpecify(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		setupMock          func(*MockClaudeExecutor)
		wantErr            bool
		wantErrContains    string
	}{
		"successful spec generation": {
			featureDescription: "Add user authentication",
			setupMock: func(m *MockClaudeExecutor) {
				// Mock succeeds immediately
			},
			wantErr: false,
		},
		"empty feature description": {
			featureDescription: "",
			setupMock: func(m *MockClaudeExecutor) {
				// Mock succeeds immediately
			},
			wantErr: false, // Empty string is valid, just creates empty spec
		},
		"execution error": {
			featureDescription: "Test feature",
			setupMock: func(m *MockClaudeExecutor) {
				m.WithExecuteError(ErrMockExecute)
			},
			wantErr:         true,
			wantErrContains: "specify failed",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			if err := os.MkdirAll(specsDir, 0755); err != nil {
				t.Fatalf("Failed to create specs directory: %v", err)
			}

			// Create spec directory that would be created by claude
			specDir := filepath.Join(specsDir, "001-test-feature")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			// Create spec.yaml that would be created by claude
			specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
  input: "test feature"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`
			if err := os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644); err != nil {
				t.Fatalf("Failed to create spec.yaml: %v", err)
			}

			cfg := &config.Configuration{
				ClaudeCmd:  "claude",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			mock := NewMockClaudeExecutor()
			tt.setupMock(mock)

			orchestrator := NewWorkflowOrchestrator(cfg)
			orchestrator.Executor.Claude = &ClaudeExecutor{
				ClaudeCmd: "echo", // Use echo for testing
			}

			// For the successful case, we need to mock the entire execution
			if !tt.wantErr {
				// The command format test verifies the command is built correctly
				command := fmt.Sprintf("/autospec.specify \"%s\"", tt.featureDescription)
				if !strings.Contains(command, tt.featureDescription) {
					t.Errorf("command should contain feature description")
				}
			}
		})
	}
}

// TestExecutePlan tests the ExecutePlan method
func TestExecutePlan(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful plan with spec name": {
			specName: "001-test-feature",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// Create plan.yaml (simulating claude output)
				planContent := `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
			},
			wantErr: false,
		},
		"plan with custom prompt": {
			specName: "002-another-feature",
			prompt:   "Focus on security",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "002-another-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "002-another-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			if err := os.MkdirAll(specsDir, 0755); err != nil {
				t.Fatalf("Failed to create specs directory: %v", err)
			}

			// Setup test files
			tt.setupFiles(specsDir, tt.specName)

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify command format
			command := "/autospec.plan"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.plan \"%s\"", tt.prompt)
			}

			if tt.prompt != "" && !strings.Contains(command, tt.prompt) {
				t.Errorf("command should contain prompt, got: %s", command)
			}

			// Verify orchestrator is properly configured
			if orchestrator.SpecsDir != specsDir {
				t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, specsDir)
			}
		})
	}
}

// TestExecuteTasks tests the ExecuteTasks method
func TestExecuteTasks(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful tasks generation": {
			specName: "001-test-feature",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				// Create spec.yaml
				specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// Create plan.yaml
				planContent := `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				// Create tasks.yaml (simulating claude output)
				tasksContent := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test Phase"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test Task"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test passes"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"tasks with custom prompt": {
			specName: "002-another-feature",
			prompt:   "Break into small steps",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "002-another-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "002-another-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				tasksContent := `tasks:
  branch: "002-another-feature"
  created: "2025-01-01"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test Phase"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test Task"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test passes"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			if err := os.MkdirAll(specsDir, 0755); err != nil {
				t.Fatalf("Failed to create specs directory: %v", err)
			}

			// Setup test files
			tt.setupFiles(specsDir, tt.specName)

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify command format
			command := "/autospec.tasks"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.tasks \"%s\"", tt.prompt)
			}

			if tt.prompt != "" && !strings.Contains(command, tt.prompt) {
				t.Errorf("command should contain prompt, got: %s", command)
			}

			// Verify orchestrator is properly configured
			if orchestrator.SpecsDir != specsDir {
				t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, specsDir)
			}
		})
	}
}

// TestExecuteImplementModeDispatch tests the ExecuteImplement method mode dispatch
func TestExecuteImplementModeDispatch(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		phaseOpts PhaseExecutionOptions
		wantMode  PhaseExecutionMode
	}{
		"default mode": {
			phaseOpts: PhaseExecutionOptions{},
			wantMode:  ModeDefault,
		},
		"task mode": {
			phaseOpts: PhaseExecutionOptions{TaskMode: true},
			wantMode:  ModeAllTasks,
		},
		"task mode with from-task": {
			phaseOpts: PhaseExecutionOptions{TaskMode: true, FromTask: "T001"},
			wantMode:  ModeAllTasks,
		},
		"all phases mode": {
			phaseOpts: PhaseExecutionOptions{RunAllPhases: true},
			wantMode:  ModeAllPhases,
		},
		"single phase mode": {
			phaseOpts: PhaseExecutionOptions{SinglePhase: 1},
			wantMode:  ModeSinglePhase,
		},
		"from phase mode": {
			phaseOpts: PhaseExecutionOptions{FromPhase: 2},
			wantMode:  ModeFromPhase,
		},
		"task mode takes precedence": {
			phaseOpts: PhaseExecutionOptions{
				TaskMode:     true,
				RunAllPhases: true,
				SinglePhase:  1,
			},
			wantMode: ModeAllTasks,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			if got := tt.phaseOpts.Mode(); got != tt.wantMode {
				t.Errorf("Mode() = %v, want %v", got, tt.wantMode)
			}
		})
	}
}

// TestPhaseExecutionOptionsMode tests the Mode() method comprehensively
func TestPhaseExecutionOptionsMode(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		opts     PhaseExecutionOptions
		wantMode PhaseExecutionMode
	}{
		"empty options returns default": {
			opts:     PhaseExecutionOptions{},
			wantMode: ModeDefault,
		},
		"task mode true returns all tasks": {
			opts:     PhaseExecutionOptions{TaskMode: true},
			wantMode: ModeAllTasks,
		},
		"run all phases true returns all phases": {
			opts:     PhaseExecutionOptions{RunAllPhases: true},
			wantMode: ModeAllPhases,
		},
		"single phase > 0 returns single phase": {
			opts:     PhaseExecutionOptions{SinglePhase: 3},
			wantMode: ModeSinglePhase,
		},
		"from phase > 0 returns from phase": {
			opts:     PhaseExecutionOptions{FromPhase: 2},
			wantMode: ModeFromPhase,
		},
		"task mode has highest priority": {
			opts: PhaseExecutionOptions{
				TaskMode:     true,
				RunAllPhases: true,
				SinglePhase:  5,
				FromPhase:    3,
			},
			wantMode: ModeAllTasks,
		},
		"run all phases has second priority": {
			opts: PhaseExecutionOptions{
				RunAllPhases: true,
				SinglePhase:  5,
				FromPhase:    3,
			},
			wantMode: ModeAllPhases,
		},
		"single phase has third priority": {
			opts: PhaseExecutionOptions{
				SinglePhase: 5,
				FromPhase:   3,
			},
			wantMode: ModeSinglePhase,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			if got := tt.opts.Mode(); got != tt.wantMode {
				t.Errorf("Mode() = %v, want %v", got, tt.wantMode)
			}
		})
	}
}

// TestExecuteConstitution tests the ExecuteConstitution method
func TestExecuteConstitution(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		prompt         string
		wantCmdContain string
	}{
		"without prompt": {
			prompt:         "",
			wantCmdContain: "/autospec.constitution",
		},
		"with prompt": {
			prompt:         "Focus on testing",
			wantCmdContain: "Focus on testing",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := "/autospec.constitution"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.constitution \"%s\"", tt.prompt)
			}

			if !strings.Contains(command, tt.wantCmdContain) {
				t.Errorf("command %q should contain %q", command, tt.wantCmdContain)
			}
		})
	}
}

// TestExecuteClarify tests the ExecuteClarify method
func TestExecuteClarify(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName       string
		prompt         string
		wantCmdContain string
	}{
		"without prompt": {
			specName:       "001-test",
			prompt:         "",
			wantCmdContain: "/autospec.clarify",
		},
		"with prompt": {
			specName:       "001-test",
			prompt:         "Clarify security requirements",
			wantCmdContain: "Clarify security requirements",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := "/autospec.clarify"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.clarify \"%s\"", tt.prompt)
			}

			if !strings.Contains(command, tt.wantCmdContain) {
				t.Errorf("command %q should contain %q", command, tt.wantCmdContain)
			}
		})
	}
}

// TestExecuteChecklist tests the ExecuteChecklist method
func TestExecuteChecklist(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName       string
		prompt         string
		wantCmdContain string
	}{
		"without prompt": {
			specName:       "001-test",
			prompt:         "",
			wantCmdContain: "/autospec.checklist",
		},
		"with prompt": {
			specName:       "001-test",
			prompt:         "Include security checks",
			wantCmdContain: "Include security checks",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := "/autospec.checklist"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.checklist \"%s\"", tt.prompt)
			}

			if !strings.Contains(command, tt.wantCmdContain) {
				t.Errorf("command %q should contain %q", command, tt.wantCmdContain)
			}
		})
	}
}

// TestExecuteAnalyze tests the ExecuteAnalyze method
func TestExecuteAnalyze(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName       string
		prompt         string
		wantCmdContain string
	}{
		"without prompt": {
			specName:       "001-test",
			prompt:         "",
			wantCmdContain: "/autospec.analyze",
		},
		"with prompt": {
			specName:       "001-test",
			prompt:         "Focus on consistency",
			wantCmdContain: "Focus on consistency",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := "/autospec.analyze"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.analyze \"%s\"", tt.prompt)
			}

			if !strings.Contains(command, tt.wantCmdContain) {
				t.Errorf("command %q should contain %q", command, tt.wantCmdContain)
			}
		})
	}
}

// TestRunPreflightIfNeeded tests the runPreflightIfNeeded method
func TestRunPreflightIfNeeded(t *testing.T) {
	tests := map[string]struct {
		skipPreflight bool
		wantSkip      bool
	}{
		"skip when explicitly disabled": {
			skipPreflight: true,
			wantSkip:      true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Check if should skip when explicitly disabled
			shouldRun := ShouldRunPreflightChecks(tt.skipPreflight)
			if tt.wantSkip && shouldRun {
				t.Errorf("Expected preflight to be skipped with skipPreflight=%v, but it would run", tt.skipPreflight)
			}
		})
	}
}

// TestShouldRunPreflightChecksSkipFlag tests the skip flag behavior
func TestShouldRunPreflightChecksSkipFlag(t *testing.T) {
	t.Parallel()

	// When skipPreflight is true, should not run
	if ShouldRunPreflightChecks(true) {
		t.Error("Expected preflight to be skipped when skipPreflight=true")
	}
}

// TestExecuteSinglePhaseSession tests the executeSinglePhaseSession method
func TestExecuteSinglePhaseSession(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		phaseNumber int
		prompt      string
		setupFiles  func(string)
		wantErr     bool
	}{
		"valid phase with tasks": {
			phaseNumber: 1,
			prompt:      "",
			setupFiles: func(specDir string) {
				os.MkdirAll(specDir, 0755)
				// Create spec.yaml
				specContent := `feature:
  branch: "001-test"
  created: "2025-01-01"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// Create plan.yaml
				planContent := `plan:
  branch: "001-test"
  created: "2025-01-01"
summary: "Test"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				// Create tasks.yaml
				tasksContent := `tasks:
  branch: "001-test"
  created: "2025-01-01"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"empty phase with no tasks": {
			phaseNumber: 1,
			prompt:      "",
			setupFiles: func(specDir string) {
				os.MkdirAll(specDir, 0755)
				// Create spec.yaml
				specContent := `feature:
  branch: "001-test"
  created: "2025-01-01"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// Create plan.yaml
				planContent := `plan:
  branch: "001-test"
  created: "2025-01-01"
summary: "Test"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				// Create tasks.yaml with empty phase
				tasksContent := `tasks:
  branch: "001-test"
  created: "2025-01-01"
summary:
  total_tasks: 0
  total_phases: 1
phases:
  - number: 1
    title: "Empty"
    purpose: "Testing"
    tasks: []
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false, // Empty phase should not error, just skip
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			specDir := filepath.Join(specsDir, "001-test")

			tt.setupFiles(specDir)

			// Verify file setup
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if _, err := os.Stat(tasksPath); os.IsNotExist(err) {
				t.Fatalf("tasks.yaml not created: %v", err)
			}
		})
	}
}

// TestExecuteSingleTaskSession tests the executeSingleTaskSession method
func TestExecuteSingleTaskSession(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		taskID    string
		taskTitle string
		prompt    string
		wantCmd   string
	}{
		"task without prompt": {
			taskID:    "T001",
			taskTitle: "Test Task",
			prompt:    "",
			wantCmd:   "/autospec.implement --task T001",
		},
		"task with prompt": {
			taskID:    "T002",
			taskTitle: "Another Task",
			prompt:    "Focus on tests",
			wantCmd:   `/autospec.implement --task T002 "Focus on tests"`,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Build command
			command := fmt.Sprintf("/autospec.implement --task %s", tt.taskID)
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.implement --task %s \"%s\"", tt.taskID, tt.prompt)
			}

			if command != tt.wantCmd {
				t.Errorf("command = %q, want %q", command, tt.wantCmd)
			}
		})
	}
}

// TestGetOrderedTasksForExecution tests the getOrderedTasksForExecution method
func TestGetOrderedTasksForExecution(t *testing.T) {
	t.Parallel()

	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "001-test")
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create spec directory: %v", err)
	}

	tests := map[string]struct {
		tasksContent string
		wantCount    int
		wantErr      bool
	}{
		"valid tasks": {
			tasksContent: `tasks:
  branch: "001-test"
  created: "2025-01-01"
summary:
  total_tasks: 2
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "First"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
      - id: "T002"
        title: "Second"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: ["T001"]
        acceptance_criteria:
          - "Test"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`,
			wantCount: 2,
			wantErr:   false,
		},
		"empty tasks": {
			tasksContent: `tasks:
  branch: "001-test"
  created: "2025-01-01"
summary:
  total_tasks: 0
  total_phases: 1
phases:
  - number: 1
    title: "Empty"
    purpose: "Testing"
    tasks: []
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`,
			wantCount: 0,
			wantErr:   true, // No tasks found
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if err := os.WriteFile(tasksPath, []byte(tt.tasksContent), 0644); err != nil {
				t.Fatalf("Failed to write tasks.yaml: %v", err)
			}

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)
			orderedTasks, _, err := orchestrator.getOrderedTasksForExecution(tasksPath)

			if tt.wantErr {
				if err == nil {
					t.Error("Expected error but got nil")
				}
			} else {
				if err != nil {
					t.Errorf("Unexpected error: %v", err)
				}
				if len(orderedTasks) != tt.wantCount {
					t.Errorf("Task count = %d, want %d", len(orderedTasks), tt.wantCount)
				}
			}
		})
	}
}

// TestFindTaskStartIndex tests the findTaskStartIndex method
func TestFindTaskStartIndex(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		fromTask     string
		orderedTasks []validation.TaskItem
		allTasks     []validation.TaskItem
		wantIdx      int
		wantErr      bool
	}{
		"empty fromTask returns 0": {
			fromTask: "",
			orderedTasks: []validation.TaskItem{
				{ID: "T001", Title: "First", Status: "Pending"},
				{ID: "T002", Title: "Second", Status: "Pending"},
			},
			allTasks: []validation.TaskItem{
				{ID: "T001", Title: "First", Status: "Pending"},
				{ID: "T002", Title: "Second", Status: "Pending"},
			},
			wantIdx: 0,
			wantErr: false,
		},
		"valid fromTask returns correct index": {
			fromTask: "T002",
			orderedTasks: []validation.TaskItem{
				{ID: "T001", Title: "First", Status: "Completed"},
				{ID: "T002", Title: "Second", Status: "Pending"},
			},
			allTasks: []validation.TaskItem{
				{ID: "T001", Title: "First", Status: "Completed"},
				{ID: "T002", Title: "Second", Status: "Pending", Dependencies: []string{"T001"}},
			},
			wantIdx: 1,
			wantErr: false,
		},
		"non-existent task returns error": {
			fromTask: "T999",
			orderedTasks: []validation.TaskItem{
				{ID: "T001", Title: "First", Status: "Pending"},
			},
			allTasks: []validation.TaskItem{
				{ID: "T001", Title: "First", Status: "Pending"},
			},
			wantIdx: 0,
			wantErr: true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   filepath.Join(tmpDir, "specs"),
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)
			idx, err := orchestrator.findTaskStartIndex(tt.orderedTasks, tt.allTasks, tt.fromTask)

			if tt.wantErr {
				if err == nil {
					t.Error("Expected error but got nil")
				}
			} else {
				if err != nil {
					t.Errorf("Unexpected error: %v", err)
				}
				if idx != tt.wantIdx {
					t.Errorf("Index = %d, want %d", idx, tt.wantIdx)
				}
			}
		})
	}
}

// TestVerifyTaskCompletion tests the verifyTaskCompletion method
func TestVerifyTaskCompletion(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		taskID       string
		taskStatus   string
		wantErr      bool
		wantErrMatch string
	}{
		"completed task": {
			taskID:     "T001",
			taskStatus: "Completed",
			wantErr:    false,
		},
		"completed lowercase": {
			taskID:     "T001",
			taskStatus: "completed",
			wantErr:    false,
		},
		"pending task": {
			taskID:       "T001",
			taskStatus:   "Pending",
			wantErr:      true,
			wantErrMatch: "did not complete",
		},
		"in progress task": {
			taskID:       "T001",
			taskStatus:   "InProgress",
			wantErr:      true,
			wantErrMatch: "did not complete",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			specDir := filepath.Join(specsDir, "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			tasksContent := fmt.Sprintf(`tasks:
  branch: "001-test"
  created: "2025-01-01"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "%s"
        title: "Test Task"
        status: "%s"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`, tt.taskID, tt.taskStatus)

			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
				t.Fatalf("Failed to write tasks.yaml: %v", err)
			}

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)
			err := orchestrator.verifyTaskCompletion(tasksPath, tt.taskID)

			if tt.wantErr {
				if err == nil {
					t.Error("Expected error but got nil")
				} else if tt.wantErrMatch != "" && !strings.Contains(err.Error(), tt.wantErrMatch) {
					t.Errorf("Error %q should contain %q", err.Error(), tt.wantErrMatch)
				}
			} else {
				if err != nil {
					t.Errorf("Unexpected error: %v", err)
				}
			}
		})
	}
}

// TestPrintTasksSummary tests the printTasksSummary function
func TestPrintTasksSummary(t *testing.T) {
	t.Parallel()

	tmpDir := t.TempDir()
	specsDir := filepath.Join(tmpDir, "specs")
	specDir := filepath.Join(specsDir, "001-test")
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("Failed to create spec directory: %v", err)
	}

	tasksContent := `tasks:
  branch: "001-test"
  created: "2025-01-01"
summary:
  total_tasks: 2
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "First"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
      - id: "T002"
        title: "Second"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
	tasksPath := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
		t.Fatalf("Failed to write tasks.yaml: %v", err)
	}

	// Create spec.yaml for markSpecCompletedAndPrint
	specContent := `feature:
  branch: "001-test"
  status: "Draft"
  created: "2025-01-01"
user_stories: []
requirements:
  functional: []
`
	if err := os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644); err != nil {
		t.Fatalf("Failed to write spec.yaml: %v", err)
	}

	// Should not panic
	printTasksSummary(tasksPath, specDir)

	// Test with invalid path - should not panic
	printTasksSummary(filepath.Join(tmpDir, "nonexistent.yaml"), specDir)
}

// TestRunCompleteWorkflow tests the RunCompleteWorkflow method (T005)
func TestRunCompleteWorkflow(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		setupMock          func(*MockClaudeExecutor)
		setupFiles         func(string)
		wantErr            bool
		wantErrContains    string
	}{
		"successful workflow execution": {
			featureDescription: "Add user authentication",
			setupMock: func(m *MockClaudeExecutor) {
				// Mock succeeds for all stages
			},
			setupFiles: func(specsDir string) {
				// Create all required files after each stage simulating Claude's output
				specDir := filepath.Join(specsDir, "001-add-user-authentication")
				os.MkdirAll(specDir, 0755)

				// spec.yaml
				specContent := `feature:
  branch: "001-add-user-authentication"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)

				// plan.yaml
				planContent := `plan:
  branch: "001-add-user-authentication"
  created: "2025-01-01"
summary: "Implementation plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)

				// tasks.yaml
				tasksContent := `tasks:
  branch: "001-add-user-authentication"
  created: "2025-01-01"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Setup"
    purpose: "Initial setup"
    tasks:
      - id: "T001"
        title: "Create auth module"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Module exists"
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"empty feature description": {
			featureDescription: "",
			setupMock:          func(m *MockClaudeExecutor) {},
			setupFiles:         func(specsDir string) {},
			wantErr:            false, // Empty string is valid, creates spec with empty input
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			if err := os.MkdirAll(specsDir, 0755); err != nil {
				t.Fatalf("Failed to create specs directory: %v", err)
			}

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)
			mock := NewMockClaudeExecutor()
			tt.setupMock(mock)
			tt.setupFiles(specsDir)

			// Test the command format
			if tt.featureDescription != "" {
				command := fmt.Sprintf("/autospec.specify \"%s\"", tt.featureDescription)
				if !strings.Contains(command, tt.featureDescription) {
					t.Errorf("command should contain feature description")
				}
			}

			// Verify orchestrator is properly configured
			if orchestrator.SpecsDir != specsDir {
				t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, specsDir)
			}
		})
	}
}

// TestRunFullWorkflow tests the RunFullWorkflow method (T005)
func TestRunFullWorkflow(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		resume             bool
		setupFiles         func(string)
		wantErr            bool
	}{
		"full workflow with resume false": {
			featureDescription: "Add new feature",
			resume:             false,
			setupFiles: func(specsDir string) {
				specDir := filepath.Join(specsDir, "001-add-new-feature")
				os.MkdirAll(specDir, 0755)
			},
			wantErr: false,
		},
		"full workflow with resume true": {
			featureDescription: "Continue feature",
			resume:             true,
			setupFiles: func(specsDir string) {
				specDir := filepath.Join(specsDir, "001-continue-feature")
				os.MkdirAll(specDir, 0755)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)
			tt.setupFiles(specsDir)

			// Verify command format includes resume flag
			command := orchestrator.buildImplementCommand(tt.resume)
			if tt.resume && !strings.Contains(command, "--resume") {
				t.Errorf("command should contain --resume flag")
			}
			if !tt.resume && strings.Contains(command, "--resume") {
				t.Errorf("command should not contain --resume flag")
			}
		})
	}
}

// TestExecuteSpecifyWithMock tests the ExecuteSpecify method with mock executor (T006)
func TestExecuteSpecifyWithMock(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		mockError          error
		setupFiles         func(string)
		wantErr            bool
		wantErrContains    string
	}{
		"successful spec generation": {
			featureDescription: "Add user authentication",
			mockError:          nil,
			setupFiles: func(specsDir string) {
				specDir := filepath.Join(specsDir, "001-test-feature")
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
			},
			wantErr: false,
		},
		"execution error from claude": {
			featureDescription: "Test feature",
			mockError:          ErrMockExecute,
			setupFiles:         func(specsDir string) {},
			wantErr:            true,
			wantErrContains:    "specify failed",
		},
		"invalid input - special characters": {
			featureDescription: "Test with 'quotes' and \"double quotes\"",
			mockError:          nil,
			setupFiles: func(specsDir string) {
				specDir := filepath.Join(specsDir, "001-test-feature")
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
_meta:
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)
			tt.setupFiles(specsDir)

			// Verify the command format
			command := fmt.Sprintf("/autospec.specify \"%s\"", tt.featureDescription)
			if !strings.Contains(command, "/autospec.specify") {
				t.Errorf("command should contain /autospec.specify")
			}

			// Verify orchestrator is configured correctly
			if orchestrator.SpecsDir != specsDir {
				t.Errorf("SpecsDir = %v, want %v", orchestrator.SpecsDir, specsDir)
			}
		})
	}
}

// TestExecutePlanWithMock tests the ExecutePlan method with mock executor (T007)
func TestExecutePlanWithMock(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful plan generation": {
			specName: "001-test-feature",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
			},
			wantErr: false,
		},
		"missing spec validation": {
			specName: "001-no-spec",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				// No spec.yaml created
			},
			wantErr:         true,
			wantErrContains: "spec.yaml",
		},
		"plan with custom prompt": {
			specName: "002-another-feature",
			prompt:   "Focus on security aspects",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "002-another-feature"
_meta:
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "002-another-feature"
_meta:
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			tt.setupFiles(specsDir, tt.specName)

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify command format
			command := "/autospec.plan"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.plan \"%s\"", tt.prompt)
			}

			if tt.prompt != "" && !strings.Contains(command, tt.prompt) {
				t.Errorf("command should contain prompt, got: %s", command)
			}

			// Verify preflight check for missing spec
			specDir := filepath.Join(specsDir, tt.specName)
			result := ValidateStagePrerequisites(StagePlan, specDir)

			if tt.wantErr && tt.wantErrContains == "spec.yaml" {
				if result.Valid {
					t.Error("Expected validation to fail for missing spec.yaml")
				}
			}

			_ = orchestrator // Ensure orchestrator is used
		})
	}
}

// TestExecuteTasksWithMock tests the ExecuteTasks method with mock executor (T008)
func TestExecuteTasksWithMock(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful task breakdown generation": {
			specName: "001-test-feature",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-test-feature"
_meta:
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				planContent := `plan:
  branch: "001-test-feature"
_meta:
  artifact_type: "plan"
`
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(planContent), 0644)
				tasksContent := `tasks:
  branch: "001-test-feature"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test Task"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"missing plan validation": {
			specName: "001-no-plan",
			prompt:   "",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				specContent := `feature:
  branch: "001-no-plan"
_meta:
  artifact_type: "spec"
`
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(specContent), 0644)
				// No plan.yaml created
			},
			wantErr:         true,
			wantErrContains: "plan.yaml",
		},
		"tasks with custom prompt": {
			specName: "002-another-feature",
			prompt:   "Break into small granular tasks",
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte("tasks:\n  branch: test\nsummary:\n  total_tasks: 0\nphases: []\n_meta:\n  artifact_type: tasks\n"), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			tt.setupFiles(specsDir, tt.specName)

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify command format
			command := "/autospec.tasks"
			if tt.prompt != "" {
				command = fmt.Sprintf("/autospec.tasks \"%s\"", tt.prompt)
			}

			if tt.prompt != "" && !strings.Contains(command, tt.prompt) {
				t.Errorf("command should contain prompt, got: %s", command)
			}

			// Verify preflight check for missing plan
			specDir := filepath.Join(specsDir, tt.specName)
			result := ValidateStagePrerequisites(StageTasks, specDir)

			if tt.wantErr && tt.wantErrContains == "plan.yaml" {
				if result.Valid {
					t.Error("Expected validation to fail for missing plan.yaml")
				}
			}

			_ = orchestrator // Ensure orchestrator is used
		})
	}
}

// TestExecuteImplementWithMock tests the ExecuteImplement method with mock executor (T009)
func TestExecuteImplementWithMock(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specName        string
		prompt          string
		resume          bool
		phaseOpts       PhaseExecutionOptions
		setupFiles      func(string, string)
		wantErr         bool
		wantErrContains string
	}{
		"successful implementation execution": {
			specName: "001-test-feature",
			prompt:   "",
			resume:   false,
			phaseOpts: PhaseExecutionOptions{
				TaskMode: false,
			},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				tasksContent := `tasks:
  branch: "001-test-feature"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Test Task"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"missing tasks validation": {
			specName:  "001-no-tasks",
			prompt:    "",
			resume:    false,
			phaseOpts: PhaseExecutionOptions{},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				// No tasks.yaml created
			},
			wantErr:         true,
			wantErrContains: "tasks.yaml",
		},
		"phase-based execution": {
			specName: "002-phase-based",
			prompt:   "",
			resume:   false,
			phaseOpts: PhaseExecutionOptions{
				RunAllPhases: true,
			},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				tasksContent := `tasks:
  branch: "002-phase-based"
summary:
  total_tasks: 2
  total_phases: 2
phases:
  - number: 1
    title: "Phase 1"
    purpose: "First phase"
    tasks:
      - id: "T001"
        title: "Task 1"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
  - number: 2
    title: "Phase 2"
    purpose: "Second phase"
    tasks:
      - id: "T002"
        title: "Task 2"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: ["T001"]
        acceptance_criteria:
          - "Test"
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"task-based execution": {
			specName: "003-task-based",
			prompt:   "",
			resume:   false,
			phaseOpts: PhaseExecutionOptions{
				TaskMode: true,
				FromTask: "T001",
			},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				tasksContent := `tasks:
  branch: "003-task-based"
summary:
  total_tasks: 2
  total_phases: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Task 1"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test"
      - id: "T002"
        title: "Task 2"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: ["T001"]
        acceptance_criteria:
          - "Test"
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
		"single phase execution": {
			specName: "004-single-phase",
			prompt:   "",
			resume:   false,
			phaseOpts: PhaseExecutionOptions{
				SinglePhase: 2,
			},
			setupFiles: func(specsDir, specName string) {
				specDir := filepath.Join(specsDir, specName)
				os.MkdirAll(specDir, 0755)
				os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte("feature:\n  branch: test\n_meta:\n  artifact_type: spec\n"), 0644)
				os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte("plan:\n  branch: test\n_meta:\n  artifact_type: plan\n"), 0644)
				tasksContent := `tasks:
  branch: "004-single-phase"
summary:
  total_tasks: 2
  total_phases: 2
phases:
  - number: 1
    title: "Phase 1"
    purpose: "First"
    tasks:
      - id: "T001"
        title: "Task 1"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
  - number: 2
    title: "Phase 2"
    purpose: "Second"
    tasks:
      - id: "T002"
        title: "Task 2"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: ["T001"]
_meta:
  artifact_type: "tasks"
`
				os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(tasksContent), 0644)
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specsDir := filepath.Join(tmpDir, "specs")
			os.MkdirAll(specsDir, 0755)

			tt.setupFiles(specsDir, tt.specName)

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   specsDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)

			// Verify mode selection
			mode := tt.phaseOpts.Mode()
			switch {
			case tt.phaseOpts.TaskMode:
				if mode != ModeAllTasks {
					t.Errorf("Mode() = %v, want ModeAllTasks", mode)
				}
			case tt.phaseOpts.RunAllPhases:
				if mode != ModeAllPhases {
					t.Errorf("Mode() = %v, want ModeAllPhases", mode)
				}
			case tt.phaseOpts.SinglePhase > 0:
				if mode != ModeSinglePhase {
					t.Errorf("Mode() = %v, want ModeSinglePhase", mode)
				}
			}

			// Verify preflight check for missing tasks
			specDir := filepath.Join(specsDir, tt.specName)
			result := ValidateStagePrerequisites(StageImplement, specDir)

			if tt.wantErr && tt.wantErrContains == "tasks.yaml" {
				if result.Valid {
					t.Error("Expected validation to fail for missing tasks.yaml")
				}
			}

			_ = orchestrator // Ensure orchestrator is used
		})
	}
}

// TestExecuteSpecifyAndPlanTasks tests the executeSpecifyPlanTasks method
func TestExecuteSpecifyAndPlanTasks(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		featureDescription string
		totalStages        int
		wantCommand        string
	}{
		"standard workflow": {
			featureDescription: "Add login feature",
			totalStages:        4,
			wantCommand:        "/autospec.specify",
		},
		"prep only": {
			featureDescription: "Plan only feature",
			totalStages:        3,
			wantCommand:        "/autospec.specify",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Test command format
			command := fmt.Sprintf("%s \"%s\"", tt.wantCommand, tt.featureDescription)
			if !strings.Contains(command, tt.wantCommand) {
				t.Errorf("command should contain %s", tt.wantCommand)
			}
			if !strings.Contains(command, tt.featureDescription) {
				t.Errorf("command should contain feature description")
			}
		})
	}
}

// TestValidateTasksCompleteFunc tests the validateTasksCompleteFunc method
func TestValidateTasksCompleteFunc(t *testing.T) {
	tests := map[string]struct {
		tasksContent string
		wantErr      bool
	}{
		"all tasks completed": {
			tasksContent: `tasks:
  branch: "test"
summary:
  total_tasks: 2
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Task 1"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
      - id: "T002"
        title: "Task 2"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
_meta:
  artifact_type: "tasks"
`,
			wantErr: false,
		},
		"some tasks pending": {
			tasksContent: `tasks:
  branch: "test"
summary:
  total_tasks: 2
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Task 1"
        status: "Completed"
        type: "implementation"
        parallel: false
        dependencies: []
      - id: "T002"
        title: "Task 2"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
_meta:
  artifact_type: "tasks"
`,
			wantErr: true,
		},
		"blocked tasks": {
			tasksContent: `tasks:
  branch: "test"
summary:
  total_tasks: 1
phases:
  - number: 1
    title: "Test"
    purpose: "Testing"
    tasks:
      - id: "T001"
        title: "Task 1"
        status: "Blocked"
        type: "implementation"
        parallel: false
        dependencies: []
_meta:
  artifact_type: "tasks"
`,
			wantErr: true, // Blocked tasks count as incomplete
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "spec")
			os.MkdirAll(specDir, 0755)

			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if err := os.WriteFile(tasksPath, []byte(tt.tasksContent), 0644); err != nil {
				t.Fatalf("Failed to write tasks.yaml: %v", err)
			}

			cfg := &config.Configuration{
				ClaudeCmd:  "echo",
				SpecsDir:   tmpDir,
				MaxRetries: 3,
				StateDir:   filepath.Join(tmpDir, "state"),
			}

			orchestrator := NewWorkflowOrchestrator(cfg)
			err := orchestrator.validateTasksCompleteFunc(specDir)

			if tt.wantErr && err == nil {
				t.Error("Expected error but got nil")
			}
			if !tt.wantErr && err != nil {
				t.Errorf("Unexpected error: %v", err)
			}
		})
	}
}

// =============================================================================
// Test Helper Functions for Execute* Method Tests
// =============================================================================
// These helpers are defined here to avoid import cycle with testutil package.

// findMockClaudePath locates the mock-claude.sh script relative to the repo root.
func findMockClaudePath(t *testing.T) string {
	t.Helper()

	// Get the path to the current source file
	_, currentFile, _, ok := runtime.Caller(0)
	if !ok {
		t.Fatal("failed to determine current file location")
	}

	// Navigate from internal/workflow/ to repo root
	repoRoot := filepath.Join(filepath.Dir(currentFile), "..", "..")

	// Try the primary location first
	mockPath := filepath.Join(repoRoot, "mocks", "scripts", "mock-claude.sh")
	if _, err := os.Stat(mockPath); err == nil {
		return mockPath
	}

	// Fallback location
	mockPath = filepath.Join(repoRoot, "tests", "mocks", "mock-claude.sh")
	if _, err := os.Stat(mockPath); err == nil {
		return mockPath
	}

	t.Fatalf("mock-claude.sh not found at expected locations")
	return ""
}

// newTestOrchestratorWithSpecName creates a WorkflowOrchestrator configured for testing.
// It uses mock-claude.sh as the Claude command to avoid real API calls.
// Sets environment variables so mock-claude.sh creates the appropriate artifacts.
func newTestOrchestratorWithSpecName(t *testing.T, specsDir, specName string) *WorkflowOrchestrator {
	t.Helper()

	// Find the mock-claude.sh script path
	mockClaudePath := findMockClaudePath(t)

	// Create state directory within the test temp area
	stateDir := filepath.Join(specsDir, ".autospec", "state")
	if err := os.MkdirAll(stateDir, 0755); err != nil {
		t.Fatalf("failed to create state directory: %v", err)
	}

	cfg := &config.Configuration{
		ClaudeCmd:     mockClaudePath,
		ClaudeArgs:    []string{},
		SpecsDir:      specsDir,
		StateDir:      stateDir,
		MaxRetries:    1, // Minimal retries for faster tests
		SkipPreflight: true,
		Timeout:       30, // 30 second timeout for tests
	}

	// Set environment variables for mock-claude.sh to generate artifacts
	t.Setenv("MOCK_ARTIFACT_DIR", specsDir)
	t.Setenv("MOCK_SPEC_NAME", specName)

	return NewWorkflowOrchestrator(cfg)
}

// setupSpecDirectory creates a test spec directory with the given name.
func setupSpecDirectory(t *testing.T, specsDir, specName string) string {
	t.Helper()

	specDir := filepath.Join(specsDir, specName)
	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("failed to create spec directory: %v", err)
	}
	return specDir
}

// writeTestSpec writes a valid spec.yaml to the given spec directory.
func writeTestSpec(t *testing.T, specDir string) {
	t.Helper()
	content := `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
  input: "test"
user_stories:
  - id: "US-001"
    title: "Test"
    priority: "P1"
    as_a: "dev"
    i_want: "test"
    so_that: "it works"
    why_this_priority: "test"
    independent_test: "test"
    acceptance_scenarios:
      - given: "a"
        when: "b"
        then: "c"
requirements:
  functional:
    - id: "FR-001"
      description: "test"
      testable: true
      acceptance_criteria: "test"
  non_functional:
    - id: "NFR-001"
      category: "code_quality"
      description: "test"
      measurable_target: "test"
success_criteria:
  measurable_outcomes:
    - id: "SC-001"
      description: "test"
      metric: "test"
      target: "test"
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`
	path := filepath.Join(specDir, "spec.yaml")
	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		t.Fatalf("failed to write spec.yaml: %v", err)
	}
}

// writeTestPlan writes a valid plan.yaml to the given spec directory.
func writeTestPlan(t *testing.T, specDir string) {
	t.Helper()
	content := `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
summary: "Test plan"
technical_context:
  language: "Go"
  framework: "None"
  primary_dependencies: []
  storage: "None"
  testing:
    framework: "Go testing"
    approach: "Unit tests"
  target_platform: "Linux"
  project_type: "cli"
  performance_goals: "Fast"
  constraints: []
  scale_scope: "Small"
constitution_check:
  constitution_path: ".autospec/memory/constitution.yaml"
  gates: []
research_findings:
  decisions: []
data_model:
  entities: []
api_contracts:
  endpoints: []
project_structure:
  documentation: []
  source_code: []
  tests: []
implementation_phases:
  - phase: 1
    name: "Test"
    goal: "Test"
    deliverables: []
risks: []
open_questions: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "plan"
`
	path := filepath.Join(specDir, "plan.yaml")
	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		t.Fatalf("failed to write plan.yaml: %v", err)
	}
}

// writeTestTasks writes a valid tasks.yaml to the given spec directory.
// Tasks have status "Pending" which is the default pre-implementation state.
func writeTestTasks(t *testing.T, specDir string) {
	t.Helper()
	content := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
  parallel_opportunities: 0
  estimated_complexity: "low"
phases:
  - number: 1
    title: "Test"
    purpose: "Test"
    tasks:
      - id: "T001"
        title: "Test task"
        status: "Pending"
        type: "implementation"
        parallel: false
        story_id: "US-001"
        file_path: "test.go"
        dependencies: []
        acceptance_criteria:
          - "Test passes"
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: [1]
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
	path := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		t.Fatalf("failed to write tasks.yaml: %v", err)
	}
}

// writeTestTasksCompleted writes tasks.yaml with all tasks marked Completed.
// Used for testing ExecuteImplement which validates task completion.
func writeTestTasksCompleted(t *testing.T, specDir string) {
	t.Helper()
	content := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
  parallel_opportunities: 0
  estimated_complexity: "low"
phases:
  - number: 1
    title: "Test"
    purpose: "Test"
    tasks:
      - id: "T001"
        title: "Test task"
        status: "Completed"
        type: "implementation"
        parallel: false
        story_id: "US-001"
        file_path: "test.go"
        dependencies: []
        acceptance_criteria:
          - "Test passes"
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: [1]
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
	path := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		t.Fatalf("failed to write tasks.yaml: %v", err)
	}
}

// writeAllTestArtifacts writes all three artifact files (spec, plan, tasks).
func writeAllTestArtifacts(t *testing.T, specDir string) {
	t.Helper()
	writeTestSpec(t, specDir)
	writeTestPlan(t, specDir)
	writeTestTasks(t, specDir)
}

// =============================================================================
// Execute* Method Tests with Mock Infrastructure (Phase 3 Tasks T005-T008)
// =============================================================================
// These tests use newTestOrchestratorWithSpecName which configures mock-claude.sh
// to generate valid artifact files, enabling actual Execute* method testing.

// TestExecuteSpecify_Success tests ExecuteSpecify creates spec.yaml via mock
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteSpecify_Success(t *testing.T) {
	tests := map[string]struct {
		featureDesc string
		specName    string
	}{
		"simple feature description": {
			featureDesc: "Add user authentication",
			specName:    "001-test-feature",
		},
		"multiline feature description": {
			featureDesc: "Add user authentication\nwith OAuth support",
			specName:    "001-test-feature",
		},
		"feature with special characters": {
			featureDesc: "Add 'user' authentication with \"quotes\"",
			specName:    "001-test-feature",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock - mock-claude.sh will create artifacts
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Call ExecuteSpecify - mock will generate spec.yaml
			specName, err := orchestrator.ExecuteSpecify(tt.featureDesc)

			// Verify success
			if err != nil {
				t.Fatalf("ExecuteSpecify() error = %v, want nil", err)
			}

			if specName != tt.specName {
				t.Errorf("ExecuteSpecify() specName = %q, want %q", specName, tt.specName)
			}

			// Verify spec.yaml was created
			specDir := filepath.Join(tmpDir, tt.specName)
			specPath := filepath.Join(specDir, "spec.yaml")
			if _, err := os.Stat(specPath); os.IsNotExist(err) {
				t.Errorf("spec.yaml was not created at %s", specPath)
			}
		})
	}
}

// TestExecutePlan_Success tests ExecutePlan creates plan.yaml via mock
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecutePlan_Success(t *testing.T) {
	tests := map[string]struct {
		specName string
		prompt   string
	}{
		"no prompt": {
			specName: "001-test-feature",
			prompt:   "",
		},
		"with prompt": {
			specName: "001-test-feature",
			prompt:   "Focus on security",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Setup prerequisite spec.yaml
			specDir := setupSpecDirectory(t, tmpDir, tt.specName)
			writeTestSpec(t, specDir)

			// Call ExecutePlan - mock will generate plan.yaml
			err := orchestrator.ExecutePlan(tt.specName, tt.prompt)

			// Verify success
			if err != nil {
				t.Fatalf("ExecutePlan() error = %v, want nil", err)
			}

			// Verify plan.yaml was created
			planPath := filepath.Join(specDir, "plan.yaml")
			if _, err := os.Stat(planPath); os.IsNotExist(err) {
				t.Errorf("plan.yaml was not created at %s", planPath)
			}
		})
	}
}

// TestExecuteTasks_Success tests ExecuteTasks creates tasks.yaml via mock
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteTasks_Success(t *testing.T) {
	tests := map[string]struct {
		specName string
		prompt   string
	}{
		"no prompt": {
			specName: "001-test-feature",
			prompt:   "",
		},
		"with prompt": {
			specName: "001-test-feature",
			prompt:   "Break into small steps",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Setup prerequisite spec.yaml and plan.yaml
			specDir := setupSpecDirectory(t, tmpDir, tt.specName)
			writeTestSpec(t, specDir)
			writeTestPlan(t, specDir)

			// Call ExecuteTasks - mock will generate tasks.yaml
			err := orchestrator.ExecuteTasks(tt.specName, tt.prompt)

			// Verify success
			if err != nil {
				t.Fatalf("ExecuteTasks() error = %v, want nil", err)
			}

			// Verify tasks.yaml was created
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if _, err := os.Stat(tasksPath); os.IsNotExist(err) {
				t.Errorf("tasks.yaml was not created at %s", tasksPath)
			}
		})
	}
}

// TestExecuteImplement_Success tests ExecuteImplement completes without error
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteImplement_Success(t *testing.T) {
	tests := map[string]struct {
		specName  string
		prompt    string
		resume    bool
		phaseOpts PhaseExecutionOptions
	}{
		"default execution": {
			specName:  "001-test-feature",
			prompt:    "",
			resume:    false,
			phaseOpts: PhaseExecutionOptions{},
		},
		"with prompt": {
			specName:  "001-test-feature",
			prompt:    "Focus on error handling",
			resume:    false,
			phaseOpts: PhaseExecutionOptions{},
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Setup all prerequisite artifacts with completed tasks
			// ExecuteImplement validates that all tasks are completed
			specDir := setupSpecDirectory(t, tmpDir, tt.specName)
			writeTestSpec(t, specDir)
			writeTestPlan(t, specDir)
			writeTestTasksCompleted(t, specDir)

			// Call ExecuteImplement
			err := orchestrator.ExecuteImplement(tt.specName, tt.prompt, tt.resume, tt.phaseOpts)

			// Verify success (implementation completes without error)
			if err != nil {
				t.Fatalf("ExecuteImplement() error = %v, want nil", err)
			}
		})
	}
}

// =============================================================================
// Run* Workflow Tests (Phase 4 Tasks T009-T010)
// =============================================================================

// TestRunCompleteWorkflow_Success tests RunCompleteWorkflow executes specify  plan  tasks
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestRunCompleteWorkflow_Success(t *testing.T) {
	tests := map[string]struct {
		featureDesc string
		specName    string
	}{
		"simple feature description": {
			featureDesc: "Add user authentication",
			specName:    "001-test-feature",
		},
		"detailed feature description": {
			featureDesc: "Add user authentication with OAuth support and password reset",
			specName:    "001-test-feature",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock - mock-claude.sh will generate artifacts
			// for each stage (specify  plan  tasks)
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)
			orchestrator.SkipPreflight = true

			// Call RunCompleteWorkflow - mock generates spec, plan, tasks in sequence
			err := orchestrator.RunCompleteWorkflow(tt.featureDesc)

			// Verify success
			if err != nil {
				t.Fatalf("RunCompleteWorkflow() error = %v, want nil", err)
			}

			// Verify all three artifacts were created in sequence
			specDir := filepath.Join(tmpDir, tt.specName)

			// Verify spec.yaml was created
			specPath := filepath.Join(specDir, "spec.yaml")
			if _, err := os.Stat(specPath); os.IsNotExist(err) {
				t.Errorf("spec.yaml was not created at %s", specPath)
			}

			// Verify plan.yaml was created
			planPath := filepath.Join(specDir, "plan.yaml")
			if _, err := os.Stat(planPath); os.IsNotExist(err) {
				t.Errorf("plan.yaml was not created at %s", planPath)
			}

			// Verify tasks.yaml was created
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if _, err := os.Stat(tasksPath); os.IsNotExist(err) {
				t.Errorf("tasks.yaml was not created at %s", tasksPath)
			}
		})
	}
}

// TestRunFullWorkflow_Success tests RunFullWorkflow executes specify  plan  tasks  implement
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestRunFullWorkflow_Success(t *testing.T) {
	tests := map[string]struct {
		featureDesc string
		specName    string
		resume      bool
	}{
		"full workflow without resume": {
			featureDesc: "Add user authentication",
			specName:    "001-test-feature",
			resume:      false,
		},
		"full workflow with resume": {
			featureDesc: "Add user authentication",
			specName:    "001-test-feature",
			resume:      true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock - mock-claude.sh will generate artifacts
			// for all four stages (specify  plan  tasks  implement)
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)
			orchestrator.SkipPreflight = true

			// Call RunFullWorkflow - mock generates all artifacts including implementation
			err := orchestrator.RunFullWorkflow(tt.featureDesc, tt.resume)

			// Verify success
			if err != nil {
				t.Fatalf("RunFullWorkflow() error = %v, want nil", err)
			}

			// Verify all artifacts were created
			specDir := filepath.Join(tmpDir, tt.specName)

			// Verify spec.yaml was created
			specPath := filepath.Join(specDir, "spec.yaml")
			if _, err := os.Stat(specPath); os.IsNotExist(err) {
				t.Errorf("spec.yaml was not created at %s", specPath)
			}

			// Verify plan.yaml was created
			planPath := filepath.Join(specDir, "plan.yaml")
			if _, err := os.Stat(planPath); os.IsNotExist(err) {
				t.Errorf("plan.yaml was not created at %s", planPath)
			}

			// Verify tasks.yaml was created
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if _, err := os.Stat(tasksPath); os.IsNotExist(err) {
				t.Errorf("tasks.yaml was not created at %s", tasksPath)
			}
		})
	}
}

// =============================================================================
// Error Path Tests (Phase 5 Tasks T011-T013)
// =============================================================================

// TestExecuteSpecify_ValidationFailure tests ExecuteSpecify when mock returns non-zero exit code
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteSpecify_ValidationFailure(t *testing.T) {
	tests := map[string]struct {
		featureDesc  string
		specName     string
		exitCode     string
		wantErrMatch string
	}{
		"mock returns error exit code": {
			featureDesc:  "Add user authentication",
			specName:     "001-test-feature",
			exitCode:     "1",
			wantErrMatch: "failed",
		},
		"mock returns timeout exit code": {
			featureDesc:  "Add user authentication",
			specName:     "001-test-feature",
			exitCode:     "124", // Common timeout exit code
			wantErrMatch: "failed",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, tt.specName)

			// Configure mock to fail with specified exit code
			t.Setenv("MOCK_EXIT_CODE", tt.exitCode)

			// Call ExecuteSpecify - should fail due to mock exit code
			_, err := orchestrator.ExecuteSpecify(tt.featureDesc)

			// Verify error is returned
			if err == nil {
				t.Fatalf("ExecuteSpecify() error = nil, want error containing %q", tt.wantErrMatch)
			}

			// Verify error contains expected context
			if !strings.Contains(err.Error(), tt.wantErrMatch) {
				t.Errorf("ExecuteSpecify() error = %q, want error containing %q", err.Error(), tt.wantErrMatch)
			}
		})
	}
}

// TestExecuteWithRetry_RetriesOnFailure tests that ExecuteSpecify retries when configured
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestExecuteWithRetry_RetriesOnFailure(t *testing.T) {
	// This test verifies retry behavior by checking call count via MOCK_CALL_LOG
	// The mock script logs each invocation, allowing verification of retry attempts

	t.Run("exhausted retries returns error", func(t *testing.T) {
		// Create isolated temp directory
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		// Create orchestrator with mock
		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Set up call log to verify retry attempts
		callLog := filepath.Join(tmpDir, "call.log")
		t.Setenv("MOCK_CALL_LOG", callLog)

		// Configure mock to always fail - will exhaust retries
		t.Setenv("MOCK_EXIT_CODE", "1")

		// Call ExecuteSpecify - should fail after retries exhausted
		_, err := orchestrator.ExecuteSpecify("Add user authentication")

		// Verify error is returned
		if err == nil {
			t.Fatal("ExecuteSpecify() error = nil, want error after retries exhausted")
		}

		// Verify call log exists (proves at least one call was made)
		if _, statErr := os.Stat(callLog); os.IsNotExist(statErr) {
			t.Error("MOCK_CALL_LOG was not created - mock was not invoked")
		}
	})

	t.Run("success after retry is possible", func(t *testing.T) {
		// Note: This test verifies the retry mechanism exists
		// Full retry-then-success testing requires more complex mock coordination
		// Here we verify the orchestrator is configured with retries > 0

		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Verify orchestrator's executor has retry capability
		if orchestrator.Executor.MaxRetries < 1 {
			t.Errorf("orchestrator.Executor.MaxRetries = %d, want >= 1", orchestrator.Executor.MaxRetries)
		}
	})
}

// TestAuxiliaryExecuteMethods tests ExecuteConstitution, ExecuteClarify, ExecuteChecklist, ExecuteAnalyze
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration
func TestAuxiliaryExecuteMethods(t *testing.T) {
	// Table-driven test for all auxiliary Execute* methods
	tests := map[string]struct {
		methodName string
		setup      func(t *testing.T, specDir string)
		execute    func(orchestrator *WorkflowOrchestrator, specName string) error
		wantErr    bool
	}{
		"ExecuteConstitution without prompt": {
			methodName: "ExecuteConstitution",
			setup:      nil, // Constitution doesn't require pre-existing artifacts
			execute: func(o *WorkflowOrchestrator, _ string) error {
				return o.ExecuteConstitution("")
			},
			wantErr: false,
		},
		"ExecuteConstitution with prompt": {
			methodName: "ExecuteConstitution",
			setup:      nil,
			execute: func(o *WorkflowOrchestrator, _ string) error {
				return o.ExecuteConstitution("Focus on testing principles")
			},
			wantErr: false,
		},
		"ExecuteClarify without prompt": {
			methodName: "ExecuteClarify",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteClarify(specName, "")
			},
			wantErr: false,
		},
		"ExecuteClarify with prompt": {
			methodName: "ExecuteClarify",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteClarify(specName, "Focus on security aspects")
			},
			wantErr: false,
		},
		"ExecuteChecklist without prompt": {
			methodName: "ExecuteChecklist",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteChecklist(specName, "")
			},
			wantErr: false,
		},
		"ExecuteChecklist with prompt": {
			methodName: "ExecuteChecklist",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteChecklist(specName, "Include accessibility checks")
			},
			wantErr: false,
		},
		"ExecuteAnalyze without prompt": {
			methodName: "ExecuteAnalyze",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteAnalyze(specName, "")
			},
			wantErr: false,
		},
		"ExecuteAnalyze with prompt": {
			methodName: "ExecuteAnalyze",
			setup: func(t *testing.T, specDir string) {
				writeTestSpec(t, specDir)
			},
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteAnalyze(specName, "Focus on API consistency")
			},
			wantErr: false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			// Create isolated temp directory
			tmpDir := t.TempDir()
			specName := "001-test-feature"

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

			// Setup prerequisite files if needed
			if tt.setup != nil {
				specDir := setupSpecDirectory(t, tmpDir, specName)
				tt.setup(t, specDir)
			}

			// Execute the method
			err := tt.execute(orchestrator, specName)

			// Verify error expectation
			if (err != nil) != tt.wantErr {
				t.Errorf("%s() error = %v, wantErr %v", tt.methodName, err, tt.wantErr)
			}
		})
	}
}

// TestAuxiliaryExecuteMethods_Errors tests error paths for auxiliary Execute* methods
func TestAuxiliaryExecuteMethods_Errors(t *testing.T) {
	tests := map[string]struct {
		methodName string
		exitCode   string
		execute    func(orchestrator *WorkflowOrchestrator, specName string) error
	}{
		"ExecuteConstitution failure": {
			methodName: "ExecuteConstitution",
			exitCode:   "1",
			execute: func(o *WorkflowOrchestrator, _ string) error {
				return o.ExecuteConstitution("")
			},
		},
		"ExecuteClarify failure": {
			methodName: "ExecuteClarify",
			exitCode:   "1",
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteClarify(specName, "")
			},
		},
		"ExecuteChecklist failure": {
			methodName: "ExecuteChecklist",
			exitCode:   "1",
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteChecklist(specName, "")
			},
		},
		"ExecuteAnalyze failure": {
			methodName: "ExecuteAnalyze",
			exitCode:   "1",
			execute: func(o *WorkflowOrchestrator, specName string) error {
				return o.ExecuteAnalyze(specName, "")
			},
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Create isolated temp directory
			tmpDir := t.TempDir()
			specName := "001-test-feature"

			// Create orchestrator with mock
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

			// Setup prerequisite spec.yaml for methods that need it
			specDir := setupSpecDirectory(t, tmpDir, specName)
			writeTestSpec(t, specDir)

			// Configure mock to fail
			t.Setenv("MOCK_EXIT_CODE", tt.exitCode)

			// Execute the method - should fail
			err := tt.execute(orchestrator, specName)

			// Verify error is returned
			if err == nil {
				t.Errorf("%s() error = nil, want error due to mock failure", tt.methodName)
			}
		})
	}
}

// TestExecuteImplementWithPhases tests phase-by-phase implementation execution
func TestExecuteImplementWithPhases(t *testing.T) {
	t.Run("no phases returns error", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with empty tasks (no phases)
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)

		// Write tasks.yaml with empty phases array
		emptyTasksContent := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 0
  total_phases: 0
  parallel_opportunities: 0
  estimated_complexity: "low"
phases: []
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: []
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
		tasksPath := filepath.Join(specDir, "tasks.yaml")
		if err := os.WriteFile(tasksPath, []byte(emptyTasksContent), 0644); err != nil {
			t.Fatalf("failed to write tasks.yaml: %v", err)
		}

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Call ExecuteImplementWithPhases
		err := orchestrator.ExecuteImplementWithPhases(specName, metadata, "", false)

		// Should return error about no phases
		if err == nil {
			t.Error("ExecuteImplementWithPhases() error = nil, want error for empty phases")
		}
		if err != nil && !strings.Contains(err.Error(), "no phases") {
			t.Errorf("ExecuteImplementWithPhases() error = %v, want error containing 'no phases'", err)
		}
	})

	t.Run("all phases complete returns early", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Call ExecuteImplementWithPhases
		err := orchestrator.ExecuteImplementWithPhases(specName, metadata, "", false)

		// Should succeed (all phases already complete)
		if err != nil {
			t.Errorf("ExecuteImplementWithPhases() error = %v, want nil", err)
		}
	})
}

// TestExecuteImplementWithTasks tests task-by-task implementation execution
func TestExecuteImplementWithTasks(t *testing.T) {
	t.Run("no tasks returns error", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with empty tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)

		// Write tasks.yaml with empty phases array
		emptyTasksContent := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 0
  total_phases: 0
  parallel_opportunities: 0
  estimated_complexity: "low"
phases: []
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: []
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
		tasksPath := filepath.Join(specDir, "tasks.yaml")
		if err := os.WriteFile(tasksPath, []byte(emptyTasksContent), 0644); err != nil {
			t.Fatalf("failed to write tasks.yaml: %v", err)
		}

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Call ExecuteImplementWithTasks (signature: specName, metadata, prompt, fromTask string)
		err := orchestrator.ExecuteImplementWithTasks(specName, metadata, "", "")

		// Should return error about no tasks
		if err == nil {
			t.Error("ExecuteImplementWithTasks() error = nil, want error for empty tasks")
		}
	})

	t.Run("all tasks complete returns early", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Call ExecuteImplementWithTasks (signature: specName, metadata, prompt, fromTask string)
		err := orchestrator.ExecuteImplementWithTasks(specName, metadata, "", "")

		// Should succeed (all tasks already complete)
		if err != nil {
			t.Errorf("ExecuteImplementWithTasks() error = %v, want nil", err)
		}
	})
}

// TestExecuteImplementSinglePhase tests single phase execution
func TestExecuteImplementSinglePhase(t *testing.T) {
	t.Run("phase not found returns error", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with one phase
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasks(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Request phase 99 which doesn't exist (signature: specName, metadata, prompt, phaseNumber)
		err := orchestrator.ExecuteImplementSinglePhase(specName, metadata, "", 99)

		// Should return error about phase out of range
		if err == nil {
			t.Error("ExecuteImplementSinglePhase() error = nil, want error for invalid phase")
		}
		if err != nil && !strings.Contains(err.Error(), "out of range") {
			t.Errorf("ExecuteImplementSinglePhase() error = %v, want error containing 'out of range'", err)
		}
	})

	t.Run("valid phase with completed tasks succeeds", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Request phase 1 (signature: specName, metadata, prompt, phaseNumber)
		err := orchestrator.ExecuteImplementSinglePhase(specName, metadata, "", 1)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplementSinglePhase() error = %v, want nil", err)
		}
	})
}

// TestExecuteImplementFromPhase tests starting implementation from a specific phase
func TestExecuteImplementFromPhase(t *testing.T) {
	t.Run("phase not found returns error", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasks(t, specDir)

		// Create minimal metadata
		metadata := &spec.Metadata{
			Number:    "001",
			Name:      "test-feature",
			Directory: specDir,
		}

		// Request starting from phase 99 which doesn't exist (signature: specName, metadata, prompt, startPhase)
		err := orchestrator.ExecuteImplementFromPhase(specName, metadata, "", 99)

		// Should return error about phase not found
		if err == nil {
			t.Error("ExecuteImplementFromPhase() error = nil, want error for invalid phase")
		}
	})
}

// TestHandleImplementError tests the implement error handler
func TestHandleImplementError(t *testing.T) {
	tests := map[string]struct {
		exhausted    bool
		wantContains string
	}{
		"exhausted retries": {
			exhausted:    true,
			wantContains: "exhausted",
		},
		"non-exhausted error": {
			exhausted:    false,
			wantContains: "implementation failed",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			tmpDir := t.TempDir()
			specName := "001-test-feature"

			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

			result := &StageResult{
				Exhausted: tt.exhausted,
			}
			originalErr := fmt.Errorf("mock error")

			err := orchestrator.handleImplementError(result, "test feature", originalErr)

			if err == nil {
				t.Fatal("handleImplementError() error = nil, want error")
			}
			if !strings.Contains(err.Error(), tt.wantContains) {
				t.Errorf("handleImplementError() error = %v, want containing %q", err, tt.wantContains)
			}
		})
	}
}

// TestRunFullWorkflow_Error tests error paths in RunFullWorkflow
func TestRunFullWorkflow_Error(t *testing.T) {
	t.Run("specify stage failure", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)
		orchestrator.SkipPreflight = true

		// Configure mock to fail
		t.Setenv("MOCK_EXIT_CODE", "1")

		err := orchestrator.RunFullWorkflow("Add test feature", false)

		if err == nil {
			t.Error("RunFullWorkflow() error = nil, want error from failing specify stage")
		}
	})
}

// TestRunCompleteWorkflow_Error tests error paths in RunCompleteWorkflow
func TestRunCompleteWorkflow_Error(t *testing.T) {
	t.Run("specify stage failure", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)
		orchestrator.SkipPreflight = true

		// Configure mock to fail
		t.Setenv("MOCK_EXIT_CODE", "1")

		err := orchestrator.RunCompleteWorkflow("Add test feature")

		if err == nil {
			t.Error("RunCompleteWorkflow() error = nil, want error from failing specify stage")
		}
	})
}

// TestGetPhaseInfo tests phase info retrieval
func TestGetPhaseInfo(t *testing.T) {
	t.Run("getUpdatedPhaseInfo returns nil for missing phase", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"
		specDir := filepath.Join(tmpDir, specName)

		// Setup basic directory structure
		if err := os.MkdirAll(specDir, 0755); err != nil {
			t.Fatalf("failed to create spec dir: %v", err)
		}

		// Write tasks with phase 1 only
		tasksContent := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
  parallel_opportunities: 0
  estimated_complexity: "low"
phases:
  - number: 1
    title: "Test"
    purpose: "Test"
    tasks:
      - id: "T001"
        title: "Test task"
        status: "Pending"
        type: "implementation"
        parallel: false
        story_id: "US-001"
        file_path: "test.go"
        dependencies: []
        acceptance_criteria:
          - "Test passes"
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: [1]
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
		tasksPath := filepath.Join(specDir, "tasks.yaml")
		if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
			t.Fatalf("failed to write tasks: %v", err)
		}

		// Request phase 99 which doesn't exist
		result := getUpdatedPhaseInfo(tasksPath, 99)
		if result != nil {
			t.Error("getUpdatedPhaseInfo() returned non-nil for missing phase, want nil")
		}
	})
}

// TestPrintPhaseCompletion tests phase completion printing
func TestPrintPhaseCompletion_NilInfo(t *testing.T) {
	// This should not panic when passed nil
	printPhaseCompletion(1, nil)
}

// TestPrintTasksSummary_NoTasks tests task summary printing with no tasks
func TestPrintTasksSummary_NoTasks(t *testing.T) {
	tmpDir := t.TempDir()
	specDir := filepath.Join(tmpDir, "001-test")

	if err := os.MkdirAll(specDir, 0755); err != nil {
		t.Fatalf("failed to create spec dir: %v", err)
	}

	// Write tasks with empty phases
	tasksContent := `tasks:
  branch: "001-test"
  created: "2025-01-01"
  spec_path: "specs/001-test/spec.yaml"
  plan_path: "specs/001-test/plan.yaml"
summary:
  total_tasks: 0
  total_phases: 0
  parallel_opportunities: 0
  estimated_complexity: "low"
phases: []
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: []
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
	tasksPath := filepath.Join(specDir, "tasks.yaml")
	if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
		t.Fatalf("failed to write tasks: %v", err)
	}

	// This should not panic
	printTasksSummary(tasksPath, specDir)
}

// TestPrintPhasesSummary tests phases summary printing
func TestPrintPhasesSummary(t *testing.T) {
	t.Run("print summary with valid tasks file", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"
		specDir := filepath.Join(tmpDir, specName)

		if err := os.MkdirAll(specDir, 0755); err != nil {
			t.Fatalf("failed to create spec dir: %v", err)
		}

		// Write valid tasks file
		tasksContent := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
  parallel_opportunities: 0
  estimated_complexity: "low"
phases:
  - number: 1
    title: "Test"
    purpose: "Test"
    tasks:
      - id: "T001"
        title: "Test task"
        status: "Completed"
        type: "implementation"
        parallel: false
        story_id: "US-001"
        file_path: "test.go"
        dependencies: []
        acceptance_criteria:
          - "Test passes"
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: [1]
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
		tasksPath := filepath.Join(specDir, "tasks.yaml")
		if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
			t.Fatalf("failed to write tasks: %v", err)
		}

		// Also write spec.yaml for markSpecCompletedAndPrint
		writeTestSpec(t, specDir)

		// This should not panic
		printPhasesSummary(tasksPath, specDir)
	})
}

// TestGetPhaseByNumberWithTasksPath tests phase retrieval by number using tasks path
func TestGetPhaseByNumberWithTasksPath(t *testing.T) {
	tests := map[string]struct {
		phaseNumber int
		wantErr     bool
		wantNil     bool
	}{
		"valid phase 1": {
			phaseNumber: 1,
			wantErr:     false,
			wantNil:     false,
		},
		"invalid phase 99": {
			phaseNumber: 99,
			wantErr:     true,
			wantNil:     true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			tmpDir := t.TempDir()
			specName := "001-test-feature"
			specDir := filepath.Join(tmpDir, specName)

			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("failed to create spec dir: %v", err)
			}

			// Write tasks with one phase
			tasksContent := `tasks:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
  plan_path: "specs/001-test-feature/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
  parallel_opportunities: 0
  estimated_complexity: "low"
phases:
  - number: 1
    title: "Test"
    purpose: "Test"
    tasks:
      - id: "T001"
        title: "Test task"
        status: "Pending"
        type: "implementation"
        parallel: false
        story_id: "US-001"
        file_path: "test.go"
        dependencies: []
        acceptance_criteria:
          - "Test passes"
dependencies:
  user_story_order: []
  phase_order: []
parallel_execution: []
implementation_strategy:
  mvp_scope:
    phases: [1]
    description: "MVP"
    validation: "Tests pass"
  incremental_delivery: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
				t.Fatalf("failed to write tasks: %v", err)
			}

			result, err := getPhaseByNumber(tasksPath, tt.phaseNumber)
			if (err != nil) != tt.wantErr {
				t.Errorf("getPhaseByNumber() error = %v, wantErr %v", err, tt.wantErr)
			}
			if (result == nil) != tt.wantNil {
				t.Errorf("getPhaseByNumber() returned nil=%v, want nil=%v", result == nil, tt.wantNil)
			}
		})
	}
}

// TestExecuteImplement_Modes tests different execution modes in ExecuteImplement
func TestExecuteImplement_Modes(t *testing.T) {
	t.Run("phases mode with completed tasks", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Call ExecuteImplement with phases mode (RunAllPhases)
		opts := PhaseExecutionOptions{
			RunAllPhases: true,
		}
		err := orchestrator.ExecuteImplement(specName, "", false, opts)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplement(phases mode) error = %v, want nil", err)
		}
	})

	t.Run("tasks mode with completed tasks", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Call ExecuteImplement with tasks mode (TaskMode)
		opts := PhaseExecutionOptions{
			TaskMode: true,
		}
		err := orchestrator.ExecuteImplement(specName, "", false, opts)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplement(tasks mode) error = %v, want nil", err)
		}
	})

	t.Run("single phase mode", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Call ExecuteImplement with single phase
		opts := PhaseExecutionOptions{
			SinglePhase: 1,
		}
		err := orchestrator.ExecuteImplement(specName, "", false, opts)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplement(single phase mode) error = %v, want nil", err)
		}
	})

	t.Run("from phase mode", func(t *testing.T) {
		tmpDir := t.TempDir()
		specName := "001-test-feature"

		orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

		// Create spec directory with completed tasks
		specDir := setupSpecDirectory(t, tmpDir, specName)
		writeTestSpec(t, specDir)
		writeTestPlan(t, specDir)
		writeTestTasksCompleted(t, specDir)

		// Call ExecuteImplement with from phase
		opts := PhaseExecutionOptions{
			FromPhase: 1,
		}
		err := orchestrator.ExecuteImplement(specName, "", false, opts)

		// Should succeed
		if err != nil {
			t.Errorf("ExecuteImplement(from phase mode) error = %v, want nil", err)
		}
	})
}

// =============================================================================
// Integration Tests for Private Execute* Methods (Phase 3 Tasks T004-T008)
// =============================================================================
// These tests actually CALL the private methods (executeSingleTaskSession,
// executeSinglePhaseSession, executeAndVerifyTask, executeTaskLoop) rather than
// just testing string formatting. They use newTestOrchestratorWithSpecName which
// configures mock-claude.sh to generate valid artifacts and update task status.

// TestExecuteSingleTaskSession_Integration tests executeSingleTaskSession by actually calling it.
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration.
func TestExecuteSingleTaskSession_Integration(t *testing.T) {
	tests := map[string]struct {
		taskID    string
		taskTitle string
		prompt    string
		wantErr   bool
	}{
		"task without prompt": {
			taskID:    "T001",
			taskTitle: "Test task",
			prompt:    "",
			wantErr:   false,
		},
		"task with prompt": {
			taskID:    "T001",
			taskTitle: "Test task",
			prompt:    "Focus on tests",
			wantErr:   false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			tmpDir := t.TempDir()
			specName := "001-test-feature"

			// Create orchestrator with mock-claude.sh
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

			// Setup spec directory with tasks.yaml containing pending task
			specDir := setupSpecDirectory(t, tmpDir, specName)
			writeTestSpec(t, specDir)
			writeTestPlan(t, specDir)
			writeTestTasks(t, specDir) // writes tasks with "Pending" status

			// Actually call executeSingleTaskSession - this is the key difference from existing tests
			err := orchestrator.executeSingleTaskSession(specName, tt.taskID, tt.taskTitle, tt.prompt)

			if tt.wantErr {
				if err == nil {
					t.Error("executeSingleTaskSession() expected error, got nil")
				}
			} else {
				if err != nil {
					t.Errorf("executeSingleTaskSession() error = %v, want nil", err)
				}
			}

			// Verify mock-claude.sh was invoked by checking task status was updated
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			allTasks, err := validation.GetAllTasks(tasksPath)
			if err != nil {
				t.Fatalf("failed to read tasks after execution: %v", err)
			}

			task, err := validation.GetTaskByID(allTasks, tt.taskID)
			if err != nil {
				t.Fatalf("failed to find task %s: %v", tt.taskID, err)
			}

			// mock-claude.sh marks tasks as Completed when /autospec.implement is called
			if task.Status != "Completed" && task.Status != "completed" {
				t.Errorf("task status = %q, want Completed (mock-claude.sh should have updated it)", task.Status)
			}
		})
	}
}

// TestExecuteSinglePhaseSession_Integration tests executeSinglePhaseSession by actually calling it.
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration.
func TestExecuteSinglePhaseSession_Integration(t *testing.T) {
	tests := map[string]struct {
		phaseNumber int
		prompt      string
		taskStatus  string // Initial task status
		wantErr     bool
		wantSkip    bool // If true, phase should be skipped (no Claude invocation)
	}{
		"phase with pending task": {
			phaseNumber: 1,
			prompt:      "",
			taskStatus:  "Pending",
			wantErr:     false,
			wantSkip:    false,
		},
		"phase with prompt": {
			phaseNumber: 1,
			prompt:      "Focus on tests",
			taskStatus:  "Pending",
			wantErr:     false,
			wantSkip:    false,
		},
		"phase with all completed tasks - should skip": {
			phaseNumber: 1,
			prompt:      "",
			taskStatus:  "Completed",
			wantErr:     false,
			wantSkip:    true,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			tmpDir := t.TempDir()
			specName := "001-test-feature"

			// Create orchestrator with mock-claude.sh
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

			// Setup spec directory with tasks.yaml
			specDir := setupSpecDirectory(t, tmpDir, specName)
			writeTestSpec(t, specDir)
			writeTestPlan(t, specDir)

			// Write tasks with specified status
			if tt.taskStatus == "Completed" {
				writeTestTasksCompleted(t, specDir)
			} else {
				writeTestTasks(t, specDir) // Pending status
			}

			// Actually call executeSinglePhaseSession - this is the key difference
			err := orchestrator.executeSinglePhaseSession(specName, tt.phaseNumber, tt.prompt)

			if tt.wantErr {
				if err == nil {
					t.Error("executeSinglePhaseSession() expected error, got nil")
				}
			} else {
				if err != nil {
					t.Errorf("executeSinglePhaseSession() error = %v, want nil", err)
				}
			}

			// Verify task status update (only if not skipped)
			if !tt.wantSkip && !tt.wantErr {
				tasksPath := filepath.Join(specDir, "tasks.yaml")
				allTasks, err := validation.GetAllTasks(tasksPath)
				if err != nil {
					t.Fatalf("failed to read tasks: %v", err)
				}

				task, err := validation.GetTaskByID(allTasks, "T001")
				if err != nil {
					t.Fatalf("failed to find task: %v", err)
				}

				// mock-claude.sh marks tasks as Completed
				if task.Status != "Completed" && task.Status != "completed" {
					t.Errorf("task status = %q, want Completed", task.Status)
				}
			}
		})
	}
}

// TestExecuteAndVerifyTask_Integration tests executeAndVerifyTask by actually calling it.
// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration.
func TestExecuteAndVerifyTask_Integration(t *testing.T) {
	tests := map[string]struct {
		taskStatus string
		prompt     string
		wantErr    bool
	}{
		"pending task executes successfully": {
			taskStatus: "Pending",
			prompt:     "",
			wantErr:    false,
		},
		"pending task with prompt": {
			taskStatus: "Pending",
			prompt:     "Focus on error handling",
			wantErr:    false,
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			tmpDir := t.TempDir()
			specName := "001-test-feature"

			// Create orchestrator with mock-claude.sh
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

			// Setup spec directory with tasks.yaml
			specDir := setupSpecDirectory(t, tmpDir, specName)
			writeTestSpec(t, specDir)
			writeTestPlan(t, specDir)
			writeTestTasks(t, specDir) // Pending status

			// Load task from tasks.yaml using validation.GetAllTasks
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			allTasks, err := validation.GetAllTasks(tasksPath)
			if err != nil {
				t.Fatalf("failed to get all tasks: %v", err)
			}

			taskPtr, err := validation.GetTaskByID(allTasks, "T001")
			if err != nil {
				t.Fatalf("failed to get task T001: %v", err)
			}
			task := *taskPtr

			// Actually call executeAndVerifyTask - this is the key difference
			err = orchestrator.executeAndVerifyTask(specName, tasksPath, task, tt.prompt)

			if tt.wantErr {
				if err == nil {
					t.Error("executeAndVerifyTask() expected error, got nil")
				}
			} else {
				if err != nil {
					t.Errorf("executeAndVerifyTask() error = %v, want nil", err)
				}
			}

			// Verify task is now completed
			if !tt.wantErr {
				freshTasks, err := validation.GetAllTasks(tasksPath)
				if err != nil {
					t.Fatalf("failed to read tasks after execution: %v", err)
				}

				freshTask, err := validation.GetTaskByID(freshTasks, "T001")
				if err != nil {
					t.Fatalf("failed to find task after execution: %v", err)
				}

				if freshTask.Status != "Completed" && freshTask.Status != "completed" {
					t.Errorf("task status = %q after executeAndVerifyTask, want Completed", freshTask.Status)
				}
			}
		})
	}
}

// TestExecuteTaskLoop_Integration tests executeTaskLoop by actually calling it.
// TestExecuteTaskLoop_EdgeCases tests edge cases for executeTaskLoop.
// These tests don't require mock-claude.sh since they test scenarios where
// no Claude execution occurs (empty list, all completed, all blocked).
func TestExecuteTaskLoop_EdgeCases(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		tasks      []validation.TaskItem
		startIdx   int
		totalTasks int
		wantErr    bool
		desc       string
	}{
		"empty task list returns nil immediately": {
			tasks:      []validation.TaskItem{},
			startIdx:   0,
			totalTasks: 0,
			wantErr:    false,
			desc:       "empty task list should return nil without any execution",
		},
		"all tasks completed skips all": {
			tasks: []validation.TaskItem{
				{ID: "T001", Title: "Task 1", Status: "Completed"},
				{ID: "T002", Title: "Task 2", Status: "Completed"},
			},
			startIdx:   0,
			totalTasks: 2,
			wantErr:    false,
			desc:       "all completed tasks should be skipped",
		},
		"all tasks blocked skips all": {
			tasks: []validation.TaskItem{
				{ID: "T001", Title: "Task 1", Status: "Blocked"},
				{ID: "T002", Title: "Task 2", Status: "Blocked"},
			},
			startIdx:   0,
			totalTasks: 2,
			wantErr:    false,
			desc:       "all blocked tasks should be skipped",
		},
		"mixed completed and blocked skips all": {
			tasks: []validation.TaskItem{
				{ID: "T001", Title: "Task 1", Status: "Completed"},
				{ID: "T002", Title: "Task 2", Status: "Blocked"},
				{ID: "T003", Title: "Task 3", Status: "completed"}, // lowercase
				{ID: "T004", Title: "Task 4", Status: "blocked"},   // lowercase
			},
			startIdx:   0,
			totalTasks: 4,
			wantErr:    false,
			desc:       "mixed completed/blocked tasks should all be skipped",
		},
		"startIdx beyond list length returns nil": {
			tasks: []validation.TaskItem{
				{ID: "T001", Title: "Task 1", Status: "Pending"},
			},
			startIdx:   5, // Beyond list length
			totalTasks: 1,
			wantErr:    false,
			desc:       "startIdx beyond list should return nil without error",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Create a minimal orchestrator - we don't need full infrastructure
			// since these edge cases don't actually execute tasks
			tmpDir := t.TempDir()
			cfg := &config.Configuration{
				ClaudeCmd:     "echo", // Won't be called
				SpecsDir:      tmpDir,
				StateDir:      tmpDir,
				MaxRetries:    1,
				SkipPreflight: true,
			}
			orchestrator := NewWorkflowOrchestrator(cfg)

			// Create a tasks.yaml file (required for validation checks)
			specDir := filepath.Join(tmpDir, "test-spec")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("failed to create spec dir: %v", err)
			}
			tasksPath := filepath.Join(specDir, "tasks.yaml")

			// Write a minimal tasks file
			tasksContent := `phases:
  - number: 1
    title: Test
    tasks: []
_meta:
  artifact_type: tasks
  version: "1.0.0"
`
			if err := os.WriteFile(tasksPath, []byte(tasksContent), 0644); err != nil {
				t.Fatalf("failed to write tasks.yaml: %v", err)
			}

			err := orchestrator.executeTaskLoop("test-spec", tasksPath, tt.tasks, tt.startIdx, tt.totalTasks, "")

			if tt.wantErr {
				if err == nil {
					t.Error("executeTaskLoop() expected error, got nil")
				}
			} else {
				if err != nil {
					t.Errorf("executeTaskLoop() error = %v, want nil", err)
				}
			}
		})
	}
}

// Note: Cannot use t.Parallel() because tests use t.Setenv for mock-claude.sh configuration.
func TestExecuteTaskLoop_Integration(t *testing.T) {
	tests := map[string]struct {
		setupTasks func(t *testing.T, specDir string)
		startIdx   int
		wantErr    bool
		desc       string
	}{
		"executes pending tasks": {
			setupTasks: writeTestTasks, // Single pending task
			startIdx:   0,
			wantErr:    false,
			desc:       "single pending task should be executed",
		},
		"skips completed tasks": {
			setupTasks: writeTestTasksCompleted, // Single completed task
			startIdx:   0,
			wantErr:    false,
			desc:       "completed tasks should be skipped",
		},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			// Note: No t.Parallel() - these tests use t.Setenv which doesn't work with parallel

			tmpDir := t.TempDir()
			specName := "001-test-feature"

			// Create orchestrator with mock-claude.sh
			orchestrator := newTestOrchestratorWithSpecName(t, tmpDir, specName)

			// Setup spec directory with tasks
			specDir := setupSpecDirectory(t, tmpDir, specName)
			writeTestSpec(t, specDir)
			writeTestPlan(t, specDir)
			tt.setupTasks(t, specDir)

			// Load ordered tasks
			tasksPath := filepath.Join(specDir, "tasks.yaml")
			orderedTasks, err := validation.GetAllTasks(tasksPath)
			if err != nil {
				t.Fatalf("failed to get ordered tasks: %v", err)
			}

			totalTasks := len(orderedTasks)
			prompt := ""

			// Actually call executeTaskLoop - this is the key difference
			err = orchestrator.executeTaskLoop(specName, tasksPath, orderedTasks, tt.startIdx, totalTasks, prompt)

			if tt.wantErr {
				if err == nil {
					t.Error("executeTaskLoop() expected error, got nil")
				}
			} else {
				if err != nil {
					t.Errorf("executeTaskLoop() error = %v, want nil", err)
				}
			}

			// Verify all tasks are completed after loop
			if !tt.wantErr {
				freshTasks, err := validation.GetAllTasks(tasksPath)
				if err != nil {
					t.Fatalf("failed to read tasks after loop: %v", err)
				}

				for _, task := range freshTasks {
					if task.Status != "Completed" && task.Status != "completed" &&
						task.Status != "Blocked" && task.Status != "blocked" {
						t.Errorf("task %s status = %q, want Completed or Blocked", task.ID, task.Status)
					}
				}
			}
		})
	}
}

// TestSchemaValidationIntegration tests that schema validators are properly wired into workflow
// These tests verify FR-006: executePlan(), executeTasks(), and executeSpecify() pass schema validator functions to ExecuteStage()
func TestSchemaValidationIntegration(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		stage        string
		validateFunc func(string) error
		validDir     string
		invalidDir   string
		errContains  string
		setupValid   func(string) error
		setupInvalid func(string) error
	}{
		"spec schema validation": {
			stage:        "specify",
			validateFunc: ValidateSpecSchema,
			errContains:  "schema validation failed for spec.yaml",
			setupValid: func(specDir string) error {
				content := `feature:
  branch: "001-test"
  created: "2025-01-01"
  status: "Draft"
  input: "Test feature"
user_stories: []
requirements:
  functional: []
  non_functional: []
success_criteria:
  measurable_outcomes: []
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`
				return os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(content), 0644)
			},
			setupInvalid: func(specDir string) error {
				// Missing required 'feature' field
				content := `user_stories: []
requirements:
  functional: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`
				return os.WriteFile(filepath.Join(specDir, "spec.yaml"), []byte(content), 0644)
			},
		},
		"plan schema validation": {
			stage:        "plan",
			validateFunc: ValidatePlanSchema,
			errContains:  "schema validation failed for plan.yaml",
			setupValid: func(specDir string) error {
				content := `plan:
  branch: "001-test"
  created: "2025-01-01"
  spec_path: "specs/001-test/spec.yaml"
summary: "Test plan"
technical_context:
  language: "Go"
  framework: "Cobra"
  project_type: "cli"
implementation_phases: []
project_structure:
  source_code: []
  tests: []
  documentation: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "plan"
`
				return os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(content), 0644)
			},
			setupInvalid: func(specDir string) error {
				// Missing required 'plan' field
				content := `summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`
				return os.WriteFile(filepath.Join(specDir, "plan.yaml"), []byte(content), 0644)
			},
		},
		"tasks schema validation": {
			stage:        "tasks",
			validateFunc: ValidateTasksSchema,
			errContains:  "schema validation failed for tasks.yaml",
			setupValid: func(specDir string) error {
				content := `tasks:
  branch: "001-test"
  created: "2025-01-01"
  spec_path: "specs/001-test/spec.yaml"
  plan_path: "specs/001-test/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Setup"
    purpose: "Initial setup"
    tasks:
      - id: "T001"
        title: "Test task"
        status: "Pending"
        type: "implementation"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Test passes"
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`
				return os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(content), 0644)
			},
			setupInvalid: func(specDir string) error {
				// Missing required 'tasks' field
				content := `summary:
  total_tasks: 1
  total_phases: 1
phases: []
_meta:
  version: "1.0.0"
  artifact_type: "tasks"
`
				return os.WriteFile(filepath.Join(specDir, "tasks.yaml"), []byte(content), 0644)
			},
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			// Test valid artifact passes validation
			t.Run("valid_"+tc.stage+"_passes", func(t *testing.T) {
				t.Parallel()
				tmpDir := t.TempDir()
				specDir := filepath.Join(tmpDir, "specs", "001-test")
				if err := os.MkdirAll(specDir, 0755); err != nil {
					t.Fatalf("Failed to create spec directory: %v", err)
				}
				if err := tc.setupValid(specDir); err != nil {
					t.Fatalf("Failed to setup valid artifact: %v", err)
				}

				err := tc.validateFunc(specDir)
				if err != nil {
					t.Errorf("Valid %s should pass validation, got error: %v", tc.stage, err)
				}
			})

			// Test invalid artifact fails validation with correct error message
			t.Run("invalid_"+tc.stage+"_fails", func(t *testing.T) {
				t.Parallel()
				tmpDir := t.TempDir()
				specDir := filepath.Join(tmpDir, "specs", "001-test")
				if err := os.MkdirAll(specDir, 0755); err != nil {
					t.Fatalf("Failed to create spec directory: %v", err)
				}
				if err := tc.setupInvalid(specDir); err != nil {
					t.Fatalf("Failed to setup invalid artifact: %v", err)
				}

				err := tc.validateFunc(specDir)
				if err == nil {
					t.Errorf("Invalid %s should fail validation, got nil error", tc.stage)
					return
				}
				if !strings.Contains(err.Error(), tc.errContains) {
					t.Errorf("Error should contain %q, got: %v", tc.errContains, err)
				}
			})
		})
	}
}

// TestSchemaValidationRejectsInvalidBeforeNextStage verifies that invalid artifacts
// are caught before proceeding to the next workflow stage (FR-001, FR-002, FR-003)
func TestSchemaValidationRejectsInvalidBeforeNextStage(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		currentStage   string
		nextStage      string
		validateFunc   func(string) error
		invalidContent string
		expectedErr    string
	}{
		"invalid spec rejected before plan stage": {
			currentStage: "specify",
			nextStage:    "plan",
			validateFunc: ValidateSpecSchema,
			invalidContent: `# Missing required 'feature' field
user_stories: []
requirements:
  functional: []
_meta:
  artifact_type: "spec"
`,
			expectedErr: "missing required field: feature",
		},
		"invalid plan rejected before tasks stage": {
			currentStage: "plan",
			nextStage:    "tasks",
			validateFunc: ValidatePlanSchema,
			invalidContent: `# Missing required 'plan' field
summary: "Test"
_meta:
  artifact_type: "plan"
`,
			expectedErr: "missing required field: plan",
		},
		"invalid tasks rejected before implement stage": {
			currentStage: "tasks",
			nextStage:    "implement",
			validateFunc: ValidateTasksSchema,
			invalidContent: `# Missing required 'tasks' field
summary:
  total_tasks: 0
phases: []
_meta:
  artifact_type: "tasks"
`,
			expectedErr: "missing required field: tasks",
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			// Create invalid artifact file
			var artifactPath string
			switch tc.currentStage {
			case "specify":
				artifactPath = filepath.Join(specDir, "spec.yaml")
			case "plan":
				artifactPath = filepath.Join(specDir, "plan.yaml")
			case "tasks":
				artifactPath = filepath.Join(specDir, "tasks.yaml")
			}
			if err := os.WriteFile(artifactPath, []byte(tc.invalidContent), 0644); err != nil {
				t.Fatalf("Failed to write invalid artifact: %v", err)
			}

			// Run validation (simulates what ExecuteStage does after Claude generates output)
			err := tc.validateFunc(specDir)

			// Verify validation fails with expected error
			if err == nil {
				t.Errorf("Expected validation to fail for invalid %s artifact before %s stage", tc.currentStage, tc.nextStage)
				return
			}
			if !strings.Contains(err.Error(), tc.expectedErr) {
				t.Errorf("Error should contain %q, got: %v", tc.expectedErr, err)
			}
		})
	}
}

// TestValidArtifactsProceedToNextStage verifies that valid artifacts proceed without error
func TestValidArtifactsProceedToNextStage(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		stage        string
		validateFunc func(string) error
		content      string
		artifactName string
	}{
		"valid spec proceeds to plan": {
			stage:        "specify",
			validateFunc: ValidateSpecSchema,
			artifactName: "spec.yaml",
			content: `feature:
  branch: "001-valid-test"
  created: "2025-01-01"
  status: "Draft"
  input: "Valid test feature"
user_stories:
  - id: "US-001"
    title: "Test story"
    priority: "P1"
    as_a: "user"
    i_want: "to test"
    so_that: "I can verify"
    acceptance_scenarios:
      - given: "a test"
        when: "I run it"
        then: "it passes"
requirements:
  functional:
    - id: "FR-001"
      description: "MUST pass test"
      testable: true
      acceptance_criteria: "Test passes"
  non_functional: []
success_criteria:
  measurable_outcomes:
    - id: "SC-001"
      description: "Tests pass"
      metric: "Pass rate"
      target: "100%"
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`,
		},
		"valid plan proceeds to tasks": {
			stage:        "plan",
			validateFunc: ValidatePlanSchema,
			artifactName: "plan.yaml",
			content: `plan:
  branch: "001-valid-test"
  created: "2025-01-01"
  spec_path: "specs/001-valid-test/spec.yaml"
summary: "Valid implementation plan"
technical_context:
  language: "Go"
  framework: "Cobra"
  project_type: "cli"
implementation_phases:
  - phase: 1
    name: "Setup"
    goal: "Initial setup"
    deliverables:
      - "Project structure"
project_structure:
  source_code:
    - path: "cmd/main.go"
      description: "Entry point"
  tests:
    - path: "cmd/main_test.go"
      description: "Entry point tests"
  documentation: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "plan"
`,
		},
		"valid tasks proceeds to implement": {
			stage:        "tasks",
			validateFunc: ValidateTasksSchema,
			artifactName: "tasks.yaml",
			content: `tasks:
  branch: "001-valid-test"
  created: "2025-01-01"
  spec_path: "specs/001-valid-test/spec.yaml"
  plan_path: "specs/001-valid-test/plan.yaml"
summary:
  total_tasks: 1
  total_phases: 1
phases:
  - number: 1
    title: "Setup"
    purpose: "Initial setup"
    tasks:
      - id: "T001"
        title: "Initialize project"
        status: "Pending"
        type: "setup"
        parallel: false
        dependencies: []
        acceptance_criteria:
          - "Project compiles"
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "tasks"
`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-valid-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			artifactPath := filepath.Join(specDir, tc.artifactName)
			if err := os.WriteFile(artifactPath, []byte(tc.content), 0644); err != nil {
				t.Fatalf("Failed to write valid artifact: %v", err)
			}

			// Run validation
			err := tc.validateFunc(specDir)

			// Verify validation passes
			if err != nil {
				t.Errorf("Valid %s artifact should pass validation, got error: %v", tc.stage, err)
			}
		})
	}
}

// TestSchemaValidationErrorMessageFormat verifies error messages are consistent (SC-003)
func TestSchemaValidationErrorMessageFormat(t *testing.T) {
	t.Parallel()

	// All schema validation errors should follow the same format:
	// "schema validation failed for <artifact>:\n- <error1>\n- <error2>..."
	tests := map[string]struct {
		validateFunc func(string) error
		artifactName string
		content      string
	}{
		"spec error format": {
			validateFunc: ValidateSpecSchema,
			artifactName: "spec.yaml",
			content: `user_stories: []
_meta:
  artifact_type: "spec"
`,
		},
		"plan error format": {
			validateFunc: ValidatePlanSchema,
			artifactName: "plan.yaml",
			content: `summary: "test"
_meta:
  artifact_type: "plan"
`,
		},
		"tasks error format": {
			validateFunc: ValidateTasksSchema,
			artifactName: "tasks.yaml",
			content: `phases: []
_meta:
  artifact_type: "tasks"
`,
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			artifactPath := filepath.Join(specDir, tc.artifactName)
			if err := os.WriteFile(artifactPath, []byte(tc.content), 0644); err != nil {
				t.Fatalf("Failed to write artifact: %v", err)
			}

			err := tc.validateFunc(specDir)
			if err == nil {
				t.Fatal("Expected validation error")
			}

			errStr := err.Error()
			// Verify error format is consistent
			if !strings.HasPrefix(errStr, "schema validation failed for "+tc.artifactName) {
				t.Errorf("Error should start with 'schema validation failed for %s', got: %s", tc.artifactName, errStr)
			}
			if !strings.Contains(errStr, "- ") {
				t.Errorf("Error should contain bullet point format '- ', got: %s", errStr)
			}
		})
	}
}

// TestSchemaValidationInAutospecRunPath verifies schema validation in the autospec run -a path (US-003, T014).
// This test simulates the full workflow run path where an invalid spec.yaml should be rejected
// before proceeding to the plan stage.
func TestSchemaValidationInAutospecRunPath(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		specContent      string
		expectValidation bool
		errContains      string
		description      string
	}{
		"invalid spec missing feature field rejected before plan": {
			specContent: `# Missing required 'feature' field
user_stories: []
requirements:
  functional: []
  non_functional: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`,
			expectValidation: false,
			errContains:      "missing required field: feature",
			description:      "Spec without 'feature' field should fail schema validation",
		},
		"invalid spec missing branch rejected before plan": {
			specContent: `feature:
  created: "2025-01-01"
  status: "Draft"
user_stories: []
requirements:
  functional: []
  non_functional: []
_meta:
  version: "1.0.0"
  artifact_type: "spec"
`,
			expectValidation: false,
			errContains:      "missing required field: branch",
			description:      "Spec without 'feature.branch' should fail schema validation",
		},
		"valid spec proceeds to plan stage": {
			specContent: `feature:
  branch: "001-test-feature"
  created: "2025-01-01"
  status: "Draft"
  input: "Test feature for validation"
user_stories:
  - id: "US-001"
    title: "Test story"
    priority: "P1"
    as_a: "developer"
    i_want: "to test validation"
    so_that: "I can verify the workflow"
    acceptance_scenarios:
      - given: "a valid spec"
        when: "validation runs"
        then: "it passes"
requirements:
  functional:
    - id: "FR-001"
      description: "MUST validate spec"
      testable: true
      acceptance_criteria: "Spec passes validation"
  non_functional: []
success_criteria:
  measurable_outcomes:
    - id: "SC-001"
      description: "Validation passes"
      metric: "Pass rate"
      target: "100%"
key_entities: []
edge_cases: []
assumptions: []
constraints: []
out_of_scope: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "spec"
`,
			expectValidation: true,
			errContains:      "",
			description:      "Valid spec should pass schema validation",
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test-feature")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			// Write spec.yaml (simulating output from specify stage)
			specPath := filepath.Join(specDir, "spec.yaml")
			if err := os.WriteFile(specPath, []byte(tc.specContent), 0644); err != nil {
				t.Fatalf("Failed to write spec.yaml: %v", err)
			}

			// Run schema validation (same function used in autospec run -a path)
			err := ValidateSpecSchema(specDir)

			// Verify validation result matches expected outcome
			if tc.expectValidation {
				if err != nil {
					t.Errorf("%s: expected validation to pass, got error: %v", tc.description, err)
				}
			} else {
				if err == nil {
					t.Errorf("%s: expected validation to fail, but it passed", tc.description)
					return
				}
				// Verify error message format matches standalone specify command
				errStr := err.Error()
				if !strings.HasPrefix(errStr, "schema validation failed for spec.yaml") {
					t.Errorf("Error format mismatch: expected prefix 'schema validation failed for spec.yaml', got: %s", errStr)
				}
				if tc.errContains != "" && !strings.Contains(errStr, tc.errContains) {
					t.Errorf("Error should contain %q, got: %s", tc.errContains, errStr)
				}
			}
		})
	}
}

// TestSchemaValidationInAutospecPrepPath verifies schema validation in the autospec prep path (US-003, T015).
// This test simulates the prep workflow where an invalid plan.yaml should be rejected
// before proceeding to the tasks stage.
func TestSchemaValidationInAutospecPrepPath(t *testing.T) {
	t.Parallel()

	tests := map[string]struct {
		planContent      string
		expectValidation bool
		errContains      string
		description      string
	}{
		"invalid plan missing plan field rejected before tasks": {
			planContent: `# Missing required 'plan' field
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`,
			expectValidation: false,
			errContains:      "missing required field: plan",
			description:      "Plan without 'plan' field should fail schema validation",
		},
		"invalid plan missing branch rejected before tasks": {
			planContent: `plan:
  created: "2025-01-01"
  spec_path: "specs/001-test/spec.yaml"
summary: "Test plan"
technical_context:
  language: "Go"
_meta:
  version: "1.0.0"
  artifact_type: "plan"
`,
			expectValidation: false,
			errContains:      "missing required field: branch",
			description:      "Plan without 'plan.branch' should fail schema validation",
		},
		"valid plan proceeds to tasks stage": {
			planContent: `plan:
  branch: "001-test-feature"
  created: "2025-01-01"
  spec_path: "specs/001-test-feature/spec.yaml"
summary: "Implementation plan for test feature"
technical_context:
  language: "Go"
  framework: "Cobra"
  project_type: "cli"
implementation_phases:
  - phase: 1
    name: "Setup"
    goal: "Initial setup"
    deliverables:
      - "Project structure"
project_structure:
  source_code:
    - path: "cmd/main.go"
      description: "Entry point"
  tests:
    - path: "cmd/main_test.go"
      description: "Entry point tests"
  documentation: []
_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "test"
  created: "2025-01-01T00:00:00Z"
  artifact_type: "plan"
`,
			expectValidation: true,
			errContains:      "",
			description:      "Valid plan should pass schema validation",
		},
	}

	for name, tc := range tests {
		t.Run(name, func(t *testing.T) {
			t.Parallel()

			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test-feature")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory: %v", err)
			}

			// Write plan.yaml (simulating output from plan stage)
			planPath := filepath.Join(specDir, "plan.yaml")
			if err := os.WriteFile(planPath, []byte(tc.planContent), 0644); err != nil {
				t.Fatalf("Failed to write plan.yaml: %v", err)
			}

			// Run schema validation (same function used in autospec prep path)
			err := ValidatePlanSchema(specDir)

			// Verify validation result matches expected outcome
			if tc.expectValidation {
				if err != nil {
					t.Errorf("%s: expected validation to pass, got error: %v", tc.description, err)
				}
			} else {
				if err == nil {
					t.Errorf("%s: expected validation to fail, but it passed", tc.description)
					return
				}
				// Verify error message format matches standalone plan command
				errStr := err.Error()
				if !strings.HasPrefix(errStr, "schema validation failed for plan.yaml") {
					t.Errorf("Error format mismatch: expected prefix 'schema validation failed for plan.yaml', got: %s", errStr)
				}
				if tc.errContains != "" && !strings.Contains(errStr, tc.errContains) {
					t.Errorf("Error should contain %q, got: %s", tc.errContains, errStr)
				}
			}
		})
	}
}

// assertSchemaValidationErrorFormat is a helper function that verifies the consistency
// of schema validation error messages across different entry points (US-003, T016).
// It ensures:
// - Error prefix is consistent: "schema validation failed for <artifact>:"
// - Error contains bullet point format "- <error message>"
// - Error includes the expected field/message
func assertSchemaValidationErrorFormat(t *testing.T, err error, artifactName string, expectedContent string) {
	t.Helper()

	if err == nil {
		t.Fatalf("Expected validation error for %s, got nil", artifactName)
	}

	errStr := err.Error()

	// Check consistent prefix format
	expectedPrefix := fmt.Sprintf("schema validation failed for %s:", artifactName)
	if !strings.HasPrefix(errStr, expectedPrefix) {
		t.Errorf("Error format inconsistent: expected prefix %q, got: %s", expectedPrefix, errStr)
	}

	// Check bullet point format is present
	if !strings.Contains(errStr, "- ") {
		t.Errorf("Error format inconsistent: expected bullet point format '- ', got: %s", errStr)
	}

	// Check expected content is present
	if expectedContent != "" && !strings.Contains(errStr, expectedContent) {
		t.Errorf("Error should contain %q, got: %s", expectedContent, errStr)
	}
}

// TestSchemaValidationErrorConsistencyAcrossEntryPoints verifies that error messages
// are consistent across different autospec entry points (US-003, T016).
// This test checks that the same validation failure produces identical error format
// regardless of whether it's triggered via:
// - autospec run -a (specify  plan  tasks  implement)
// - autospec specify (standalone)
// - autospec prep (specify  plan  tasks)
// - autospec plan (standalone)
// - autospec tasks (standalone)
func TestSchemaValidationErrorConsistencyAcrossEntryPoints(t *testing.T) {
	t.Parallel()

	// Test that the same invalid spec produces consistent errors
	// regardless of which command path invokes validation
	t.Run("spec validation error format consistency", func(t *testing.T) {
		t.Parallel()

		invalidSpecContent := `user_stories: []
requirements:
  functional: []
_meta:
  artifact_type: "spec"
`

		// Create two separate directories to simulate different entry points
		entries := []string{"run-a", "specify-standalone"}
		errors := make(map[string]error)

		for _, entry := range entries {
			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory for %s: %v", entry, err)
			}

			specPath := filepath.Join(specDir, "spec.yaml")
			if err := os.WriteFile(specPath, []byte(invalidSpecContent), 0644); err != nil {
				t.Fatalf("Failed to write spec.yaml for %s: %v", entry, err)
			}

			// Both entry points use the same ValidateSpecSchema function
			errors[entry] = ValidateSpecSchema(specDir)
		}

		// Verify both entry points produce errors
		for entry, err := range errors {
			assertSchemaValidationErrorFormat(t, err, "spec.yaml", "missing required field: feature")
			t.Logf("%s error: %v", entry, err)
		}

		// Verify error formats are identical (prefix and structure)
		runErr := errors["run-a"].Error()
		specifyErr := errors["specify-standalone"].Error()
		if runErr != specifyErr {
			t.Errorf("Error format mismatch across entry points:\n  run -a: %s\n  specify: %s", runErr, specifyErr)
		}
	})

	// Test that the same invalid plan produces consistent errors
	t.Run("plan validation error format consistency", func(t *testing.T) {
		t.Parallel()

		invalidPlanContent := `summary: "test"
technical_context:
  language: "Go"
_meta:
  artifact_type: "plan"
`

		entries := []string{"prep", "plan-standalone"}
		errors := make(map[string]error)

		for _, entry := range entries {
			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory for %s: %v", entry, err)
			}

			planPath := filepath.Join(specDir, "plan.yaml")
			if err := os.WriteFile(planPath, []byte(invalidPlanContent), 0644); err != nil {
				t.Fatalf("Failed to write plan.yaml for %s: %v", entry, err)
			}

			errors[entry] = ValidatePlanSchema(specDir)
		}

		for entry, err := range errors {
			assertSchemaValidationErrorFormat(t, err, "plan.yaml", "missing required field: plan")
			t.Logf("%s error: %v", entry, err)
		}

		prepErr := errors["prep"].Error()
		planErr := errors["plan-standalone"].Error()
		if prepErr != planErr {
			t.Errorf("Error format mismatch across entry points:\n  prep: %s\n  plan: %s", prepErr, planErr)
		}
	})

	// Test that the same invalid tasks produces consistent errors
	t.Run("tasks validation error format consistency", func(t *testing.T) {
		t.Parallel()

		invalidTasksContent := `summary:
  total_tasks: 0
phases: []
_meta:
  artifact_type: "tasks"
`

		entries := []string{"run-a", "tasks-standalone"}
		errors := make(map[string]error)

		for _, entry := range entries {
			tmpDir := t.TempDir()
			specDir := filepath.Join(tmpDir, "specs", "001-test")
			if err := os.MkdirAll(specDir, 0755); err != nil {
				t.Fatalf("Failed to create spec directory for %s: %v", entry, err)
			}

			tasksPath := filepath.Join(specDir, "tasks.yaml")
			if err := os.WriteFile(tasksPath, []byte(invalidTasksContent), 0644); err != nil {
				t.Fatalf("Failed to write tasks.yaml for %s: %v", entry, err)
			}

			errors[entry] = ValidateTasksSchema(specDir)
		}

		for entry, err := range errors {
			assertSchemaValidationErrorFormat(t, err, "tasks.yaml", "missing required field: tasks")
			t.Logf("%s error: %v", entry, err)
		}

		runErr := errors["run-a"].Error()
		tasksErr := errors["tasks-standalone"].Error()
		if runErr != tasksErr {
			t.Errorf("Error format mismatch across entry points:\n  run -a: %s\n  tasks: %s", runErr, tasksErr)
		}
	})
}
