feature:
    branch: "041-orchestrator-schema-validation"
    created: "2025-12-18"
    status: "Completed"
    completed_at: 2025-12-18T03:38:13Z
    input: |
        The Go orchestrator only validates file existence after each stage completes, not schema compliance.
        This affects ALL stage execution paths: autospec run, specify, plan, tasks, prep, implement.
        Claude's slash commands include artifact validation calls, but Claude might skip the validation step,
        output might pass YAML syntax but fail schema validation, and the orchestrator doesn't catch this
        before proceeding to the next stage. The solution is to replace file-existence checks with full
        schema validation using existing artifact validators (SpecValidator, PlanValidator, TasksValidator).
        On retry, validation errors must be injected into the next Claude invocation so it knows exactly
        what to fix.
user_stories:
    - id: "US-001"
      title: "Programmatic schema validation after artifact generation"
      priority: "P1"
      as_a: "developer using autospec"
      i_want: "the orchestrator to validate generated artifacts against their full schema"
      so_that: "malformed artifacts are caught before proceeding to the next workflow stage"
      why_this_priority: "Core functionality - without this, invalid artifacts cascade through the pipeline causing downstream failures"
      independent_test: "Generate an artifact with missing required fields and verify the orchestrator rejects it"
      acceptance_scenarios:
        - given: "Claude generates a spec.yaml missing required field 'feature.branch'"
          when: "the specify stage completes and validation runs"
          then: "the orchestrator returns a schema validation error listing the missing field"
        - given: "Claude generates a plan.yaml with invalid enum value 'status=pending'"
          when: "the plan stage completes and validation runs"
          then: "the orchestrator returns a schema validation error identifying the invalid enum"
        - given: "Claude generates a tasks.yaml with correct schema"
          when: "the tasks stage completes and validation runs"
          then: "the orchestrator accepts the artifact and proceeds to the next stage"
    - id: "US-002"
      title: "Actionable error feedback on retry"
      priority: "P1"
      as_a: "developer using autospec"
      i_want: "validation errors to be passed to Claude on retry attempts"
      so_that: "Claude can fix specific schema errors instead of regenerating blindly"
      why_this_priority: "Critical for retry effectiveness - without error context, retries are guesswork"
      independent_test: "Trigger a schema validation failure with retries enabled and verify the retry prompt includes the error details"
      acceptance_scenarios:
        - given: "spec.yaml validation failed on attempt 1 of 3 with error 'missing required field: feature.branch'"
          when: "the orchestrator initiates retry attempt 2"
          then: "the Claude command includes the error message and retry indicator 'RETRY 2/3'"
        - given: "plan.yaml validation failed with multiple schema errors"
          when: "the orchestrator initiates a retry"
          then: "all schema errors are included in the retry prompt"
        - given: "validation succeeds on first attempt"
          when: "the stage completes"
          then: "no retry context is injected into any subsequent commands"
    - id: "US-003"
      title: "Consistent validation across all execution paths"
      priority: "P1"
      as_a: "developer using autospec"
      i_want: "schema validation to work identically across all autospec commands"
      so_that: "I get the same validation quality regardless of how I invoke the tool"
      why_this_priority: "User expectation - validation behavior should be predictable across all entry points"
      independent_test: "Run the same invalid artifact through 'autospec run -a', 'autospec specify', and 'autospec prep' and verify identical error handling"
      acceptance_scenarios:
        - given: "an invalid spec.yaml would be generated"
          when: "running 'autospec run -a'"
          then: "schema validation catches the error before proceeding to plan stage"
        - given: "an invalid spec.yaml would be generated"
          when: "running 'autospec specify'"
          then: "schema validation catches the error with the same error message format"
        - given: "an invalid plan.yaml would be generated"
          when: "running 'autospec prep'"
          then: "schema validation catches the error before proceeding to tasks stage"
requirements:
    functional:
        - id: "FR-001"
          description: "MUST validate spec.yaml against full schema after specify stage completes"
          testable: true
          acceptance_criteria: "ValidateSpecSchema() function returns error when spec.yaml has missing required fields, invalid enum values, or wrong types"
        - id: "FR-002"
          description: "MUST validate plan.yaml against full schema after plan stage completes"
          testable: true
          acceptance_criteria: "ValidatePlanSchema() function returns error when plan.yaml has missing required fields, invalid enum values, or wrong types"
        - id: "FR-003"
          description: "MUST validate tasks.yaml against full schema after tasks stage completes"
          testable: true
          acceptance_criteria: "ValidateTasksSchema() function returns error when tasks.yaml has missing required fields, invalid enum values, invalid dependency references, or wrong types"
        - id: "FR-004"
          description: "MUST include full validation error message in retry command when validation fails"
          testable: true
          acceptance_criteria: "buildRetryCommand() returns command string containing all schema errors from the failed validation"
        - id: "FR-005"
          description: "MUST include retry attempt indicator (e.g., 'RETRY 2/3') in retry command"
          testable: true
          acceptance_criteria: "Retry command string starts with stage command followed by 'RETRY X/Y' where X is current attempt and Y is max retries"
        - id: "FR-009"
          description: "MUST inject retry context into existing $ARGUMENTS placeholder in command templates"
          testable: true
          acceptance_criteria: "Orchestrator passes retry info via the same $ARGUMENTS used for normal input; no new template parameters required"
        - id: "FR-010"
          description: "MUST update all command templates (internal/commands/*.md) to document retry context handling"
          testable: true
          acceptance_criteria: "Each template (specify, plan, tasks, implement, clarify, analyze, checklist, constitution) includes a 'Retry Context' section explaining how to parse and handle retry info in $ARGUMENTS"
        - id: "FR-011"
          description: "MUST use standardized retry context format in $ARGUMENTS"
          testable: true
          acceptance_criteria: "Retry context format is 'RETRY X/Y\\nSchema validation failed:\\n- error1\\n- error2' prepended to any existing arguments"
        - id: "FR-006"
          description: "MUST wire schema validators into workflow execution for all stages"
          testable: true
          acceptance_criteria: "executePlan(), executeTasks(), and executeSpecify() pass schema validator functions to ExecuteStage()"
        - id: "FR-007"
          description: "MUST use existing artifact validators (NewSpecValidator, NewPlanValidator, NewTasksValidator) for schema validation"
          testable: true
          acceptance_criteria: "ValidateXxxSchema functions call validation.NewXxxValidator().Validate() internally"
        - id: "FR-008"
          description: "MUST pass all quality gates: make test, make fmt, make lint, and make build"
          testable: true
          acceptance_criteria: "All commands exit 0; no test failures, format changes, lint errors, or build failures"
    non_functional:
        - id: "NFR-001"
          category: "performance"
          description: "Schema validation MUST complete within performance budget"
          measurable_target: "Each validation function completes in under 10ms (existing validation performance contract)"
        - id: "NFR-002"
          category: "code_quality"
          description: "All functions MUST be under 40 lines; extract helpers for complex logic"
          measurable_target: "No function exceeds 40 lines excluding comments"
        - id: "NFR-003"
          category: "code_quality"
          description: "All errors MUST be wrapped with context using fmt.Errorf(\"doing X: %w\", err)"
          measurable_target: "Zero bare 'return err' statements in new code"
        - id: "NFR-004"
          category: "code_quality"
          description: "Tests MUST use map-based table-driven pattern with t.Parallel()"
          measurable_target: "All new test functions use map[string]struct pattern and call t.Parallel()"
        - id: "NFR-005"
          category: "reliability"
          description: "Schema validation MUST not introduce regressions in existing validation behavior"
          measurable_target: "All existing validation tests continue to pass"
success_criteria:
    measurable_outcomes:
        - id: "SC-001"
          description: "Schema-invalid artifacts are rejected before stage transition"
          metric: "Percentage of invalid artifacts caught by orchestrator"
          target: "100% of artifacts with schema errors are rejected before next stage begins"
        - id: "SC-002"
          description: "Retry attempts include actionable error information"
          metric: "Retry prompts containing schema error details"
          target: "100% of retry commands include the specific validation errors that caused failure"
        - id: "SC-003"
          description: "Validation behavior is consistent across entry points"
          metric: "Error message consistency across autospec run, specify, plan, tasks, prep commands"
          target: "Identical error format and content for same validation failure regardless of entry point"
        - id: "SC-004"
          description: "Existing functionality remains unaffected"
          metric: "Regression test pass rate"
          target: "100% of existing tests pass after implementation"
key_entities:
    - name: "SchemaValidator"
      description: "Component that validates YAML artifacts against their defined schema"
      attributes:
        - "validate(filePath) returns ValidationResult"
        - "handles spec.yaml, plan.yaml, tasks.yaml formats"
    - name: "ValidationResult"
      description: "Outcome of schema validation including validity status and error details"
      attributes:
        - "Valid (boolean)"
        - "Errors (list of validation errors)"
        - "FormatErrors() returns human-readable error string"
    - name: "RetryCommand"
      description: "Command string sent to Claude on retry, containing error context"
      attributes:
        - "Stage command (e.g., /autospec.plan)"
        - "Retry indicator (RETRY X/Y)"
        - "Validation errors from previous attempt"
    - name: "RetryContext"
      description: "Standardized format for retry information injected into $ARGUMENTS"
      attributes:
        - "Format: 'RETRY X/Y' on first line where X=current attempt, Y=max retries"
        - "Second line: 'Schema validation failed:'"
        - "Subsequent lines: '- <error message>' for each validation error"
        - "Followed by blank line then original arguments (if any)"
        - "Example: 'RETRY 2/3\\nSchema validation failed:\\n- missing required field: feature.branch\\n\\n<original args>'"
edge_cases:
    - scenario: "Validation error contains special characters or very long error messages"
      expected_behavior: "Error message is properly escaped/formatted for command injection without truncation"
    - scenario: "Multiple validation errors occur simultaneously"
      expected_behavior: "All errors are collected and included in the retry prompt, not just the first error"
    - scenario: "Validation passes but file is empty or nearly empty"
      expected_behavior: "Empty/minimal files with required fields are accepted; files without required fields are rejected"
    - scenario: "Retry limit exhausted with validation still failing"
      expected_behavior: "Final error includes accumulated validation errors and exits with appropriate code"
    - scenario: "tasks.yaml references non-existent task IDs in dependencies"
      expected_behavior: "Dependency validation catches invalid references and includes them in error message"
    - scenario: "Retry with no original $ARGUMENTS (empty initial input)"
      expected_behavior: "Retry context is injected as the sole content of $ARGUMENTS; template handles gracefully"
    - scenario: "First attempt (not a retry)"
      expected_behavior: "$ARGUMENTS contains only user input with no retry prefix; templates process normally"
assumptions:
    - "Existing artifact validators (NewSpecValidator, NewPlanValidator, NewTasksValidator) are fully implemented and tested"
    - "Existing validation performance contract (<10ms) is already met by current validators"
    - "Retry system already exists and handles retry counting and limits"
    - "ExecuteStage function accepts a validation callback that can be replaced"
    - "Error injection into Claude commands is technically feasible via command string modification"
constraints:
    - "Must reuse existing validation infrastructure - no new validation logic for schema rules"
    - "Must maintain backward compatibility with existing CLI behavior"
    - "Validation functions must not exceed 40 lines per Go best practices"
    - "Must not change the public API of existing commands"
out_of_scope:
    - "Modifying schema definitions for spec.yaml, plan.yaml, or tasks.yaml"
    - "Adding new validation rules beyond what existing validators check"
    - "Implementing validation for other artifact types (checklist.yaml, constitution.yaml, etc.)"
    - "Adding new template parameters beyond existing $ARGUMENTS"
    - "Performance optimization of existing validators"
    - "Interactive/real-time validation during Claude generation"
_meta:
    version: "1.1.0"
    generator: "autospec"
    generator_version: "autospec dev"
    created: "2025-12-18T03:02:50Z"
    artifact_type: "spec"
