feature:
    branch: "040-workflow-mock-coverage"
    created: "2025-12-18"
    status: "Completed"
    completed_at: 2025-12-18T03:30:31Z
    input: "Enhance workflow test coverage to 85-90% by implementing proper mock integration that creates artifact files during test execution, enabling actual Execute* method testing"
user_stories:
    - id: "US-001"
      title: "Test Execute* Methods with Artifact-Generating Mocks"
      priority: "P1"
      as_a: "developer maintaining autospec"
      i_want: "workflow tests that actually call Execute* methods using mocks that create expected artifact files"
      so_that: "I can achieve 85%+ test coverage on workflow package and catch regressions in orchestration logic"
      why_this_priority: "Core workflow orchestration is critical functionality - 50% coverage is unacceptable for production code"
      independent_test: "Run go test -cover ./internal/workflow/ and verify coverage >= 85%"
      acceptance_scenarios:
        - given: "a test for ExecuteSpecify with MockClaudeExecutor configured"
          when: "the test calls orchestrator.ExecuteSpecify(featureDescription)"
          then: "the mock creates spec.yaml in the expected location and returns success"
        - given: "a test for ExecutePlan with MockClaudeExecutor configured"
          when: "the test calls orchestrator.ExecutePlan(specName, prompt)"
          then: "the mock creates plan.yaml in the expected location and returns success"
        - given: "a test for ExecuteTasks with MockClaudeExecutor configured"
          when: "the test calls orchestrator.ExecuteTasks(specName, prompt)"
          then: "the mock creates tasks.yaml in the expected location and returns success"
        - given: "a test for ExecuteImplement with MockClaudeExecutor configured"
          when: "the test calls orchestrator.ExecuteImplement(specName, options)"
          then: "the mock simulates implementation and updates task statuses"
    - id: "US-002"
      title: "Test Full Workflow Orchestration"
      priority: "P1"
      as_a: "developer maintaining autospec"
      i_want: "tests that exercise RunCompleteWorkflow and RunFullWorkflow end-to-end"
      so_that: "I can verify the complete specify->plan->tasks->implement flow works correctly"
      why_this_priority: "These are the main entry points for the CLI - must be tested"
      independent_test: "Run tests for RunCompleteWorkflow and RunFullWorkflow, verify they execute all stages"
      acceptance_scenarios:
        - given: "MockClaudeExecutor configured to generate artifacts for each stage"
          when: "test calls RunCompleteWorkflow(featureDescription)"
          then: "all three stages execute and artifacts are created in sequence"
        - given: "MockClaudeExecutor configured for full workflow"
          when: "test calls RunFullWorkflow(featureDescription, resume=false)"
          then: "all four stages execute including implementation"
    - id: "US-003"
      title: "Test Workflow Error Handling Paths"
      priority: "P2"
      as_a: "developer maintaining autospec"
      i_want: "tests that cover error handling in Execute* methods"
      so_that: "I can verify proper error propagation and retry behavior"
      why_this_priority: "Error paths are important but less critical than happy paths"
      independent_test: "Run tests with mock errors configured, verify correct error messages"
      acceptance_scenarios:
        - given: "MockClaudeExecutor configured to fail on first attempt"
          when: "ExecuteSpecify is called"
          then: "error is wrapped with context and returned"
        - given: "MockClaudeExecutor configured to fail then succeed"
          when: "ExecuteWithRetry is called"
          then: "retry mechanism is exercised and second attempt succeeds"
requirements:
    functional:
        - id: "FR-001"
          description: "mock-claude.sh MUST generate valid artifact files (spec.yaml, plan.yaml, tasks.yaml) based on the command invoked"
          testable: true
          acceptance_criteria: "When mock-claude.sh receives /autospec.specify, it creates spec.yaml; /autospec.plan creates plan.yaml; etc."
        - id: "FR-002"
          description: "Tests MUST actually call the Execute* methods being tested (not just setup code)"
          testable: true
          acceptance_criteria: "grep for 'orchestrator.Execute' calls in test file shows actual method invocations"
        - id: "FR-003"
          description: "MockClaudeExecutor MUST be properly injected into WorkflowOrchestrator for tests"
          testable: true
          acceptance_criteria: "Tests use orchestrator.Executor.Claude = mock pattern correctly"
        - id: "FR-004"
          description: "Tests MUST cover ExecuteSpecify, ExecutePlan, ExecuteTasks, ExecuteImplement methods"
          testable: true
          acceptance_criteria: "Coverage report shows >0% for each Execute* method"
        - id: "FR-005"
          description: "Tests MUST cover RunCompleteWorkflow and RunFullWorkflow methods"
          testable: true
          acceptance_criteria: "Coverage report shows >0% for each Run* method"
        - id: "FR-006"
          description: "Tests MUST cover ExecuteConstitution, ExecuteClarify, ExecuteChecklist, ExecuteAnalyze methods"
          testable: true
          acceptance_criteria: "Coverage report shows >0% for each auxiliary Execute* method"
        - id: "FR-007"
          description: "Mock artifact files MUST pass autospec validation"
          testable: true
          acceptance_criteria: "Generated spec.yaml, plan.yaml, tasks.yaml pass validation.ValidateXXX functions"
        - id: "FR-008"
          description: "MUST pass all quality gates: make test, make fmt, make lint, make build"
          testable: true
          acceptance_criteria: "All commands exit 0; no test failures, format changes, lint errors, or build failures"
        - id: "FR-009"
          description: "Tests MUST use mock-claude.sh as ClaudeCmd - this script simulates Claude behavior without ever making real API calls"
          testable: true
          acceptance_criteria: "All tests set ClaudeCmd to mock-claude.sh path; script creates expected artifacts and never contacts Claude API"
        - id: "FR-010"
          description: "Tests MUST NEVER change git branches or modify repository git state"
          testable: true
          acceptance_criteria: "No git checkout/branch commands; tests verify branch unchanged after execution"
        - id: "FR-011"
          description: "Each test MUST use a unique isolated temp directory via t.TempDir()"
          testable: true
          acceptance_criteria: "Every test creates its own t.TempDir() for specs/state; no shared directories"
        - id: "FR-012"
          description: "All mock resources MUST be automatically cleaned up after test completion"
          testable: true
          acceptance_criteria: "Use t.TempDir() (auto-cleanup) or t.Cleanup() for manual resources; no leaked files"
    non_functional:
        - id: "NFR-001"
          category: "code_quality"
          description: "workflow package MUST achieve at least 85% test coverage"
          measurable_target: "go test -cover ./internal/workflow/ reports >= 85%"
        - id: "NFR-002"
          category: "code_quality"
          description: "workflow package SHOULD achieve 90% coverage as it is core functionality"
          measurable_target: "go test -cover ./internal/workflow/ reports >= 90%"
        - id: "NFR-003"
          category: "code_quality"
          description: "All functions must be under 40 lines; extract helpers for complex logic"
          measurable_target: "No function exceeds 40 lines excluding comments"
        - id: "NFR-004"
          category: "code_quality"
          description: "All errors must be wrapped with context using fmt.Errorf(\"doing X: %w\", err)"
          measurable_target: "Zero bare 'return err' statements in new code"
        - id: "NFR-005"
          category: "code_quality"
          description: "Tests must use map-based table-driven pattern with t.Parallel()"
          measurable_target: "All new test functions use map[string]struct pattern and call t.Parallel()"
        - id: "NFR-006"
          category: "reliability"
          description: "Tests must be deterministic and not flaky"
          measurable_target: "3 consecutive test runs all pass"
success_criteria:
    measurable_outcomes:
        - id: "SC-001"
          description: "Workflow package test coverage reaches 85% minimum"
          metric: "Coverage percentage from go test -cover"
          target: ">= 85%"
        - id: "SC-002"
          description: "All Execute* methods have non-zero coverage"
          metric: "go tool cover -func shows coverage for each method"
          target: "Every Execute* method > 0%"
        - id: "SC-003"
          description: "All Run* workflow methods have non-zero coverage"
          metric: "go tool cover -func shows coverage for RunCompleteWorkflow, RunFullWorkflow"
          target: "Both methods > 0%"
        - id: "SC-004"
          description: "Quality gates pass"
          metric: "Exit codes from make test, make fmt, make lint, make build"
          target: "All exit 0"
key_entities:
    - name: "mock-claude.sh"
      description: "Shell script that simulates Claude CLI behavior without making real API calls"
      attributes:
        - "Parses command to detect stage (specify, plan, tasks, implement)"
        - "Generates valid artifact files in the correct location"
        - "Supports MOCK_EXIT_CODE for error simulation"
        - "Supports MOCK_DELAY for timeout testing"
        - "Logs calls to MOCK_CALL_LOG for verification"
    - name: "WorkflowOrchestrator"
      description: "Main orchestration component being tested"
      attributes:
        - "Executor with injectable Claude implementation"
        - "Config for specs directory and other settings"
        - "Execute* methods for each workflow stage"
        - "Run* methods for complete workflows"
edge_cases:
    - scenario: "Execute* called with empty feature description"
      expected_behavior: "Method accepts empty string (validation happens at command level)"
    - scenario: "Execute* called when artifact already exists"
      expected_behavior: "Method succeeds and overwrites existing artifact"
    - scenario: "Mock configured to fail on specific stage"
      expected_behavior: "Error properly propagated with stage context"
    - scenario: "RunFullWorkflow with resume=true"
      expected_behavior: "Workflow continues from last successful stage"
    - scenario: "ClaudeCmd is executed during test"
      expected_behavior: "mock-claude.sh runs instead of real claude; creates artifacts locally, never makes API calls"
    - scenario: "Multiple tests run in parallel"
      expected_behavior: "Each test has isolated t.TempDir(); no file conflicts or race conditions"
    - scenario: "Test panics or fails mid-execution"
      expected_behavior: "t.TempDir() and t.Cleanup() ensure automatic resource cleanup"
    - scenario: "WorkflowOrchestrator tries to detect git branch"
      expected_behavior: "Tests provide mock spec metadata; no git commands executed"
assumptions:
    - "Existing MockClaudeExecutor can be enhanced without breaking existing tests"
    - "Test temp directories are properly cleaned up via t.TempDir() or t.Cleanup()"
    - "Artifact templates from generate-mock-artifacts.sh are valid and can be adapted"
    - "Tests can run in parallel with proper isolation"
constraints:
    - "CRITICAL: ClaudeCmd MUST be set to mock-claude.sh in all tests - script never makes real API calls (costs $0)"
    - "CRITICAL: Must NEVER checkout git branches or modify repository git state"
    - "CRITICAL: Each test must run in its own unique t.TempDir() - no shared state between tests"
    - "CRITICAL: All test resources (temp dirs, files, mocks) must be automatically cleaned up"
    - "Must not modify actual repository files during tests"
    - "Must maintain backward compatibility with existing test patterns"
    - "Test execution time should not significantly increase (< 10 seconds added)"
    - "Tests must be parallelizable without interference (unique directories per test)"
out_of_scope:
    - "Testing actual Claude CLI integration (covered by separate integration tests)"
    - "Increasing coverage for other packages (completion, notify, health, retry)"
    - "Adding new workflow features or functionality"
    - "Refactoring workflow.go implementation (only adding tests)"
_meta:
    version: "1.0.0"
    generator: "autospec"
    generator_version: "autospec dev"
    created: "2025-12-18T02:41:51Z"
    artifact_type: "spec"
