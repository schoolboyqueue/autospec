feature:
  branch: "047-task-complexity"
  created: "2025-12-18"
  status: "Draft"
  input: "Add optional 'complexity' field to tasks in tasks.yaml. PROBLEM: No way to estimate effort or identify tasks likely to fail. SCHEMA CHANGE: Add complexity field (XS/S/M/L/XL) to TaskItem struct. VALIDATION: Emit warning if complexity missing on implementation tasks. DISPLAY: Show total complexity in 'autospec st' summary. ANALYTICS: Track actual completion time vs estimated complexity to calibrate future estimates. BENEFIT: Helps identify tasks that need session isolation (--tasks flag) vs batching."

user_stories:
  - id: "US-001"
    title: "Task author assigns complexity estimates to tasks"
    priority: "P1"
    as_a: "task author"
    i_want: "to assign a complexity estimate (XS/S/M/L/XL) to each task"
    so_that: "I can communicate expected effort and identify tasks that may need special handling"
    why_this_priority: "Core functionality - without complexity field, no other features can work"
    independent_test: "Create tasks.yaml with complexity field on tasks, run validation, verify it passes"
    acceptance_scenarios:
      - given: "A tasks.yaml file with tasks"
        when: "I add complexity: 'M' to a task"
        then: "The file validates successfully"
      - given: "A task with an invalid complexity value"
        when: "I run autospec artifact on tasks.yaml"
        then: "Validation fails with clear error about valid values (XS/S/M/L/XL)"

  - id: "US-002"
    title: "Task author receives warning for missing complexity on implementation tasks"
    priority: "P2"
    as_a: "task author"
    i_want: "to receive a warning when implementation tasks lack complexity estimates"
    so_that: "I am reminded to add estimates for tasks where effort matters"
    why_this_priority: "Encourages adoption without breaking existing workflows"
    independent_test: "Create tasks.yaml with implementation task missing complexity, run validation, observe warning"
    acceptance_scenarios:
      - given: "A tasks.yaml with an implementation task without complexity"
        when: "I run autospec artifact"
        then: "I see a warning suggesting to add complexity"
      - given: "A tasks.yaml with a setup task without complexity"
        when: "I run autospec artifact"
        then: "No warning is emitted (setup tasks don't require complexity)"
      - given: "A tasks.yaml with all implementation tasks having complexity"
        when: "I run autospec artifact"
        then: "No warnings about missing complexity"

  - id: "US-003"
    title: "User views complexity summary in status display"
    priority: "P1"
    as_a: "developer"
    i_want: "to see a summary of task complexity in the autospec st output"
    so_that: "I can quickly assess total effort and identify complex tasks"
    why_this_priority: "Key visibility feature - makes complexity data actionable"
    independent_test: "Create spec with tasks.yaml containing complexity values, run 'autospec st', verify summary appears"
    acceptance_scenarios:
      - given: "A tasks.yaml with tasks having various complexity values"
        when: "I run autospec st"
        then: "I see a breakdown showing count per complexity level (XS: 2, S: 3, M: 5, etc.)"
      - given: "A tasks.yaml with no complexity values"
        when: "I run autospec st"
        then: "The complexity section shows 'No complexity estimates' or is omitted"
      - given: "A tasks.yaml with some tasks having complexity and others not"
        when: "I run autospec st"
        then: "I see the breakdown plus a note about tasks without estimates"

  - id: "US-004"
    title: "System tracks actual completion time for complexity calibration"
    priority: "P3"
    as_a: "developer"
    i_want: "the system to track actual completion time against estimated complexity"
    so_that: "I can calibrate future estimates based on historical data"
    why_this_priority: "Nice-to-have analytics feature that builds on core functionality"
    independent_test: "Complete a task with complexity estimate, verify history log captures both complexity and duration"
    acceptance_scenarios:
      - given: "A task with complexity 'L' that takes 15 minutes to complete"
        when: "The task is marked complete"
        then: "The history log captures complexity 'L' and duration '15m'"
      - given: "Historical data exists for multiple completed tasks"
        when: "I want to analyze accuracy of estimates"
        then: "I can query history to see actual vs estimated complexity patterns"

requirements:
  functional:
    - id: "FR-001"
      description: "MUST add complexity field to TaskItem struct with valid values XS, S, M, L, XL"
      testable: true
      acceptance_criteria: "TaskItem struct has Complexity field with yaml tag; validation rejects invalid values"
    - id: "FR-002"
      description: "MUST emit validation warning when implementation-type tasks lack complexity field"
      testable: true
      acceptance_criteria: "Warning appears for type:implementation tasks missing complexity; no warning for setup/test tasks"
    - id: "FR-003"
      description: "MUST display complexity summary in autospec st output showing count per level"
      testable: true
      acceptance_criteria: "Status output shows 'XS: N, S: N, M: N, L: N, XL: N' when complexity values exist"
    - id: "FR-004"
      description: "SHOULD capture task complexity in history log entries when tasks complete"
      testable: true
      acceptance_criteria: "History entries include complexity field from completed task"
    - id: "FR-005"
      description: "MAY support aggregation of complexity data across multiple specs for analytics"
      testable: true
      acceptance_criteria: "History query can filter/aggregate by complexity level"
    - id: "FR-006"
      description: "MUST pass all quality gates: make test, make fmt, make lint, and make build"
      testable: true
      acceptance_criteria: "All commands exit 0; no test failures, format changes, lint errors, or build failures"
  non_functional:
    - id: "NFR-001"
      category: "code_quality"
      description: "All functions must be under 40 lines; extract helpers for complex logic"
      measurable_target: "No function exceeds 40 lines excluding comments"
    - id: "NFR-002"
      category: "code_quality"
      description: "All errors must be wrapped with context using fmt.Errorf(\"doing X: %w\", err)"
      measurable_target: "Zero bare 'return err' statements in new code"
    - id: "NFR-003"
      category: "code_quality"
      description: "Tests must use map-based table-driven pattern with t.Parallel()"
      measurable_target: "All new test functions use map[string]struct pattern and call t.Parallel()"
    - id: "NFR-004"
      category: "code_quality"
      description: "Accept interfaces, return concrete types"
      measurable_target: "Function signatures follow interface-in, concrete-out pattern where applicable"
    - id: "NFR-005"
      category: "performance"
      description: "Validation of complexity field must not degrade validation performance"
      measurable_target: "Validation completes in under 10ms per artifact (existing contract)"
    - id: "NFR-006"
      category: "usability"
      description: "Complexity values must be intuitive T-shirt sizes"
      measurable_target: "Valid values are XS, S, M, L, XL (industry standard sizing)"

success_criteria:
  measurable_outcomes:
    - id: "SC-001"
      description: "Users can assign complexity estimates to tasks without validation errors"
      metric: "Successful validation rate"
      target: "100% of valid complexity values pass validation"
    - id: "SC-002"
      description: "Users receive actionable warnings for missing complexity on implementation tasks"
      metric: "Warning specificity"
      target: "Warnings include task ID and suggest valid values"
    - id: "SC-003"
      description: "Status display provides at-a-glance complexity overview"
      metric: "Information density"
      target: "Complexity summary fits in one line showing all levels with counts"
    - id: "SC-004"
      description: "Existing workflows continue without disruption"
      metric: "Backward compatibility"
      target: "All existing tasks.yaml files validate without errors"

key_entities:
  - name: "TaskItem"
    description: "Represents a single task in tasks.yaml with metadata including new complexity field"
    attributes:
      - "complexity (optional): XS, S, M, L, XL"
      - "type: setup, implementation, test, documentation"
      - "status: Pending, In-progress, Completed, Blocked"
  - name: "ComplexityLevel"
    description: "Enumeration of valid complexity values using T-shirt sizing"
    attributes:
      - "XS: Extra small, trivial change"
      - "S: Small, straightforward change"
      - "M: Medium, moderate effort"
      - "L: Large, significant effort"
      - "XL: Extra large, substantial undertaking"
  - name: "ComplexitySummary"
    description: "Aggregated view of complexity distribution across tasks"
    attributes:
      - "counts per level"
      - "tasks without estimates"
      - "total tasks"

edge_cases:
  - scenario: "Tasks.yaml with mixed complexity and missing complexity"
    expected_behavior: "Display summary of present values; indicate count of tasks without estimates"
  - scenario: "All tasks are setup type with no complexity"
    expected_behavior: "No warnings emitted; complexity section shows 'No implementation tasks' or similar"
  - scenario: "Invalid complexity value like 'Medium' instead of 'M'"
    expected_behavior: "Validation error with clear message listing valid values"
  - scenario: "Complexity field present but empty string"
    expected_behavior: "Treat as missing; emit warning for implementation tasks"
  - scenario: "Legacy tasks.yaml files without any complexity fields"
    expected_behavior: "Validate successfully; warnings for implementation tasks; status shows no estimates"

assumptions:
  - "T-shirt sizing (XS/S/M/L/XL) is universally understood by development teams"
  - "Implementation tasks are the primary type requiring complexity estimates"
  - "Setup and documentation tasks typically have predictable, low complexity"
  - "History logging infrastructure exists and can be extended"
  - "ValidationResult already supports warnings (added in 046-plan-risks-section)"

constraints:
  - "Must not break existing tasks.yaml files that lack complexity field"
  - "Complexity field must be optional to maintain backward compatibility"
  - "Validation performance must remain under 10ms per artifact"
  - "Must use existing warning infrastructure from validation package"

out_of_scope:
  - "Automatic complexity estimation based on task content"
  - "Integration with external project management tools"
  - "Time tracking beyond what history logging provides"
  - "Complexity-based task assignment or scheduling recommendations"
  - "Visualization or charts of complexity data"
  - "Complexity inheritance from parent phases"

_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "autospec dev"
  created: "2025-12-18T06:10:32Z"
  artifact_type: "spec"
